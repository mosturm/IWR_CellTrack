{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import copy\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src1_batch[3])\n",
    "    #print('src2_batch',src2_batch[3])\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:],src1_batch[:,0,:].size())\n",
    "    #print('src2',src2_batch[:,0,:],src2_batch[:,0,:].size())\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,3) #!!!!!!!!!!##100 total data size\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,3) #!!!!!!!!\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "      \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        #print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    #print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    #print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        #print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    Ad_n = []\n",
    "    #Ad_n=copy.deepcopy(Ad)\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        \n",
    "        A_t,ill_flag=treshold(Ad[h],t=0.5)\n",
    "        \n",
    "        #print('ill_flag',ill_flag)\n",
    "        #print(Ad[h],A_t)\n",
    "        if ill_flag==True:\n",
    "            Ad[h]=np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy())\n",
    "            A_t = postprocess_MinCostAss(np.array([Ad[h]]),a)[0]\n",
    "            \n",
    "        #print(Ad[h],A_t)\n",
    "        Ad_n.append(A_t)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad_n\n",
    "\n",
    "def treshold(matrix, t):\n",
    "    z=np.where(matrix >= t, 1, 0)\n",
    "    \n",
    "    ill_flag=False\n",
    "\n",
    "      \n",
    "    if (~z.any(axis=0)).any() or any(np.sum(z[:,1:], axis=0)>1):\n",
    "        ill_flag=True\n",
    "          \n",
    "    return z,ill_flag\n",
    "\n",
    "def err_perc(a,b):\n",
    "    w=0\n",
    "    s=0\n",
    "    for i in range(len(a)):\n",
    "        m=a[i]-b[i].detach().numpy()\n",
    "        w=w+0.5*np.sum(np.abs(m))\n",
    "        s=s+np.size(m)\n",
    "    \n",
    "    \n",
    "    print('w,s',w,s)\n",
    "    \n",
    "    return w*100/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n",
      "[[0.87 0.05 0.08 0.77 0.11]\n",
      " [0.12 0.19 0.11 0.14 0.93]\n",
      " [0.12 0.45 0.89 0.23 0.05]\n",
      " [0.1  0.97 0.65 0.34 0.02]]\n",
      "True\n",
      "0.2222222222222222\n",
      "tensor(0.8848)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 5.1446e-01, -4.6608e-01, -4.8747e-01,  3.9730e-01, -7.0962e-02,\n",
       "           8.2392e-03,  1.2985e-01,  1.3680e-02, -5.2331e-02, -1.1606e-02,\n",
       "           8.1521e-02,  8.4285e-03,  2.6700e-02,  6.2394e-02, -9.2668e-03,\n",
       "           2.4149e-02, -4.9685e-02, -8.8619e-02, -1.4535e-01, -3.6507e-02,\n",
       "           3.4414e-02, -3.2187e-02, -2.5402e-02,  1.3105e-02, -2.2977e-02,\n",
       "          -7.7621e-03, -4.7917e-02, -1.7902e-02, -2.0271e-02, -4.9418e-03,\n",
       "          -2.0306e-02, -7.2812e-03,  4.0101e-02,  5.3344e-03,  3.1541e-02,\n",
       "          -4.7438e-02, -3.6705e-02,  3.4051e-02, -1.4871e-02,  9.7471e-03,\n",
       "           9.9112e-02,  7.3282e-02,  2.6340e-02,  4.0065e-02,  4.0045e-02,\n",
       "          -4.7557e-02,  2.5060e-02,  3.2530e-02, -2.6735e-03,  1.6553e-02,\n",
       "          -3.5783e-02, -8.8525e-02,  3.6611e-03, -3.7487e-02, -2.2043e-02,\n",
       "          -9.2531e-03, -4.2799e-02,  8.5479e-04,  1.6476e-02,  3.7989e-02],\n",
       "         [ 4.1257e-01, -3.4358e-01,  6.7212e-01,  5.0655e-02,  1.0482e-01,\n",
       "          -3.8736e-02,  9.7359e-02, -2.1116e-02, -2.5382e-02,  1.0065e-01,\n",
       "          -2.2293e-01, -2.1347e-02,  2.8321e-01,  1.8414e-02,  2.2444e-02,\n",
       "          -1.9875e-02, -1.0046e-02,  5.8475e-02,  7.3112e-02,  1.7937e-02,\n",
       "           1.1102e-02,  7.5975e-03,  2.3492e-02,  3.0056e-03,  3.4987e-02,\n",
       "           2.1610e-01,  1.2375e-01, -3.0463e-02,  7.0162e-03, -1.0132e-02,\n",
       "          -1.7476e-02, -9.2427e-03, -3.7400e-03, -5.9920e-02, -3.4575e-03,\n",
       "          -5.1593e-04,  5.4442e-02, -2.4610e-02,  1.4916e-02,  1.7803e-02,\n",
       "           7.3827e-02,  3.9662e-02,  3.2795e-02, -2.1492e-03,  1.8361e-02,\n",
       "          -1.3865e-02,  1.9717e-02,  1.5899e-02, -8.0203e-03,  5.1114e-03,\n",
       "           2.5435e-03, -1.2782e-03, -1.3855e-02, -9.9643e-03,  3.3568e-05,\n",
       "           1.9814e-02, -2.0594e-03,  2.4882e-03, -1.0163e-02, -1.0667e-02],\n",
       "         [ 5.4811e-01,  6.8184e-01, -1.5652e-02,  5.9947e-02,  1.1216e-01,\n",
       "          -3.7457e-02,  4.1619e-02, -2.6765e-02,  3.1161e-01, -4.8714e-02,\n",
       "           1.8084e-01,  2.3538e-02,  3.9662e-02,  8.8387e-02,  3.2951e-03,\n",
       "           1.3541e-02, -8.7123e-02, -6.8975e-02,  2.0076e-02,  1.0151e-02,\n",
       "          -4.7100e-02, -6.0040e-02, -1.6322e-02,  2.5572e-02, -3.9682e-02,\n",
       "           4.3185e-03, -2.9950e-02, -2.6804e-02,  2.6856e-03, -1.5506e-02,\n",
       "          -5.4388e-03, -2.2979e-02, -1.1083e-01, -1.1631e-01,  4.3108e-02,\n",
       "          -3.7329e-02, -8.1547e-03,  4.9694e-03,  1.8780e-02,  1.6105e-02,\n",
       "          -2.9656e-02, -2.2418e-03, -1.8049e-02, -5.9529e-02, -3.8795e-03,\n",
       "           1.7900e-02, -2.9176e-02, -3.3287e-02,  5.5362e-02,  4.8200e-03,\n",
       "          -9.3430e-03, -2.4774e-02,  1.9165e-02, -3.3257e-02, -4.6310e-02,\n",
       "           2.9986e-03, -3.5219e-02,  2.7049e-02, -4.7686e-02, -1.7536e-03]]),\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 5.1582e-01,  7.5171e-01,  2.9533e-02,  1.0671e-01,  5.1754e-02,\n",
       "          -2.0538e-02,  8.6587e-04, -2.5977e-02,  3.2848e-01, -5.8680e-02,\n",
       "           1.1895e-01,  1.5467e-02,  5.1541e-02,  4.2132e-02,  8.6867e-03,\n",
       "          -2.8931e-03, -4.0353e-02, -8.6143e-03,  9.8255e-03,  2.1702e-02,\n",
       "          -1.8178e-02,  1.1487e-02,  4.0467e-03,  1.2318e-02, -6.3247e-03,\n",
       "           2.7013e-02,  2.7019e-03, -5.9952e-03,  7.3248e-03, -2.9332e-03,\n",
       "           5.4951e-03, -9.2632e-03, -1.7197e-02, -3.4553e-02,  1.3735e-02,\n",
       "          -6.6964e-03,  1.0133e-02, -1.2308e-02, -2.3443e-03,  1.1981e-02,\n",
       "           7.1448e-03, -1.1924e-02, -3.6682e-03, -2.7565e-02, -6.3407e-03,\n",
       "           1.8032e-02, -3.1417e-02, -3.8988e-04,  3.3051e-02,  1.4797e-02,\n",
       "          -6.8803e-03, -4.0917e-02,  2.2601e-02, -3.8443e-02, -3.0328e-02,\n",
       "          -5.1845e-02, -4.7495e-03,  2.8041e-02, -6.7425e-02, -1.6071e-02],\n",
       "         [ 4.5949e-01, -3.3718e-01, -4.4662e-01,  9.6880e-02,  2.6982e-01,\n",
       "          -1.2365e-01,  5.7858e-01,  5.3379e-02, -1.2946e-01, -3.1783e-03,\n",
       "           7.9367e-02,  1.2606e-02,  9.2937e-03,  2.5190e-02,  6.6387e-03,\n",
       "           5.4749e-03, -3.5184e-02, -2.6801e-02,  2.8667e-02, -9.8100e-03,\n",
       "          -3.0406e-03, -5.2415e-02,  3.0576e-02,  3.5821e-02, -4.0672e-02,\n",
       "           5.6864e-03, -6.8069e-03,  3.4378e-02,  1.2280e-02, -3.9123e-03,\n",
       "           2.4022e-02, -9.9622e-03,  1.3788e-03,  6.9632e-03, -4.0222e-03,\n",
       "           5.6856e-03,  1.0632e-03, -7.1510e-03, -5.4671e-03, -3.0404e-04,\n",
       "          -2.5007e-02, -3.4829e-02, -3.4943e-02,  1.6037e-02, -5.9543e-04,\n",
       "          -8.3758e-03, -9.3036e-03,  7.1623e-03,  8.4003e-03,  5.7478e-03,\n",
       "           1.5580e-02, -3.0086e-03,  4.4279e-04, -1.8360e-02,  7.6778e-03,\n",
       "          -6.5042e-03,  1.9926e-02,  3.2724e-03,  8.5199e-03, -3.6238e-03],\n",
       "         [ 4.3988e-01, -3.6852e-01,  7.6753e-01,  1.2491e-01, -2.5459e-02,\n",
       "           7.0976e-03, -3.1522e-02, -1.6172e-02,  1.4405e-02,  5.1280e-02,\n",
       "          -6.3243e-02, -6.8521e-03,  1.9879e-01,  4.6008e-02,  1.3332e-02,\n",
       "           3.6731e-03, -3.9620e-02, -1.0212e-02, -6.4141e-02, -1.3789e-02,\n",
       "           3.1264e-02, -4.0958e-02, -2.5979e-02,  3.9056e-03, -3.5196e-03,\n",
       "           4.2275e-03, -5.8461e-03,  6.8099e-03,  2.6496e-02,  1.2284e-02,\n",
       "           1.1552e-02, -1.3633e-02, -1.2287e-02, -2.3817e-02,  3.4462e-02,\n",
       "          -1.0074e-02, -1.0218e-02, -3.2383e-03, -2.9657e-03,  2.1351e-03,\n",
       "           3.1415e-03, -5.1414e-03, -1.3771e-03,  5.6272e-03,  1.4194e-04,\n",
       "          -1.5051e-02,  6.1140e-03, -1.9389e-03,  1.0465e-02, -2.1335e-03,\n",
       "           7.3647e-03, -1.1729e-02,  1.3443e-02,  2.2338e-03, -6.6457e-03,\n",
       "          -1.4250e-02, -2.1163e-02, -1.0188e-02,  1.5371e-02,  2.6878e-02]]),\n",
       " tensor([[1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0.]]),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 0.1760, 1.0000, 0.1025],\n",
       "         [1.0000, 0.1295, 0.0678, 1.0000],\n",
       "         [1.0000, 1.0000, 0.1643, 0.1766]], dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "#print(threshold_matrix(a, 0.2))\n",
    "print(a[:,1:])\n",
    "print(any(np.sum(a[:,1:], axis=0)>1))\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "c = np.array([[1, 0,0], [0, 1,0],[0, 0,1]])\n",
    "d = np.array([[1, 0,0], [0, 0,1],[0, 0,1]])\n",
    "\n",
    "\n",
    "m=c-d\n",
    "print(np.sum(np.abs(m))/np.size(m))\n",
    "\n",
    "\n",
    "c = torch.from_numpy(c).float()\n",
    "d = torch.from_numpy(d).float()\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = cross_entropy_loss(c, d)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = False, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05):\n",
    "        super(AdjacencyTransformer_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = src_t1\n",
    "        src2_emb = src_t2\n",
    "        #print('src1',src1_emb.size())\n",
    "        #print('src2',src2_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        #print('out1',out1.size())\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        #print('out_dec1',out_dec1.size())\n",
    "        #out_dec1=self.lin2(out_dec1)\n",
    "        #print('out_dec1b',out_dec1.size())\n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        #print('z',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 60 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.537, Val loss: 4.300, Epoch time = 2.944s\n",
      "Epoch: 2, Train loss: 4.793, Val loss: 4.380, Epoch time = 3.179s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f41ac327290>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack_2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack_2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=10000\n",
    "plt.plot(np.convolve(loss_over_time, np.ones(N)/N, mode='valid'),c='red',label='train loss')\n",
    "plt.plot(np.convolve(test_error, np.ones(N)/N, mode='valid'),label='test loss')\n",
    "plt.title('Running mean of loss over epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.]])\n",
      "Ad tensor([[1.0000, 0.9767, 0.9341, 0.9624],\n",
      "        [0.9974, 0.8354, 1.0000, 0.9914],\n",
      "        [0.8336, 0.0033, 0.5013, 1.0000],\n",
      "        [0.9948, 1.0000, 0.9309, 0.2526]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 0.9999, 0.9996, 0.9997],\n",
      "        [0.9512, 0.0015, 0.0032, 0.8828, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.5083, 0.9983],\n",
      "        [0.9886, 0.7781, 0.9931, 1.0000, 0.8776]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9993, 0.9998, 0.9990],\n",
      "        [0.9998, 0.0665, 1.0000, 0.2011],\n",
      "        [0.9895, 0.0675, 1.0000, 0.0112],\n",
      "        [0.9939, 1.0000, 0.4625, 0.2910],\n",
      "        [0.9994, 0.4460, 0.8340, 1.0000]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9993, 0.9990, 1.0000],\n",
      "        [0.9997, 0.0018, 1.0000, 0.9326],\n",
      "        [1.0000, 0.9776, 0.1659, 1.0000],\n",
      "        [0.9999, 1.0000, 0.7979, 0.0505]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.5916, 0.9956, 0.9665],\n",
      "        [0.9947, 0.0397, 0.5336, 1.0000],\n",
      "        [0.1359, 1.0000, 0.0157, 0.0038],\n",
      "        [1.0000, 0.7841, 1.0000, 0.9971]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9540, 1.0000, 0.9996],\n",
      "        [0.8407, 1.0000, 0.4544, 0.0554],\n",
      "        [0.9983, 0.0067, 1.0000, 0.4865],\n",
      "        [1.0000, 0.9584, 0.9987, 1.0000]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9238, 0.9867, 0.9932],\n",
      "        [0.7790, 1.0000, 0.0572, 0.2502],\n",
      "        [0.8828, 0.0015, 0.0216, 1.0000],\n",
      "        [0.9973, 0.4978, 1.0000, 0.5859]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9767, 0.9341, 0.9624],\n",
      "        [0.9974, 0.8354, 1.0000, 0.9914],\n",
      "        [0.8336, 0.0033, 0.5013, 1.0000],\n",
      "        [0.9948, 1.0000, 0.9309, 0.2526]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.9997e-01, 9.9999e-01, 9.9955e-01, 9.9977e-01],\n",
      "        [2.0921e-02, 9.9999e-01, 1.2127e-01, 8.5899e-05, 1.3692e-03],\n",
      "        [8.2311e-01, 4.4219e-02, 2.9198e-01, 1.0000e+00, 1.0000e+00],\n",
      "        [9.9999e-01, 9.9557e-01, 1.0000e+00, 9.9921e-01, 9.9974e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.7923, 0.9999, 0.9990],\n",
      "        [0.9984, 1.0000, 1.0000, 0.9999],\n",
      "        [0.9990, 0.9862, 1.0000, 0.9894],\n",
      "        [1.0000, 0.9487, 0.9999, 1.0000],\n",
      "        [0.9991, 0.7076, 0.9988, 1.0000]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.6389, 0.9880, 0.9932, 0.9975, 0.4035],\n",
      "        [0.9909, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "        [0.7493, 0.7819, 0.9727, 0.9950, 0.9159, 0.0012],\n",
      "        [0.9999, 0.9375, 1.0000, 1.0000, 1.0000, 0.9110]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9999, 0.9804, 0.9904, 0.9998],\n",
      "        [0.9914, 0.9985, 1.0000, 1.0000, 0.9986],\n",
      "        [0.9999, 1.0000, 0.9977, 0.9994, 1.0000],\n",
      "        [0.9351, 0.9998, 0.6574, 0.8861, 0.9977],\n",
      "        [1.0000, 1.0000, 0.9990, 0.9996, 1.0000],\n",
      "        [0.9984, 1.0000, 0.9999, 1.0000, 1.0000]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9996, 0.9999, 0.9998, 0.9998, 0.9999, 0.9999],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.8409e-01, 8.3083e-01, 3.6902e-01, 4.8980e-01, 4.3122e-01, 2.7225e-01,\n",
      "         8.0232e-02],\n",
      "        [1.3901e-01, 7.8997e-01, 7.5865e-01, 2.2772e-01, 7.5401e-01, 3.7518e-01,\n",
      "         4.8768e-01],\n",
      "        [1.0151e-01, 6.7017e-01, 4.4717e-01, 1.7928e-01, 5.3661e-01, 2.3740e-01,\n",
      "         2.6752e-01],\n",
      "        [9.9426e-01, 9.9842e-01, 9.8755e-01, 9.9434e-01, 9.8467e-01, 9.8727e-01,\n",
      "         9.7430e-01],\n",
      "        [2.0325e-01, 9.0616e-01, 8.8101e-01, 4.1848e-01, 8.8692e-01, 6.8727e-01,\n",
      "         7.4303e-01],\n",
      "        [2.9032e-01, 9.2292e-01, 7.7144e-01, 5.0844e-01, 7.7375e-01, 5.9769e-01,\n",
      "         6.2269e-01],\n",
      "        [6.4360e-04, 1.3027e-02, 9.4757e-03, 1.0769e-03, 2.6497e-02, 3.4006e-03,\n",
      "         6.3391e-03]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[0.9998, 0.9386, 0.9377, 0.9929, 0.9955, 0.9917, 0.9950],\n",
      "        [0.9990, 0.9991, 0.9998, 1.0000, 1.0000, 0.9999, 0.9999],\n",
      "        [0.9997, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9981, 0.9996, 0.9997, 0.9999, 0.9999, 0.9995, 0.9992],\n",
      "        [1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9983, 0.9997, 0.9998, 1.0000, 0.9999, 0.9998, 0.9996]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[0.9999, 0.9976, 0.9902, 0.9997, 0.9831, 0.9996, 0.9997],\n",
      "        [0.9981, 0.9999, 1.0000, 0.9998, 0.9999, 1.0000, 0.9999],\n",
      "        [0.9565, 0.9987, 0.9997, 0.9966, 0.9992, 1.0000, 0.9996],\n",
      "        [0.9947, 0.9997, 0.9991, 0.9998, 0.9993, 1.0000, 0.9999],\n",
      "        [0.9948, 0.9998, 0.9997, 0.9999, 0.9995, 1.0000, 1.0000],\n",
      "        [0.9976, 0.9999, 0.9997, 1.0000, 0.9996, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 0.9996, 0.9999, 0.9999, 0.9999, 0.9999, 1.0000],\n",
      "        [0.9943, 0.9995, 0.9992, 0.9993, 0.9993, 0.9988, 0.9997, 0.9995],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9586, 0.9995, 0.9906, 0.9941, 0.9961, 0.9987, 0.9999, 0.9999],\n",
      "        [0.9983, 1.0000, 0.9997, 0.9998, 0.9996, 0.9999, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 0.9999, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9997, 1.0000, 0.9999, 0.9996, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9998, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9745, 0.9996, 0.9936, 0.9791, 0.9973, 0.9972, 0.9997, 0.9991],\n",
      "        [0.9950, 0.9998, 0.9992, 0.9960, 0.9995, 0.9993, 0.9999, 0.9999]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9981, 0.9999, 0.9997, 0.9921, 0.9999, 0.9998, 0.9997],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9869, 0.9969, 0.9968, 0.9941, 0.9916, 0.9999, 0.9896],\n",
      "        [0.9994, 1.0000, 0.9999, 0.9976, 1.0000, 0.9997, 0.9999],\n",
      "        [0.9984, 1.0000, 0.9999, 0.9990, 0.9998, 0.9998, 0.9999],\n",
      "        [0.9992, 1.0000, 0.9999, 0.9954, 1.0000, 0.9999, 0.9999],\n",
      "        [0.9999, 1.0000, 0.9999, 0.9991, 0.9999, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9981, 0.9932, 0.9974, 0.9997, 0.9975, 0.9948, 0.9872, 0.9997, 0.9998],\n",
      "        [0.9741, 0.9992, 0.9954, 0.9993, 0.9991, 0.9994, 0.9996, 0.9994, 0.9999],\n",
      "        [0.9988, 0.9837, 0.9996, 0.9993, 0.9990, 0.9897, 0.9915, 0.9994, 0.9996],\n",
      "        [0.9603, 0.9587, 0.9950, 0.9935, 0.9993, 0.9953, 0.9999, 0.9865, 0.9994],\n",
      "        [0.9999, 0.9993, 1.0000, 0.9999, 1.0000, 0.9996, 0.9999, 0.9999, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9988, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9998, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9991, 0.9502, 0.9997, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9995, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9998, 0.9998, 1.0000, 0.9997, 1.0000, 1.0000, 0.9999, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9962, 0.9931, 1.0000, 0.9981, 0.9970, 0.9999, 0.9972, 0.9996],\n",
      "        [0.0135, 0.0043, 0.2769, 0.5712, 0.1409, 0.0725, 0.2731, 0.1355, 0.0887],\n",
      "        [0.9707, 0.9406, 0.9820, 0.9998, 0.9960, 0.9772, 0.9996, 0.9989, 0.9959],\n",
      "        [0.0384, 0.0422, 0.0346, 0.8252, 0.1275, 0.0066, 0.6016, 0.6800, 0.0849],\n",
      "        [0.0071, 0.0116, 0.0114, 0.6932, 0.1172, 0.0056, 0.5313, 0.5149, 0.5201],\n",
      "        [0.1349, 0.0080, 0.0427, 0.9890, 0.2768, 0.0392, 0.5894, 0.4772, 0.4131],\n",
      "        [0.6256, 0.8396, 0.9869, 0.9994, 0.9970, 0.9153, 0.9970, 0.9996, 0.9176],\n",
      "        [0.8623, 0.8055, 0.9614, 0.9999, 0.9983, 0.9789, 0.9974, 0.9994, 0.9909]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9573, 0.6848, 0.9892, 0.9448, 0.9709, 0.9911, 0.9581, 0.9901, 0.9577],\n",
      "        [0.8727, 0.9991, 0.9992, 0.9994, 0.9980, 0.7352, 0.9993, 0.9932, 0.9800],\n",
      "        [0.2634, 0.2912, 0.8747, 0.4813, 0.5334, 0.0233, 0.1305, 0.2986, 0.0858],\n",
      "        [0.7689, 0.9776, 0.9987, 0.9568, 0.9789, 0.4949, 0.9992, 0.9544, 0.7545],\n",
      "        [0.9994, 1.0000, 1.0000, 1.0000, 0.9999, 0.9962, 1.0000, 1.0000, 0.9996],\n",
      "        [1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9992, 0.9999, 0.9999, 0.9999],\n",
      "        [0.9981, 0.9999, 1.0000, 0.9999, 1.0000, 0.9983, 0.9999, 0.9999, 0.9991],\n",
      "        [0.9997, 0.9975, 0.9999, 0.9995, 0.9998, 0.9994, 0.9995, 0.9996, 0.9981]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9998e-01, 9.9962e-01, 9.9948e-01, 9.9504e-01, 9.8278e-01, 9.9942e-01,\n",
      "         9.0062e-01, 9.9975e-01, 9.7820e-01, 9.9893e-01],\n",
      "        [4.5479e-03, 9.8387e-02, 6.9175e-01, 8.8422e-02, 4.9516e-03, 9.5688e-01,\n",
      "         6.2888e-05, 6.4292e-01, 3.6172e-02, 2.1384e-01],\n",
      "        [1.4687e-02, 3.7269e-01, 1.8984e-01, 6.7644e-03, 7.9012e-04, 4.8630e-01,\n",
      "         1.4659e-05, 1.3183e-01, 8.6025e-04, 2.1127e-02],\n",
      "        [2.7586e-03, 1.6824e-02, 2.6408e-01, 5.9942e-03, 4.9877e-04, 3.0191e-01,\n",
      "         5.8759e-06, 6.9827e-01, 3.4013e-03, 8.5283e-03],\n",
      "        [2.4507e-02, 1.8650e-01, 7.1189e-01, 5.3818e-02, 7.5991e-03, 6.4088e-01,\n",
      "         2.0702e-04, 9.1253e-01, 1.1110e-02, 6.3811e-01],\n",
      "        [2.2616e-03, 2.2647e-03, 9.0389e-03, 9.8569e-04, 3.0578e-02, 8.9428e-03,\n",
      "         1.3621e-02, 1.4033e-02, 5.1459e-03, 5.7037e-02],\n",
      "        [3.8993e-05, 3.0125e-02, 2.2176e-02, 6.9134e-04, 1.2400e-04, 2.5109e-01,\n",
      "         3.1535e-06, 9.6303e-04, 1.7616e-04, 1.2156e-03],\n",
      "        [5.0956e-04, 9.1913e-03, 5.8617e-03, 1.0379e-03, 1.4954e-03, 4.2004e-02,\n",
      "         1.9353e-05, 3.4215e-03, 7.0189e-04, 1.1687e-02],\n",
      "        [1.7341e-02, 2.1208e-01, 6.4579e-01, 2.1659e-01, 2.9481e-02, 8.3526e-01,\n",
      "         2.1807e-04, 4.1949e-01, 5.2674e-02, 6.1072e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9941e-01, 9.0907e-01, 9.7585e-01, 2.5312e-01, 8.6851e-01, 9.7821e-01,\n",
      "         7.1415e-01, 9.8227e-01, 9.8030e-01, 9.7660e-01, 2.1372e-01],\n",
      "        [4.9802e-06, 5.4653e-07, 1.9429e-05, 1.6956e-07, 2.2971e-06, 7.3227e-06,\n",
      "         1.7691e-07, 7.5130e-06, 1.4750e-06, 1.8711e-06, 1.1517e-06],\n",
      "        [2.9737e-01, 1.9312e-02, 6.4187e-01, 1.0571e-01, 7.2525e-01, 4.2962e-01,\n",
      "         1.0877e-01, 8.1691e-01, 1.6938e-02, 5.4272e-01, 6.3691e-02],\n",
      "        [6.1813e-03, 5.9480e-04, 4.5876e-02, 8.8929e-03, 1.3759e-02, 4.0337e-02,\n",
      "         6.2656e-03, 2.7839e-01, 1.1207e-03, 1.0525e-02, 4.0021e-03],\n",
      "        [1.0535e-02, 1.3281e-01, 2.2805e-02, 6.0486e-03, 7.9938e-02, 6.5951e-01,\n",
      "         1.9844e-02, 4.1824e-01, 1.7154e-01, 1.3657e-02, 5.8621e-03],\n",
      "        [1.3963e-02, 1.5438e-04, 6.8708e-03, 1.9101e-02, 3.1010e-02, 1.5347e-02,\n",
      "         1.1348e-03, 2.3512e-02, 1.1652e-03, 2.7716e-02, 7.7305e-03],\n",
      "        [3.9052e-01, 4.9614e-01, 2.6648e-02, 9.2921e-03, 7.2837e-02, 3.8509e-01,\n",
      "         8.3108e-02, 3.1374e-01, 6.8393e-01, 1.4142e-02, 2.4507e-03],\n",
      "        [1.4740e-01, 1.6604e-03, 9.6763e-01, 2.2541e-03, 2.8345e-02, 3.4020e-02,\n",
      "         4.5539e-03, 5.0757e-02, 1.9229e-03, 1.1344e-02, 1.3171e-03],\n",
      "        [3.2928e-05, 8.9228e-06, 5.9932e-04, 5.6530e-05, 8.2908e-05, 1.7607e-03,\n",
      "         1.2589e-04, 1.0726e-03, 8.4408e-06, 6.2741e-04, 2.6821e-05],\n",
      "        [3.4482e-01, 1.3229e-02, 9.5867e-01, 5.0662e-02, 2.2822e-01, 6.0379e-01,\n",
      "         2.7495e-02, 7.2082e-01, 1.4524e-02, 1.1426e-02, 5.9202e-03]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[0.9985, 0.9451, 0.6325, 0.0151, 0.5523, 0.8806, 0.5223, 0.9356, 0.0992,\n",
      "         0.9040, 0.6755],\n",
      "        [0.9565, 0.9990, 0.9335, 0.8233, 0.9164, 0.9999, 0.4357, 0.9997, 0.4315,\n",
      "         0.9990, 0.9808],\n",
      "        [0.9986, 0.9947, 0.9997, 0.8501, 0.9989, 0.9964, 0.9999, 0.9875, 0.9511,\n",
      "         0.9871, 0.9996],\n",
      "        [0.5653, 0.4865, 0.8909, 0.9340, 0.7927, 0.7785, 0.3606, 0.2433, 0.7189,\n",
      "         0.7960, 0.6656],\n",
      "        [0.1249, 0.3960, 0.9915, 0.7133, 0.9319, 0.8829, 0.2132, 0.7097, 0.8112,\n",
      "         0.7705, 0.9698],\n",
      "        [0.0067, 0.6464, 0.8139, 0.0454, 0.7694, 0.9301, 0.0608, 0.8989, 0.8550,\n",
      "         0.9572, 0.8950],\n",
      "        [0.2083, 0.1370, 0.9730, 0.9939, 0.8018, 0.9755, 0.1039, 0.7102, 0.9206,\n",
      "         0.9697, 0.9733],\n",
      "        [0.0421, 0.3114, 0.9847, 0.1312, 0.4886, 0.9297, 0.1266, 0.3228, 0.2328,\n",
      "         0.9453, 0.7338],\n",
      "        [0.9426, 0.9786, 0.9909, 0.9423, 0.9502, 0.9991, 0.5186, 0.9986, 0.9525,\n",
      "         0.9997, 0.9833],\n",
      "        [0.8965, 0.9881, 0.9926, 0.6454, 0.9697, 0.9990, 0.2674, 0.9968, 0.9900,\n",
      "         0.9972, 0.9922],\n",
      "        [0.6742, 0.9438, 0.9949, 0.9971, 0.9936, 0.9990, 0.5744, 0.9943, 0.9961,\n",
      "         0.9969, 0.9990]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9757e-01, 7.3490e-02, 1.3133e-01, 8.7187e-02, 1.8145e-01, 2.1053e-01,\n",
      "         2.7844e-01, 6.3374e-02, 6.9178e-02, 4.0742e-02, 1.3385e-01, 1.5865e-02,\n",
      "         1.4105e-01, 4.6825e-02],\n",
      "        [9.9878e-01, 9.3467e-01, 9.9940e-01, 9.9709e-01, 9.9914e-01, 9.9920e-01,\n",
      "         9.9464e-01, 9.9593e-01, 9.6887e-01, 9.9764e-01, 9.9720e-01, 9.6698e-01,\n",
      "         9.2535e-01, 8.9223e-01],\n",
      "        [9.1785e-01, 9.8672e-01, 2.5315e-01, 8.3041e-01, 7.6651e-01, 2.6624e-01,\n",
      "         9.9681e-01, 9.7143e-01, 9.7635e-01, 6.9808e-01, 9.4228e-01, 8.3289e-01,\n",
      "         9.9864e-01, 9.9137e-01],\n",
      "        [9.3521e-01, 9.6802e-01, 2.2674e-01, 4.2340e-01, 8.0999e-01, 6.8660e-01,\n",
      "         8.6399e-02, 9.7355e-01, 3.0010e-01, 8.8861e-01, 8.1287e-01, 7.4512e-01,\n",
      "         9.0996e-01, 9.8057e-01],\n",
      "        [9.9993e-01, 9.9999e-01, 9.9866e-01, 9.9947e-01, 9.9932e-01, 9.9784e-01,\n",
      "         9.9932e-01, 9.9968e-01, 9.9994e-01, 9.9995e-01, 9.9996e-01, 9.9997e-01,\n",
      "         9.9992e-01, 9.9994e-01],\n",
      "        [9.8631e-01, 8.4677e-01, 9.4469e-01, 9.5271e-01, 9.9177e-01, 9.7168e-01,\n",
      "         6.4555e-01, 9.8372e-01, 9.3517e-01, 8.8248e-01, 9.4960e-01, 9.2724e-01,\n",
      "         9.0366e-01, 6.9447e-01],\n",
      "        [9.9553e-01, 9.1253e-01, 6.4769e-01, 8.3080e-01, 7.8291e-01, 5.8168e-01,\n",
      "         9.9985e-01, 9.0681e-01, 9.5020e-01, 2.1874e-01, 9.1603e-01, 4.8720e-01,\n",
      "         9.9487e-01, 9.3972e-01],\n",
      "        [1.3156e-01, 3.7900e-03, 3.9214e-03, 8.2026e-03, 3.0505e-02, 6.5184e-03,\n",
      "         1.8142e-03, 1.0646e-01, 2.0356e-02, 1.9871e-02, 9.6303e-03, 4.2542e-02,\n",
      "         8.8137e-04, 3.7258e-03],\n",
      "        [4.1741e-01, 9.7517e-01, 7.4954e-01, 8.5418e-01, 7.8412e-01, 8.2502e-01,\n",
      "         9.4339e-02, 9.0069e-01, 9.8998e-01, 6.8573e-01, 9.9556e-01, 9.8305e-01,\n",
      "         9.9825e-01, 9.7938e-01],\n",
      "        [8.2404e-01, 7.6420e-01, 4.0609e-01, 9.5376e-01, 9.5662e-01, 6.2404e-01,\n",
      "         1.5600e-01, 9.8244e-01, 9.4113e-01, 7.8828e-01, 9.5342e-01, 9.5102e-01,\n",
      "         7.3597e-01, 8.6335e-01],\n",
      "        [5.2458e-01, 5.8089e-01, 8.1754e-02, 1.0055e-01, 2.1511e-01, 1.1163e-01,\n",
      "         5.1171e-02, 2.7372e-01, 2.6351e-01, 1.6346e-01, 4.2279e-01, 5.3323e-01,\n",
      "         6.1845e-01, 3.0993e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.0433e-01, 9.6725e-04, 1.5599e-04, 3.4227e-04, 1.7638e-03, 8.8940e-05,\n",
      "         5.0390e-05, 1.5469e-03, 2.0893e-04, 6.4584e-05, 1.8725e-03, 1.5003e-04,\n",
      "         1.3786e-04, 2.5434e-04, 6.9076e-04],\n",
      "        [9.0312e-01, 9.0189e-01, 9.9666e-01, 3.6836e-01, 9.9873e-01, 9.1184e-01,\n",
      "         8.4624e-01, 7.5885e-01, 2.3817e-01, 9.8160e-01, 9.3238e-01, 9.9771e-01,\n",
      "         9.2583e-01, 8.4562e-01, 9.7507e-01],\n",
      "        [5.6301e-01, 8.2547e-01, 2.6581e-03, 8.2493e-01, 1.1631e-01, 6.1390e-03,\n",
      "         4.2710e-04, 2.4804e-01, 8.9211e-01, 1.1869e-02, 2.5330e-01, 2.4392e-02,\n",
      "         1.0467e-02, 7.1370e-04, 1.0098e-01],\n",
      "        [9.6605e-01, 9.9933e-01, 9.7791e-01, 9.9432e-01, 9.9271e-01, 9.8769e-01,\n",
      "         5.1703e-01, 9.9893e-01, 9.8587e-01, 9.5779e-01, 9.9502e-01, 9.8774e-01,\n",
      "         7.5527e-01, 7.9623e-01, 9.8703e-01],\n",
      "        [9.6432e-01, 9.9825e-01, 7.1513e-01, 9.8703e-01, 9.6599e-01, 8.7697e-01,\n",
      "         2.4683e-01, 9.9687e-01, 9.6807e-01, 7.3642e-01, 9.6081e-01, 8.3678e-01,\n",
      "         6.3687e-01, 3.1154e-01, 9.1619e-01],\n",
      "        [9.0948e-01, 9.7219e-01, 1.2100e-02, 9.4069e-01, 3.5511e-01, 6.3475e-02,\n",
      "         2.4673e-03, 8.1408e-01, 9.5653e-01, 5.7209e-02, 6.8746e-01, 4.8117e-02,\n",
      "         5.8774e-02, 2.9683e-03, 3.4976e-01],\n",
      "        [4.3819e-02, 2.6841e-04, 8.2568e-04, 1.2861e-04, 4.7877e-04, 2.1057e-03,\n",
      "         2.3823e-01, 1.4310e-03, 5.7807e-05, 5.8406e-05, 3.7742e-03, 8.3263e-04,\n",
      "         6.1814e-05, 4.6820e-01, 7.4515e-05],\n",
      "        [7.5303e-02, 8.7351e-01, 5.8944e-01, 1.4267e-01, 9.6480e-01, 9.2543e-01,\n",
      "         5.3759e-02, 8.2959e-01, 6.7005e-02, 6.4862e-01, 8.2180e-01, 5.4770e-01,\n",
      "         1.2523e-01, 5.5101e-02, 8.8060e-01],\n",
      "        [8.2432e-03, 2.6071e-01, 5.7013e-01, 4.8232e-02, 4.1584e-01, 6.7608e-01,\n",
      "         1.6960e-02, 5.9587e-01, 8.2095e-03, 4.0141e-01, 8.1268e-01, 3.7399e-01,\n",
      "         3.0750e-02, 2.9129e-02, 5.0428e-01],\n",
      "        [9.7211e-01, 9.9867e-01, 9.1639e-01, 9.8795e-01, 9.9989e-01, 9.9431e-01,\n",
      "         8.2548e-01, 9.9884e-01, 9.9475e-01, 9.9828e-01, 9.9553e-01, 9.9599e-01,\n",
      "         9.9142e-01, 7.6781e-01, 9.9830e-01],\n",
      "        [4.2559e-01, 9.7438e-01, 7.7200e-01, 9.1665e-01, 9.6027e-01, 5.4503e-01,\n",
      "         1.1496e-02, 8.9353e-01, 9.0785e-01, 8.6045e-01, 8.9177e-01, 8.2528e-01,\n",
      "         5.5509e-01, 1.8666e-02, 9.3246e-01],\n",
      "        [1.3459e-01, 9.4423e-01, 9.3993e-01, 2.3734e-01, 9.8643e-01, 8.9633e-01,\n",
      "         5.7822e-03, 9.7696e-01, 2.2458e-01, 9.9151e-01, 9.5990e-01, 8.7848e-01,\n",
      "         5.2447e-01, 3.9955e-03, 9.9193e-01],\n",
      "        [2.8620e-03, 6.8851e-03, 8.0531e-01, 2.4803e-03, 1.7352e-01, 5.1904e-03,\n",
      "         1.0673e-03, 3.0068e-04, 2.0515e-04, 1.4198e-02, 3.3254e-02, 3.8384e-01,\n",
      "         1.1840e-02, 1.1119e-02, 4.1461e-02],\n",
      "        [1.0318e-03, 3.7812e-04, 1.9335e-01, 3.0253e-05, 6.3496e-02, 1.2662e-03,\n",
      "         5.1828e-04, 1.4932e-04, 5.4671e-06, 5.4475e-03, 2.1085e-03, 8.8591e-02,\n",
      "         2.5099e-03, 1.1270e-03, 3.8158e-03]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[9.6391e-01, 2.0746e-01, 2.1730e-02, 4.3149e-03, 1.6787e-02, 8.0562e-02,\n",
      "         3.6234e-02, 8.8517e-03, 2.5624e-03, 1.6004e-02, 7.6176e-02, 2.5626e-03,\n",
      "         1.9766e-03, 2.5321e-03, 1.3733e-02],\n",
      "        [7.2832e-01, 9.9588e-01, 8.5197e-01, 2.6512e-01, 9.6763e-01, 9.9338e-01,\n",
      "         9.8389e-01, 6.4606e-01, 8.2265e-01, 7.9394e-01, 9.5076e-01, 2.4657e-01,\n",
      "         4.3953e-01, 3.2716e-01, 9.2935e-01],\n",
      "        [1.3265e-05, 7.2153e-04, 2.4792e-04, 3.3510e-05, 1.0144e-03, 1.2772e-04,\n",
      "         1.5735e-05, 1.9275e-07, 2.7102e-04, 1.6734e-06, 1.8080e-05, 3.6289e-05,\n",
      "         8.3746e-04, 3.0022e-04, 2.2249e-05],\n",
      "        [9.9859e-01, 9.9989e-01, 9.9934e-01, 9.9669e-01, 9.9970e-01, 9.9987e-01,\n",
      "         9.9994e-01, 9.9980e-01, 9.9362e-01, 9.9987e-01, 9.9963e-01, 9.9215e-01,\n",
      "         9.9200e-01, 9.8087e-01, 9.9987e-01],\n",
      "        [4.3062e-05, 3.7652e-03, 1.2774e-04, 4.8326e-06, 1.2520e-03, 2.6768e-04,\n",
      "         1.8172e-04, 1.3445e-06, 7.5206e-04, 6.0435e-06, 1.1650e-04, 3.7774e-05,\n",
      "         3.0952e-04, 6.3615e-05, 6.4773e-05],\n",
      "        [6.5895e-02, 9.9815e-01, 7.6592e-01, 2.2519e-01, 9.8055e-01, 9.8013e-01,\n",
      "         6.5744e-01, 2.0939e-03, 9.5126e-01, 1.3359e-02, 8.9195e-01, 1.6827e-01,\n",
      "         7.5369e-01, 8.0727e-01, 3.6423e-01],\n",
      "        [9.5297e-01, 9.9812e-01, 9.9863e-01, 9.9879e-01, 9.3463e-01, 9.3444e-01,\n",
      "         9.6395e-01, 6.3532e-01, 7.4558e-01, 5.7100e-01, 8.8150e-01, 9.8961e-01,\n",
      "         9.2669e-01, 9.9458e-01, 9.7459e-01],\n",
      "        [9.5395e-01, 9.9937e-01, 9.8170e-01, 7.7480e-01, 9.9820e-01, 9.9982e-01,\n",
      "         9.9716e-01, 9.1017e-01, 9.8649e-01, 9.6116e-01, 9.9907e-01, 8.1188e-01,\n",
      "         8.4317e-01, 9.0301e-01, 9.8592e-01],\n",
      "        [9.9829e-01, 9.9970e-01, 9.9879e-01, 9.9120e-01, 9.9979e-01, 9.9976e-01,\n",
      "         9.9997e-01, 9.9998e-01, 9.9416e-01, 9.9998e-01, 9.9978e-01, 9.9599e-01,\n",
      "         9.9049e-01, 9.7408e-01, 9.9996e-01],\n",
      "        [3.4994e-01, 9.9598e-01, 9.2521e-01, 5.1082e-01, 9.9874e-01, 9.9553e-01,\n",
      "         9.7103e-01, 5.2990e-02, 9.9566e-01, 2.6528e-01, 9.8424e-01, 9.5895e-01,\n",
      "         9.8610e-01, 9.6468e-01, 8.0553e-01],\n",
      "        [6.9471e-01, 9.9756e-01, 9.5118e-01, 7.7497e-01, 9.9439e-01, 9.9683e-01,\n",
      "         9.2992e-01, 2.4101e-01, 9.5543e-01, 4.9841e-01, 9.8623e-01, 7.2478e-01,\n",
      "         8.8066e-01, 8.7712e-01, 9.0556e-01],\n",
      "        [5.6711e-03, 3.8199e-01, 1.8516e-01, 1.2177e-01, 2.0630e-01, 3.6141e-02,\n",
      "         1.7384e-02, 3.5329e-04, 3.8484e-02, 1.9228e-03, 7.4079e-03, 3.7149e-02,\n",
      "         1.9137e-01, 1.9682e-01, 4.7192e-02],\n",
      "        [9.9956e-01, 9.9995e-01, 9.9999e-01, 9.9994e-01, 9.9998e-01, 9.9983e-01,\n",
      "         9.9993e-01, 9.9873e-01, 9.9989e-01, 9.9950e-01, 9.9993e-01, 9.9999e-01,\n",
      "         9.9999e-01, 9.9994e-01, 9.9979e-01],\n",
      "        [1.1175e-01, 8.0488e-01, 7.6347e-01, 9.0752e-01, 6.5404e-02, 2.7606e-02,\n",
      "         3.1414e-02, 4.0757e-03, 1.7579e-02, 6.1864e-03, 1.1168e-02, 8.0394e-02,\n",
      "         7.5162e-02, 6.5349e-01, 1.2565e-01],\n",
      "        [8.9141e-02, 8.8264e-01, 4.0716e-01, 5.0792e-02, 9.7742e-01, 8.3099e-01,\n",
      "         6.2551e-01, 2.9348e-02, 9.5991e-01, 1.9700e-01, 8.0084e-01, 2.5584e-01,\n",
      "         7.9942e-01, 4.1836e-01, 4.6180e-01]], grad_fn=<SelectBackward0>)\n",
      "M0 tensor([[9.9844e-01, 2.8859e-01, 4.1041e-01, 8.2609e-01, 7.5058e-01, 2.8139e-01,\n",
      "         5.3077e-01, 9.9115e-01, 3.9325e-01, 5.6667e-02, 9.4548e-01, 3.3046e-01,\n",
      "         1.2439e-01, 8.0289e-01],\n",
      "        [2.0705e-03, 1.3163e-03, 1.9589e-01, 5.4146e-01, 5.6274e-01, 1.0469e-02,\n",
      "         3.2873e-01, 9.1029e-01, 3.4705e-02, 2.7493e-04, 1.3946e-02, 4.3960e-02,\n",
      "         9.5575e-05, 3.1262e-01],\n",
      "        [2.2672e-01, 3.0323e-01, 9.8100e-01, 8.6789e-01, 9.4844e-01, 3.7591e-01,\n",
      "         9.8041e-01, 9.5495e-01, 9.2777e-01, 4.6407e-01, 3.6235e-01, 8.4678e-01,\n",
      "         8.2406e-02, 9.8615e-01],\n",
      "        [7.9200e-03, 2.5621e-03, 7.1595e-01, 8.6777e-02, 5.6447e-02, 1.0754e-02,\n",
      "         7.9202e-01, 5.0869e-02, 7.6241e-02, 2.1904e-02, 3.4682e-03, 2.4277e-02,\n",
      "         1.5923e-03, 4.4636e-01],\n",
      "        [1.1808e-01, 6.4832e-01, 9.7294e-01, 9.9178e-01, 9.9182e-01, 9.3847e-01,\n",
      "         9.8543e-01, 9.9709e-01, 9.6963e-01, 3.6886e-01, 6.9977e-01, 9.8806e-01,\n",
      "         1.6918e-01, 9.6633e-01],\n",
      "        [5.9861e-01, 5.8150e-01, 9.6040e-01, 9.9704e-01, 9.9480e-01, 9.4315e-01,\n",
      "         9.7679e-01, 9.9991e-01, 9.4457e-01, 4.8125e-01, 9.7931e-01, 9.5513e-01,\n",
      "         1.3079e-01, 9.8342e-01],\n",
      "        [9.8039e-01, 9.9848e-01, 9.9747e-01, 9.9972e-01, 9.9958e-01, 9.9941e-01,\n",
      "         9.9782e-01, 9.9996e-01, 9.9671e-01, 9.4947e-01, 9.9949e-01, 9.9737e-01,\n",
      "         9.9326e-01, 9.9751e-01],\n",
      "        [9.5711e-01, 9.9983e-01, 6.9446e-01, 9.1358e-01, 9.1410e-01, 9.9287e-01,\n",
      "         5.8820e-01, 9.9364e-01, 8.6506e-01, 6.5582e-01, 9.8369e-01, 6.2883e-01,\n",
      "         9.9920e-01, 7.0970e-01],\n",
      "        [8.0556e-02, 9.6174e-02, 9.2298e-01, 9.8314e-01, 9.7940e-01, 7.0012e-01,\n",
      "         9.6512e-01, 9.9291e-01, 8.1396e-01, 1.0929e-01, 4.9619e-01, 9.8633e-01,\n",
      "         1.2267e-02, 9.6639e-01],\n",
      "        [9.8218e-01, 9.9989e-01, 9.6424e-01, 9.9273e-01, 9.9156e-01, 9.9911e-01,\n",
      "         9.6832e-01, 9.9962e-01, 9.8964e-01, 9.2587e-01, 9.9834e-01, 9.7786e-01,\n",
      "         9.9945e-01, 9.6285e-01],\n",
      "        [8.9052e-01, 9.8600e-01, 9.9642e-01, 9.9985e-01, 9.9977e-01, 9.9771e-01,\n",
      "         9.9915e-01, 9.9999e-01, 9.9744e-01, 9.4678e-01, 9.9082e-01, 9.9577e-01,\n",
      "         8.9748e-01, 9.9509e-01],\n",
      "        [9.8591e-01, 9.7548e-01, 9.9956e-01, 9.9895e-01, 9.9917e-01, 9.8924e-01,\n",
      "         9.9936e-01, 9.9962e-01, 9.9933e-01, 9.9939e-01, 9.7182e-01, 9.9677e-01,\n",
      "         9.5319e-01, 9.9986e-01],\n",
      "        [3.0079e-01, 1.8214e-01, 9.9939e-01, 9.9275e-01, 9.8882e-01, 7.5465e-01,\n",
      "         9.9811e-01, 9.8636e-01, 9.9893e-01, 8.9346e-01, 3.4404e-01, 9.8237e-01,\n",
      "         6.2657e-02, 9.9951e-01],\n",
      "        [2.6590e-01, 7.7648e-02, 9.9515e-01, 9.7765e-01, 9.7378e-01, 3.9567e-01,\n",
      "         9.9164e-01, 9.6565e-01, 9.5348e-01, 3.4720e-01, 1.9629e-01, 9.4066e-01,\n",
      "         1.8565e-02, 9.9577e-01],\n",
      "        [9.6279e-01, 9.9123e-01, 9.9810e-01, 9.9727e-01, 9.9418e-01, 9.9664e-01,\n",
      "         9.9701e-01, 9.9919e-01, 9.9741e-01, 8.9529e-01, 9.9676e-01, 9.8439e-01,\n",
      "         9.7196e-01, 9.9751e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.1221e-01, 4.2573e-02, 5.3368e-02, 1.2315e-02, 4.7851e-01, 2.7294e-02,\n",
      "         1.6417e-01, 3.9094e-02, 6.0487e-02, 3.0874e-02, 1.1350e-01, 1.6731e-02,\n",
      "         1.8506e-01, 7.1477e-02, 6.4869e-01],\n",
      "        [4.8758e-01, 9.0643e-01, 1.3100e-01, 9.9564e-01, 9.7194e-01, 9.8139e-01,\n",
      "         9.9859e-01, 9.3369e-02, 6.4900e-01, 4.8063e-01, 9.7141e-01, 2.4487e-01,\n",
      "         9.2863e-01, 8.0638e-01, 9.9386e-01],\n",
      "        [4.1587e-02, 9.9516e-01, 9.3336e-01, 7.3858e-03, 7.8082e-01, 4.3435e-01,\n",
      "         9.7333e-02, 8.8427e-01, 9.9307e-01, 6.5785e-01, 6.5047e-02, 9.9004e-01,\n",
      "         8.9433e-01, 9.8514e-01, 8.8327e-01],\n",
      "        [4.5030e-04, 1.7939e-01, 3.2692e-03, 7.6876e-05, 7.2316e-02, 4.4054e-03,\n",
      "         1.7566e-03, 2.0017e-03, 7.2255e-02, 1.2015e-03, 3.5293e-03, 7.3346e-03,\n",
      "         4.6128e-02, 8.1104e-02, 1.3201e-01],\n",
      "        [8.8239e-04, 8.0001e-01, 3.5503e-02, 2.5951e-03, 5.7444e-01, 1.8085e-01,\n",
      "         3.8608e-03, 2.0236e-02, 4.9092e-01, 2.2571e-02, 4.5057e-03, 1.3771e-01,\n",
      "         1.9165e-01, 5.3981e-01, 3.4279e-01],\n",
      "        [2.1373e-01, 9.6698e-01, 1.6900e-01, 7.2479e-01, 9.2441e-01, 8.7855e-01,\n",
      "         9.7556e-01, 1.0731e-01, 8.4627e-01, 3.8761e-01, 9.6123e-01, 4.5708e-01,\n",
      "         9.6902e-01, 9.3104e-01, 9.8456e-01],\n",
      "        [1.5735e-04, 3.4899e-01, 2.2506e-01, 5.1433e-05, 3.2922e-02, 3.3324e-03,\n",
      "         1.5586e-04, 7.8587e-02, 2.4632e-01, 8.7717e-03, 2.0666e-04, 2.1045e-01,\n",
      "         1.6507e-02, 1.4263e-01, 1.8874e-02],\n",
      "        [5.0996e-04, 8.5053e-02, 7.9157e-04, 1.4083e-04, 4.8639e-02, 6.5013e-03,\n",
      "         5.9427e-03, 3.9292e-04, 1.3482e-02, 5.9352e-04, 7.6145e-03, 3.5600e-03,\n",
      "         2.7174e-02, 2.0650e-02, 1.2448e-01],\n",
      "        [3.6919e-03, 8.0262e-01, 2.9813e-02, 1.9269e-03, 2.5436e-01, 3.6987e-02,\n",
      "         4.3276e-02, 1.0564e-02, 7.4977e-01, 2.1668e-02, 1.1935e-02, 3.1896e-01,\n",
      "         4.6132e-01, 5.1779e-01, 4.4167e-01],\n",
      "        [8.2744e-01, 9.9835e-01, 9.7182e-01, 8.6420e-01, 9.9716e-01, 9.6797e-01,\n",
      "         9.0831e-01, 9.5728e-01, 9.9568e-01, 9.9957e-01, 8.5104e-01, 9.9837e-01,\n",
      "         9.9704e-01, 9.9815e-01, 9.9753e-01],\n",
      "        [5.1053e-01, 4.9728e-01, 7.9693e-02, 1.0666e-01, 5.2470e-01, 4.8033e-01,\n",
      "         9.2543e-01, 3.4198e-02, 1.6898e-01, 5.4538e-02, 9.4170e-01, 7.2685e-02,\n",
      "         6.2292e-01, 3.0649e-01, 9.4885e-01],\n",
      "        [5.5278e-03, 9.4887e-01, 3.4626e-02, 2.6845e-02, 5.8583e-01, 3.4566e-01,\n",
      "         3.8044e-02, 1.5581e-02, 7.6919e-01, 5.7437e-02, 3.1686e-02, 4.0411e-01,\n",
      "         9.1414e-01, 9.2170e-01, 5.2925e-01],\n",
      "        [9.4863e-01, 9.8281e-01, 7.3981e-01, 9.9971e-01, 9.9886e-01, 9.9842e-01,\n",
      "         9.9973e-01, 6.4870e-01, 8.7429e-01, 9.7106e-01, 9.9712e-01, 8.3583e-01,\n",
      "         9.8786e-01, 9.7885e-01, 9.9903e-01],\n",
      "        [2.5558e-04, 3.4345e-01, 6.9520e-03, 9.0263e-06, 8.6989e-03, 4.2023e-04,\n",
      "         2.3318e-04, 2.4116e-03, 2.6258e-01, 2.2871e-03, 1.6745e-04, 7.4435e-02,\n",
      "         5.4817e-02, 1.9939e-01, 2.9859e-02]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[2.9319e-05],\n",
      "        [1.8408e-04],\n",
      "        [5.2396e-08],\n",
      "        [2.8940e-02],\n",
      "        [3.9243e-06],\n",
      "        [1.9537e-03],\n",
      "        [2.9995e-04],\n",
      "        [4.6944e-08],\n",
      "        [4.9314e-04],\n",
      "        [1.5663e-03],\n",
      "        [1.3057e-04],\n",
      "        [4.2989e-03],\n",
      "        [3.6067e-07],\n",
      "        [1.0478e-03],\n",
      "        [8.5253e-09]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "Ad tensor([[9.9978e-01, 5.2097e-01, 1.8662e-03, 2.2997e-02, 3.7156e-01],\n",
      "        [3.6154e-02, 9.9970e-01, 1.7538e-02, 8.0026e-01, 2.0140e-05],\n",
      "        [3.3709e-02, 7.2144e-01, 9.9987e-01, 9.9999e-01, 6.1700e-05],\n",
      "        [9.6662e-01, 9.7686e-01, 1.0000e+00, 1.0000e+00, 4.0100e-01],\n",
      "        [2.3549e-01, 5.9674e-02, 4.7967e-04, 5.7758e-04, 9.9998e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 0.9998],\n",
      "        [0.9999, 1.0000, 1.0000, 0.9999],\n",
      "        [0.9997, 0.9987, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 0.9996]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9996, 0.9987, 0.9999],\n",
      "        [0.9999, 1.0000, 0.9985, 1.0000],\n",
      "        [0.9864, 0.9982, 0.9944, 1.0000],\n",
      "        [0.9138, 0.8209, 1.0000, 0.9981]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9872, 0.9905, 0.9872, 0.9689],\n",
      "        [0.9827, 1.0000, 0.5189, 0.9971, 0.7455],\n",
      "        [0.9754, 0.8984, 1.0000, 0.7188, 1.0000],\n",
      "        [0.8930, 0.7950, 0.9409, 1.0000, 0.9525]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9988e-01, 8.4128e-01, 3.9300e-02, 7.4950e-01],\n",
      "        [8.0851e-02, 4.1235e-01, 9.9207e-01, 3.8360e-01],\n",
      "        [5.4478e-06, 9.4449e-01, 3.2283e-07, 2.8287e-05],\n",
      "        [1.0873e-04, 1.3881e-05, 3.2182e-05, 9.9989e-01],\n",
      "        [1.4920e-05, 9.9732e-01, 1.1600e-06, 2.7347e-05]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9319e-01, 1.1054e-02, 4.1245e-02, 1.7420e-03, 1.9852e-04, 5.4711e-02],\n",
      "        [2.8338e-05, 8.4914e-05, 7.5055e-04, 1.1368e-04, 2.0968e-05, 9.9730e-01],\n",
      "        [1.3198e-02, 2.6169e-01, 8.7687e-01, 9.9992e-01, 9.9399e-01, 1.8959e-02],\n",
      "        [4.5780e-07, 3.8601e-01, 8.9013e-01, 3.6397e-08, 1.0293e-08, 1.9625e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9998e-01, 8.3760e-01, 5.9712e-02, 9.8423e-02, 9.3583e-01],\n",
      "        [3.6319e-02, 9.9991e-01, 1.7936e-01, 1.0150e-02, 9.0826e-04],\n",
      "        [8.5451e-02, 9.9969e-01, 7.7680e-01, 3.1048e-01, 1.1139e-03],\n",
      "        [1.6199e-01, 7.6444e-01, 9.9999e-01, 9.9995e-01, 1.0558e-02],\n",
      "        [9.7807e-01, 9.9935e-01, 1.0000e+00, 1.0000e+00, 5.6058e-01],\n",
      "        [2.5727e-03, 6.2552e-04, 2.5070e-04, 5.3449e-05, 1.0000e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9978e-01, 5.2097e-01, 1.8662e-03, 2.2997e-02, 3.7156e-01],\n",
      "        [3.6154e-02, 9.9970e-01, 1.7538e-02, 8.0026e-01, 2.0140e-05],\n",
      "        [3.3709e-02, 7.2144e-01, 9.9987e-01, 9.9999e-01, 6.1700e-05],\n",
      "        [9.6662e-01, 9.7686e-01, 1.0000e+00, 1.0000e+00, 4.0100e-01],\n",
      "        [2.3549e-01, 5.9674e-02, 4.7967e-04, 5.7758e-04, 9.9998e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.6660e-01, 4.6262e-03, 1.3441e-04, 3.1106e-05, 1.0213e-03, 1.9743e-02,\n",
      "         2.1213e-02],\n",
      "        [2.3679e-06, 3.5045e-08, 2.5902e-07, 1.0242e-06, 3.4726e-01, 9.4066e-01,\n",
      "         8.7784e-01],\n",
      "        [1.2737e-01, 1.0521e-03, 1.0000e+00, 9.9996e-01, 9.9686e-01, 9.1303e-01,\n",
      "         6.4152e-01],\n",
      "        [4.9823e-06, 1.0926e-07, 8.5013e-01, 9.1931e-01, 9.9212e-01, 5.7933e-01,\n",
      "         2.5573e-01],\n",
      "        [9.9958e-01, 1.0000e+00, 2.1479e-03, 5.2408e-04, 1.1433e-02, 7.6762e-01,\n",
      "         9.1971e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9976e-01, 1.4903e-01, 8.9706e-06, 1.3784e-04, 3.4251e-05, 2.9703e-02],\n",
      "        [9.9993e-01, 1.0000e+00, 6.3449e-03, 1.5478e-01, 1.2093e-01, 9.9927e-01],\n",
      "        [7.6417e-01, 1.2823e-02, 9.9995e-01, 9.9999e-01, 9.9998e-01, 9.7363e-01],\n",
      "        [9.9861e-01, 8.2740e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [1.8274e-09, 3.1560e-10, 1.9521e-06, 1.6458e-05, 6.6582e-07, 3.0143e-06],\n",
      "        [2.1250e-06, 8.2266e-08, 1.9090e-06, 7.5222e-05, 1.5602e-06, 2.2798e-05],\n",
      "        [2.6663e-07, 1.8444e-08, 3.0229e-07, 4.1217e-06, 2.8326e-07, 2.1633e-06]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.9848e-01, 9.9944e-01, 1.0000e+00, 9.9997e-01],\n",
      "        [9.9994e-01, 2.3467e-02, 1.4941e-01, 1.0000e+00, 9.7693e-01],\n",
      "        [9.9999e-01, 1.0000e+00, 1.0000e+00, 2.3613e-02, 1.0000e+00],\n",
      "        [9.9778e-01, 1.0000e+00, 1.0000e+00, 7.2621e-04, 1.0000e+00],\n",
      "        [9.9681e-01, 1.0000e+00, 1.0000e+00, 5.2249e-04, 1.0000e+00],\n",
      "        [9.9948e-01, 9.9998e-01, 9.9997e-01, 9.6382e-01, 9.9990e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9998e-01, 2.5167e-01, 9.5334e-01, 9.1254e-01, 9.8324e-01],\n",
      "        [9.9151e-01, 4.2390e-04, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [9.5939e-01, 7.6713e-05, 1.0000e+00, 9.9998e-01, 1.0000e+00],\n",
      "        [9.9998e-01, 1.0000e+00, 3.7879e-01, 4.8501e-02, 8.7098e-02],\n",
      "        [9.9818e-01, 4.3099e-03, 9.9999e-01, 9.9999e-01, 1.0000e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.9752, 0.9609, 1.0000, 0.8722],\n",
      "        [1.0000, 0.9998, 1.0000, 1.0000, 0.9973, 1.0000],\n",
      "        [0.9999, 0.9967, 1.0000, 0.9999, 0.9803, 1.0000],\n",
      "        [0.9979, 0.7295, 1.0000, 0.9997, 0.2360, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.9976, 1.0000, 1.0000, 0.9911, 0.9999],\n",
      "        [1.0000, 1.0000, 0.9998, 0.9997, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.9998, 1.0000, 1.0000, 0.9990, 1.0000],\n",
      "        [1.0000, 1.0000, 0.9732, 0.9491, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 0.9963, 0.9998, 0.9998, 0.9998, 0.9998, 0.9975, 0.9995],\n",
      "        [1.0000, 1.0000, 0.9783, 0.9994, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.9795, 1.0000, 1.0000, 1.0000, 0.9748, 0.9850, 0.9918],\n",
      "        [1.0000, 0.9900, 1.0000, 1.0000, 1.0000, 0.9904, 0.9925, 0.9968],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.9484, 1.0000, 1.0000, 0.8797, 0.9174],\n",
      "        [1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9457, 0.9993, 1.0000, 1.0000, 0.8459, 0.2800, 0.3541],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9995, 0.9999],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.8953, 1.0000, 0.9998, 0.4405, 0.9898, 0.7444],\n",
      "        [0.9277, 1.0000, 1.0000, 0.9480, 0.9992, 0.7477],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.2072, 0.9318, 0.9999, 0.9872, 0.9982, 0.9999, 0.7923, 0.2457],\n",
      "        [0.9956, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9992],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 0.9997, 0.9994, 0.9999, 0.9995, 0.9994, 1.0000, 1.0000]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.2946e-01, 5.7887e-01, 8.2584e-01, 4.8592e-01, 4.4320e-01, 8.3745e-01],\n",
      "        [3.2797e-04, 5.8255e-05, 6.2001e-02, 1.0222e-01, 8.8587e-02, 6.8399e-01],\n",
      "        [7.6338e-01, 3.5508e-01, 9.9970e-01, 9.4991e-01, 9.9884e-01, 9.9681e-01],\n",
      "        [1.6647e-01, 5.2411e-02, 9.9513e-01, 9.9185e-01, 9.9807e-01, 9.9902e-01],\n",
      "        [9.3119e-01, 4.9563e-01, 9.9999e-01, 9.9967e-01, 9.9996e-01, 9.9990e-01],\n",
      "        [5.2244e-03, 7.2133e-04, 2.8184e-01, 1.5161e-02, 1.5168e-01, 3.5461e-01],\n",
      "        [3.4030e-01, 4.4253e-02, 9.7327e-01, 9.4904e-01, 9.7837e-01, 9.9328e-01],\n",
      "        [1.5158e-01, 9.9550e-01, 7.6356e-01, 3.1855e-01, 4.0047e-01, 5.5758e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.4991e-01, 1.0206e-01, 5.5760e-02, 1.0241e-01, 4.7378e-01, 1.7172e-01,\n",
      "         1.1639e-01],\n",
      "        [5.4175e-01, 1.1376e-02, 2.4935e-03, 9.6089e-01, 5.3576e-02, 7.1506e-03,\n",
      "         1.0877e-02],\n",
      "        [2.6940e-01, 9.5200e-01, 9.7451e-01, 8.1917e-03, 8.8874e-01, 6.6849e-01,\n",
      "         8.1827e-01],\n",
      "        [3.6140e-02, 7.4305e-02, 3.5764e-01, 4.4380e-04, 3.6306e-01, 2.7164e-01,\n",
      "         4.6025e-01],\n",
      "        [4.1382e-01, 9.5089e-01, 9.6032e-01, 1.0155e-02, 9.6616e-01, 9.5567e-01,\n",
      "         9.7845e-01],\n",
      "        [2.4256e-03, 1.2793e-02, 1.3287e-02, 1.0873e-04, 2.1448e-01, 5.5173e-02,\n",
      "         4.8009e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9981e-01, 9.9370e-01, 9.9029e-01, 8.4562e-01, 9.9974e-01, 8.3058e-01,\n",
      "         9.5915e-01, 9.8415e-01],\n",
      "        [9.8096e-01, 1.0000e+00, 9.9994e-01, 9.9999e-01, 1.0000e+00, 1.4681e-01,\n",
      "         2.6435e-01, 9.9991e-01],\n",
      "        [7.9979e-01, 9.9531e-01, 8.6121e-01, 9.8005e-01, 9.9880e-01, 1.9266e-02,\n",
      "         5.0926e-02, 9.8672e-01],\n",
      "        [5.2200e-02, 1.5802e-03, 7.2941e-03, 2.0923e-04, 8.0989e-03, 9.6965e-01,\n",
      "         9.8681e-01, 1.2417e-03],\n",
      "        [7.0888e-01, 9.8964e-01, 9.5634e-01, 8.4434e-01, 9.9983e-01, 9.5933e-03,\n",
      "         7.3591e-03, 9.4918e-01],\n",
      "        [4.3274e-01, 9.9642e-01, 9.4865e-01, 8.1855e-01, 9.9925e-01, 1.1330e-02,\n",
      "         1.3308e-02, 9.6203e-01],\n",
      "        [4.4502e-01, 8.8390e-01, 1.0821e-01, 8.4411e-01, 9.9770e-01, 7.2674e-04,\n",
      "         4.3841e-03, 5.8387e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9965e-01, 5.0857e-01, 5.2786e-01, 8.8544e-01, 9.0855e-01, 7.6201e-01,\n",
      "         9.2419e-01, 4.3725e-01, 7.3286e-01, 5.8292e-01],\n",
      "        [9.1934e-01, 9.9791e-01, 9.8959e-01, 9.9990e-01, 9.9965e-01, 9.9422e-01,\n",
      "         1.8877e-01, 1.6599e-01, 9.9946e-01, 6.1907e-01],\n",
      "        [9.9701e-01, 9.9875e-01, 9.9971e-01, 9.9997e-01, 9.9998e-01, 9.9558e-01,\n",
      "         9.9201e-01, 9.9852e-01, 9.9993e-01, 9.9530e-01],\n",
      "        [1.0060e-06, 5.5653e-03, 1.9739e-05, 1.4114e-02, 3.6763e-03, 4.1580e-03,\n",
      "         1.9084e-07, 1.0520e-08, 1.8521e-03, 1.7844e-06],\n",
      "        [4.9192e-02, 5.6988e-01, 1.5626e-02, 9.4571e-01, 8.9170e-01, 4.3936e-01,\n",
      "         2.1138e-04, 2.2119e-04, 9.4863e-01, 4.9537e-03],\n",
      "        [5.7471e-04, 8.6721e-07, 6.8483e-05, 1.0989e-05, 3.2083e-05, 1.2834e-06,\n",
      "         1.7213e-01, 2.6929e-01, 9.1514e-05, 5.2229e-02],\n",
      "        [3.4431e-01, 2.1465e-04, 1.7169e-03, 4.9255e-04, 2.9817e-03, 3.3511e-04,\n",
      "         8.3823e-01, 8.7199e-01, 6.5888e-03, 7.5245e-01],\n",
      "        [9.9682e-01, 9.9863e-01, 9.9989e-01, 9.9999e-01, 9.9998e-01, 9.9894e-01,\n",
      "         9.5097e-01, 9.8570e-01, 9.9997e-01, 9.8828e-01]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[9.9983e-01, 4.7032e-01, 9.6492e-01, 2.0622e-01, 1.7346e-01, 9.8123e-01,\n",
      "         5.7989e-01, 7.1439e-01, 6.9255e-01, 2.9864e-01, 2.1755e-01, 3.0590e-01],\n",
      "        [8.0194e-03, 3.8054e-01, 5.4063e-05, 9.9554e-01, 1.6964e-01, 3.4805e-04,\n",
      "         1.3947e-05, 9.6639e-01, 9.3929e-01, 9.9697e-01, 9.7814e-01, 9.9566e-01],\n",
      "        [9.5533e-01, 9.9753e-01, 9.1047e-01, 9.9456e-01, 9.8331e-01, 9.8168e-01,\n",
      "         7.5580e-01, 9.9897e-01, 9.5105e-01, 9.7234e-01, 9.7834e-01, 9.8384e-01],\n",
      "        [1.8872e-01, 9.9624e-01, 8.3918e-03, 9.9970e-01, 9.9487e-01, 1.5812e-01,\n",
      "         3.4860e-03, 9.9915e-01, 9.9079e-01, 9.9412e-01, 9.9971e-01, 9.9937e-01],\n",
      "        [9.2145e-03, 8.2357e-01, 1.5724e-04, 8.7755e-01, 2.2234e-01, 1.8055e-03,\n",
      "         2.9487e-05, 9.7385e-01, 7.5113e-01, 3.8883e-01, 9.7873e-01, 8.9305e-01],\n",
      "        [5.5139e-02, 7.5691e-01, 8.0887e-05, 9.8325e-01, 3.1841e-01, 5.6903e-04,\n",
      "         1.5464e-04, 9.5777e-01, 9.7692e-01, 9.7875e-01, 9.8303e-01, 9.6581e-01],\n",
      "        [1.7890e-01, 1.4828e-03, 9.9238e-01, 9.8740e-05, 2.4770e-02, 9.8791e-01,\n",
      "         9.7124e-01, 8.5531e-03, 2.1345e-02, 1.6388e-04, 1.3231e-04, 1.2441e-04],\n",
      "        [8.1718e-01, 1.3918e-01, 9.9937e-01, 2.3787e-03, 7.1045e-01, 9.9960e-01,\n",
      "         9.9886e-01, 6.0269e-02, 3.0273e-02, 5.6075e-04, 8.9893e-03, 1.2814e-03],\n",
      "        [1.9128e-04, 3.6413e-02, 1.7004e-05, 4.6586e-01, 4.7152e-02, 1.3089e-04,\n",
      "         1.1010e-05, 3.8273e-01, 1.3012e-02, 5.8291e-03, 3.1453e-01, 1.1378e-01],\n",
      "        [2.7817e-01, 6.6803e-02, 9.7967e-01, 1.3696e-02, 3.6365e-01, 9.6524e-01,\n",
      "         9.7561e-01, 4.2976e-01, 1.9753e-01, 1.3939e-02, 2.9809e-02, 9.3172e-03]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.8565e-01, 1.2206e-01, 1.1854e-01, 8.5768e-01, 7.5797e-01, 1.1606e-01,\n",
      "         1.2285e-02, 3.5678e-02, 2.9408e-02, 3.6100e-03, 3.6993e-02, 3.9294e-02,\n",
      "         9.6614e-01],\n",
      "        [9.8838e-01, 9.9997e-01, 9.7014e-01, 9.9201e-01, 1.0000e+00, 9.9999e-01,\n",
      "         9.9919e-01, 9.9998e-01, 9.9993e-01, 9.9666e-01, 9.9729e-01, 9.9963e-01,\n",
      "         9.9968e-01],\n",
      "        [7.0166e-03, 1.9035e-02, 6.5174e-01, 9.4893e-01, 1.8253e-03, 7.9805e-04,\n",
      "         1.3829e-05, 1.0357e-04, 3.1873e-05, 2.1630e-05, 1.8569e-03, 2.0478e-04,\n",
      "         9.3117e-01],\n",
      "        [8.9879e-01, 9.9014e-01, 5.0047e-02, 4.8087e-01, 9.9999e-01, 9.9998e-01,\n",
      "         9.9986e-01, 9.9977e-01, 1.0000e+00, 9.9843e-01, 9.9996e-01, 1.0000e+00,\n",
      "         8.3828e-01],\n",
      "        [1.9501e-01, 9.8420e-01, 8.4712e-01, 9.8227e-01, 9.9996e-01, 9.9999e-01,\n",
      "         7.5384e-01, 9.9681e-01, 9.8643e-01, 7.4560e-01, 9.9942e-01, 9.8804e-01,\n",
      "         9.9704e-01],\n",
      "        [8.3828e-01, 9.9116e-01, 9.9916e-01, 9.9996e-01, 8.8720e-01, 9.2944e-01,\n",
      "         2.9385e-02, 1.4276e-01, 6.6913e-02, 6.1129e-02, 6.9871e-01, 2.5507e-01,\n",
      "         9.9993e-01],\n",
      "        [7.6100e-02, 3.1467e-01, 9.8460e-01, 9.9798e-01, 6.9329e-02, 5.0343e-03,\n",
      "         3.0749e-05, 9.6667e-04, 5.8547e-05, 3.4755e-04, 1.7601e-02, 2.3823e-04,\n",
      "         9.8760e-01],\n",
      "        [5.6113e-04, 2.2297e-02, 5.1513e-05, 2.9445e-04, 4.4218e-01, 3.6211e-01,\n",
      "         1.7559e-02, 4.7733e-02, 1.9069e-01, 4.3305e-03, 7.0751e-02, 5.8247e-01,\n",
      "         1.6569e-03],\n",
      "        [1.3479e-03, 5.1684e-01, 1.2100e-04, 2.1973e-03, 6.5951e-01, 4.2652e-01,\n",
      "         6.0552e-01, 9.5243e-01, 6.1499e-01, 3.0485e-01, 4.1824e-01, 8.4381e-01,\n",
      "         5.0921e-03],\n",
      "        [2.0818e-01, 5.8022e-01, 8.5120e-05, 4.2965e-03, 9.9006e-01, 9.3216e-01,\n",
      "         9.9990e-01, 9.9919e-01, 9.9990e-01, 9.9907e-01, 9.9761e-01, 9.9997e-01,\n",
      "         7.2891e-03],\n",
      "        [1.8864e-02, 1.4119e-01, 3.5835e-04, 1.1219e-03, 9.9848e-01, 9.9992e-01,\n",
      "         8.0730e-01, 9.9054e-01, 9.9745e-01, 3.8787e-01, 9.4861e-01, 9.9543e-01,\n",
      "         5.2117e-02],\n",
      "        [1.0948e-01, 5.9153e-01, 4.1757e-04, 4.6295e-03, 9.8252e-01, 9.9347e-01,\n",
      "         9.9980e-01, 9.9843e-01, 9.9990e-01, 9.9578e-01, 9.9553e-01, 9.9995e-01,\n",
      "         1.5703e-02]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9996e-01, 2.1671e-02, 3.5507e-02, 4.0438e-02, 1.6182e-02, 7.4913e-03,\n",
      "         8.6815e-01, 1.1665e-01, 4.0768e-03, 4.2306e-03, 7.1917e-02, 8.5857e-01,\n",
      "         1.3478e-01],\n",
      "        [6.8378e-01, 9.5837e-01, 4.7336e-01, 9.9992e-01, 9.5498e-01, 9.8406e-01,\n",
      "         9.3988e-01, 9.9308e-01, 9.7247e-01, 9.5923e-01, 8.3294e-01, 9.9579e-01,\n",
      "         9.7687e-01],\n",
      "        [9.9963e-01, 8.3329e-02, 4.3810e-02, 9.9920e-01, 9.2741e-02, 9.5023e-01,\n",
      "         9.2750e-01, 3.4763e-01, 9.9915e-01, 4.0525e-01, 4.2636e-01, 9.9999e-01,\n",
      "         9.1807e-01],\n",
      "        [9.9955e-01, 3.5710e-02, 2.2361e-02, 9.8980e-01, 7.7119e-02, 8.3250e-01,\n",
      "         8.9980e-01, 2.1354e-01, 9.7837e-01, 3.6272e-01, 2.0762e-01, 9.9960e-01,\n",
      "         7.6746e-01],\n",
      "        [8.5377e-01, 9.1917e-01, 4.5592e-01, 7.4412e-01, 4.8245e-01, 3.4344e-02,\n",
      "         9.9997e-01, 6.6903e-01, 8.0553e-04, 4.6901e-03, 7.6319e-01, 7.4163e-03,\n",
      "         9.2969e-02],\n",
      "        [6.4681e-02, 5.3973e-01, 6.0145e-01, 7.1056e-02, 5.1931e-01, 3.5402e-02,\n",
      "         9.9800e-01, 3.4365e-01, 3.7687e-04, 4.3048e-04, 8.2944e-02, 2.1081e-03,\n",
      "         5.5645e-02],\n",
      "        [3.8599e-02, 9.7896e-01, 8.8328e-01, 1.8164e-02, 9.9164e-01, 2.1149e-01,\n",
      "         8.4793e-01, 9.9811e-01, 1.7080e-04, 9.6796e-01, 9.9154e-01, 1.6759e-04,\n",
      "         9.6148e-01],\n",
      "        [3.2837e-05, 9.2386e-02, 1.4378e-02, 1.5039e-04, 2.2173e-01, 4.5700e-03,\n",
      "         2.4876e-02, 2.1593e-01, 8.5593e-06, 5.8205e-02, 4.4781e-02, 2.8028e-06,\n",
      "         8.5267e-04],\n",
      "        [5.5650e-01, 9.9981e-01, 9.9492e-01, 4.4344e-01, 9.8911e-01, 2.0156e-01,\n",
      "         9.9991e-01, 9.9977e-01, 4.0485e-04, 3.5878e-01, 9.9868e-01, 8.2518e-03,\n",
      "         9.8363e-01],\n",
      "        [9.6871e-01, 9.9989e-01, 9.9746e-01, 9.7098e-01, 9.9931e-01, 9.2602e-01,\n",
      "         9.9392e-01, 1.0000e+00, 1.2716e-02, 9.9971e-01, 9.9994e-01, 4.0672e-01,\n",
      "         9.9973e-01],\n",
      "        [4.2075e-03, 7.2866e-01, 5.6362e-01, 3.4878e-03, 3.6233e-02, 5.7407e-03,\n",
      "         1.7084e-01, 9.1582e-01, 1.2318e-05, 1.4038e-01, 8.5266e-01, 5.0799e-04,\n",
      "         3.9259e-02],\n",
      "        [6.2050e-02, 9.8067e-01, 9.3629e-01, 3.0026e-03, 4.8501e-01, 8.8198e-03,\n",
      "         8.9387e-01, 9.7102e-01, 1.3284e-05, 7.7465e-03, 9.4384e-01, 1.0840e-04,\n",
      "         6.3296e-01],\n",
      "        [9.9555e-01, 6.4119e-02, 2.4049e-02, 9.8192e-01, 1.5757e-01, 6.8172e-01,\n",
      "         9.9550e-01, 1.1627e-01, 9.1284e-01, 1.3545e-01, 7.1002e-02, 9.9776e-01,\n",
      "         4.2951e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[9.9955e-01, 6.7177e-01, 8.0048e-01, 3.4044e-01, 2.7213e-01, 7.9694e-01,\n",
      "         9.5047e-01, 8.9855e-03, 3.0464e-02, 7.4332e-01, 8.1652e-01, 9.2459e-01,\n",
      "         9.3611e-01, 7.1287e-01, 3.2754e-01],\n",
      "        [6.4463e-01, 8.8093e-01, 9.9221e-01, 9.9951e-01, 9.9831e-01, 1.6448e-01,\n",
      "         4.5960e-01, 8.4385e-01, 5.8878e-01, 9.5814e-01, 9.9900e-01, 9.9967e-01,\n",
      "         9.9741e-01, 9.9973e-01, 9.9746e-01],\n",
      "        [4.7312e-03, 8.9395e-04, 3.8608e-03, 2.4504e-01, 4.1424e-02, 3.7585e-05,\n",
      "         2.3098e-04, 2.9358e-03, 4.4906e-04, 8.0805e-03, 6.4804e-02, 2.3123e-01,\n",
      "         1.9648e-01, 5.1533e-01, 5.5726e-03],\n",
      "        [3.7669e-02, 9.3339e-01, 5.0298e-03, 3.0888e-02, 6.8974e-01, 2.9362e-01,\n",
      "         6.4489e-01, 1.7710e-04, 9.5497e-04, 3.1622e-02, 6.4562e-02, 2.2246e-01,\n",
      "         9.1780e-01, 3.4475e-02, 6.8636e-01],\n",
      "        [2.9173e-02, 1.9940e-01, 6.3421e-02, 8.1749e-01, 9.0063e-01, 1.6396e-04,\n",
      "         2.8109e-03, 2.7426e-01, 2.6216e-02, 2.9462e-01, 8.6448e-01, 6.3391e-01,\n",
      "         9.9195e-01, 9.4409e-01, 4.2245e-01],\n",
      "        [5.7186e-01, 9.9503e-01, 2.7816e-01, 9.5080e-01, 9.8455e-01, 4.7931e-01,\n",
      "         9.2629e-01, 4.8213e-01, 2.4307e-01, 9.0737e-01, 9.3073e-01, 8.8435e-01,\n",
      "         9.9879e-01, 9.0689e-01, 9.9278e-01],\n",
      "        [6.8668e-04, 3.0573e-05, 1.0308e-04, 6.7980e-04, 2.4760e-03, 2.0738e-06,\n",
      "         4.3434e-05, 2.3413e-05, 2.9700e-06, 7.8832e-04, 4.1348e-03, 7.7820e-02,\n",
      "         2.8929e-02, 9.9877e-03, 9.0467e-05],\n",
      "        [2.9228e-01, 3.0221e-01, 8.3486e-01, 9.9082e-01, 9.9008e-01, 2.9330e-03,\n",
      "         8.4606e-03, 4.0227e-01, 2.4631e-01, 4.0048e-01, 9.6981e-01, 9.1468e-01,\n",
      "         9.5072e-01, 9.8851e-01, 8.4873e-01],\n",
      "        [3.1557e-03, 8.8623e-01, 2.6420e-03, 3.2018e-03, 5.1884e-02, 1.7875e-01,\n",
      "         8.4259e-01, 2.6269e-04, 1.4614e-03, 1.3544e-02, 1.6095e-02, 4.8014e-02,\n",
      "         3.1703e-01, 4.3135e-03, 9.0384e-02],\n",
      "        [1.5983e-01, 6.7151e-01, 9.4587e-01, 9.8934e-01, 9.7216e-01, 1.5670e-01,\n",
      "         2.3162e-01, 9.0704e-01, 9.7378e-01, 7.9209e-01, 9.9694e-01, 9.1407e-01,\n",
      "         9.1565e-01, 9.8930e-01, 9.8148e-01],\n",
      "        [5.6940e-01, 8.8078e-02, 9.8696e-01, 9.9863e-01, 9.6818e-01, 2.1311e-02,\n",
      "         3.8325e-02, 6.9114e-01, 4.8247e-01, 9.0882e-01, 9.9600e-01, 9.9360e-01,\n",
      "         9.4616e-01, 9.9902e-01, 8.8223e-01],\n",
      "        [1.6183e-02, 2.0638e-01, 2.0997e-03, 8.5475e-04, 5.1356e-04, 7.3842e-01,\n",
      "         8.9394e-01, 2.2839e-03, 2.9291e-03, 2.9242e-03, 2.5276e-03, 9.0283e-02,\n",
      "         7.6002e-02, 4.2817e-03, 1.2026e-01],\n",
      "        [3.0205e-02, 5.5947e-02, 8.5577e-01, 7.9690e-01, 9.0461e-01, 1.5065e-03,\n",
      "         1.3918e-03, 8.1773e-02, 2.5592e-01, 8.2698e-03, 9.0043e-01, 7.3243e-01,\n",
      "         5.5425e-01, 9.2504e-01, 2.1757e-01]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.8755e-01, 9.9310e-01, 9.5081e-01, 8.7104e-01, 9.9519e-01,\n",
      "         9.1023e-01, 9.1529e-01, 8.6670e-01, 9.5676e-01, 9.7038e-01, 9.9021e-01,\n",
      "         9.9654e-01, 9.5789e-01],\n",
      "        [9.3329e-02, 6.6526e-01, 1.0207e-01, 9.1961e-04, 3.1684e-02, 5.5387e-01,\n",
      "         2.3186e-03, 1.2944e-03, 1.4100e-03, 2.4161e-01, 4.7622e-03, 5.6911e-01,\n",
      "         9.7195e-03, 1.3929e-01],\n",
      "        [9.9813e-01, 9.9858e-01, 9.9824e-01, 9.9967e-01, 9.9978e-01, 9.9624e-01,\n",
      "         9.9365e-01, 9.9914e-01, 9.9967e-01, 9.9154e-01, 9.9999e-01, 9.9702e-01,\n",
      "         9.9995e-01, 9.9384e-01],\n",
      "        [9.2829e-01, 9.4939e-01, 9.6077e-01, 9.9091e-01, 9.8055e-01, 9.7191e-01,\n",
      "         8.1608e-01, 9.9263e-01, 9.8196e-01, 9.2805e-01, 9.9986e-01, 9.7158e-01,\n",
      "         9.9958e-01, 9.8446e-01],\n",
      "        [1.1087e-02, 2.2394e-01, 1.5507e-01, 2.0040e-01, 1.4254e-01, 1.5277e-02,\n",
      "         7.2910e-04, 3.6114e-02, 3.2265e-02, 4.5221e-02, 3.4146e-01, 1.2837e-02,\n",
      "         2.9599e-01, 1.2521e-01],\n",
      "        [9.9841e-01, 9.9268e-01, 9.9410e-01, 6.9288e-01, 9.9878e-01, 1.0000e+00,\n",
      "         9.9391e-01, 9.6841e-01, 9.8716e-01, 9.7276e-01, 6.6881e-01, 9.9986e-01,\n",
      "         9.9048e-01, 9.7197e-01],\n",
      "        [9.8210e-01, 9.4474e-01, 9.1262e-01, 4.6602e-02, 9.5970e-01, 1.0000e+00,\n",
      "         8.5915e-01, 5.2844e-01, 9.3942e-01, 9.3800e-01, 8.0779e-02, 9.9949e-01,\n",
      "         9.3772e-01, 9.3394e-01],\n",
      "        [3.4538e-02, 2.5059e-01, 6.0843e-01, 8.4252e-02, 8.9014e-01, 5.1190e-01,\n",
      "         8.5572e-01, 7.8048e-01, 9.3704e-01, 8.4176e-01, 7.3305e-01, 3.0681e-01,\n",
      "         7.7607e-01, 2.3555e-01],\n",
      "        [9.9547e-01, 9.9989e-01, 9.9990e-01, 9.9925e-01, 1.0000e+00, 9.9988e-01,\n",
      "         9.9999e-01, 9.9992e-01, 9.9999e-01, 9.9995e-01, 9.9999e-01, 9.9982e-01,\n",
      "         9.9971e-01, 9.9939e-01],\n",
      "        [9.9104e-01, 9.0643e-01, 8.9392e-01, 3.0525e-01, 6.4912e-01, 8.8088e-01,\n",
      "         2.9472e-01, 9.9284e-01, 9.9295e-01, 9.9812e-01, 9.4538e-01, 9.8989e-01,\n",
      "         9.9994e-01, 9.9972e-01],\n",
      "        [4.2676e-01, 6.7537e-01, 4.2839e-01, 7.2820e-01, 8.0072e-01, 4.3858e-01,\n",
      "         6.2759e-02, 7.8873e-01, 9.5087e-01, 5.4202e-01, 9.8654e-01, 5.6358e-01,\n",
      "         9.9345e-01, 7.3388e-01],\n",
      "        [8.8676e-01, 9.4432e-01, 9.6015e-01, 7.2613e-01, 8.6772e-01, 9.9368e-01,\n",
      "         4.0893e-01, 9.8642e-01, 9.9364e-01, 9.9303e-01, 9.9400e-01, 9.3555e-01,\n",
      "         9.9996e-01, 9.9162e-01],\n",
      "        [7.5915e-03, 1.0748e-01, 7.8382e-02, 1.3581e-04, 1.4744e-03, 2.5157e-02,\n",
      "         1.6015e-03, 2.3670e-03, 3.3615e-04, 7.6969e-01, 7.1374e-03, 2.8901e-03,\n",
      "         2.5891e-01, 2.1939e-03],\n",
      "        [2.3996e-03, 1.7149e-02, 1.1160e-02, 9.0182e-02, 1.3122e-02, 2.2125e-02,\n",
      "         9.3464e-04, 4.5416e-02, 6.5101e-02, 1.0936e-02, 6.9952e-01, 3.4144e-03,\n",
      "         5.4612e-01, 5.0183e-02],\n",
      "        [7.3865e-01, 9.5333e-01, 9.8300e-01, 1.6265e-01, 9.5626e-01, 9.9376e-01,\n",
      "         3.8582e-01, 9.7798e-01, 9.4781e-01, 9.9538e-01, 9.5443e-01, 9.9366e-01,\n",
      "         9.9916e-01, 9.9157e-01]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[1.0000, 0.9990, 0.9700, 0.9905, 0.9934, 0.9963, 0.9687, 0.9868, 0.9955,\n",
      "         0.9917, 0.9912, 0.9818, 0.9923],\n",
      "        [0.9997, 0.9992, 1.0000, 0.9999, 0.9998, 0.9999, 1.0000, 0.9998, 0.9997,\n",
      "         0.9998, 1.0000, 0.9998, 0.9997],\n",
      "        [0.9982, 0.9959, 0.9998, 0.9959, 0.9997, 1.0000, 0.9999, 0.9990, 0.9995,\n",
      "         0.9995, 1.0000, 0.9999, 0.9984],\n",
      "        [0.0137, 0.0041, 0.0721, 0.0013, 0.0111, 0.0165, 0.1070, 0.0095, 0.0389,\n",
      "         0.0043, 0.0059, 0.0051, 0.0021],\n",
      "        [0.9399, 0.9215, 0.9994, 0.9071, 0.9971, 0.9945, 0.9998, 0.9852, 0.9966,\n",
      "         0.9885, 0.9985, 0.9985, 0.9244],\n",
      "        [0.9999, 1.0000, 1.0000, 0.9996, 0.9995, 0.9999, 1.0000, 1.0000, 0.9999,\n",
      "         0.9999, 0.9999, 1.0000, 0.9999],\n",
      "        [0.9999, 0.9990, 1.0000, 0.9962, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998,\n",
      "         0.9998, 1.0000, 1.0000, 0.9997],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.5436, 0.9536, 0.9983, 0.8756, 0.9827, 0.9685, 0.9993, 0.9900, 0.9862,\n",
      "         0.9957, 0.9766, 0.9774, 0.6683],\n",
      "        [0.9995, 0.9991, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 0.9994,\n",
      "         0.9999, 1.0000, 0.9999, 0.9999],\n",
      "        [0.7278, 0.8166, 0.9971, 0.9653, 0.9903, 0.9943, 0.9984, 0.9799, 0.9949,\n",
      "         0.9637, 0.9933, 0.9553, 0.9405],\n",
      "        [0.9993, 0.9999, 0.9997, 0.9985, 0.9990, 0.9979, 0.9998, 0.9991, 0.9999,\n",
      "         0.9997, 0.9984, 0.9998, 0.9984],\n",
      "        [0.9870, 0.9995, 0.9986, 1.0000, 0.9938, 0.9931, 0.9997, 0.9965, 0.9985,\n",
      "         0.9983, 0.9991, 0.9630, 0.9985],\n",
      "        [0.9726, 0.9390, 0.9771, 0.9877, 0.9601, 0.9310, 0.9800, 0.9942, 0.9944,\n",
      "         0.9989, 0.9944, 0.6120, 0.9906]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[0.9994, 0.7900, 0.7017, 0.8736, 0.9659, 0.8381, 0.8404, 0.9350, 0.9335,\n",
      "         0.9565, 0.8912, 0.9418, 0.9424],\n",
      "        [0.9605, 0.7851, 0.8311, 0.9695, 0.8761, 0.7532, 0.9674, 0.9444, 0.9741,\n",
      "         0.9990, 0.8981, 0.8953, 0.9981],\n",
      "        [0.3951, 0.7137, 0.9841, 0.9910, 0.9376, 0.9018, 0.9260, 0.8090, 0.9781,\n",
      "         0.9039, 0.7237, 0.9412, 0.6197],\n",
      "        [0.9984, 0.9939, 0.9980, 0.9991, 0.9998, 0.9995, 0.9982, 0.9992, 0.9968,\n",
      "         1.0000, 0.9960, 0.9913, 0.9977],\n",
      "        [0.9383, 0.9407, 0.9941, 0.9960, 0.9894, 0.9867, 0.9871, 0.9845, 0.9969,\n",
      "         0.9935, 0.9581, 0.9845, 0.9111],\n",
      "        [0.9927, 0.9771, 0.9977, 0.9993, 0.9989, 0.9993, 0.9996, 0.9993, 0.9989,\n",
      "         0.9975, 0.9976, 0.9998, 0.9941],\n",
      "        [0.0739, 0.0475, 0.6902, 0.6927, 0.8012, 0.6950, 0.4771, 0.1763, 0.5298,\n",
      "         0.3240, 0.2156, 0.4998, 0.1247],\n",
      "        [0.8644, 0.5197, 0.7855, 0.9839, 0.9558, 0.9569, 0.9415, 0.9355, 0.9654,\n",
      "         0.9827, 0.9508, 0.9353, 0.9562],\n",
      "        [0.2298, 0.1296, 0.5668, 0.4958, 0.4711, 0.7443, 0.2683, 0.7491, 0.9192,\n",
      "         0.6132, 0.3770, 0.2791, 0.1725],\n",
      "        [0.7865, 0.3167, 0.8803, 0.9860, 0.9345, 0.9690, 0.9120, 0.9308, 0.9823,\n",
      "         0.9710, 0.8888, 0.6419, 0.7900],\n",
      "        [0.8125, 0.3160, 0.8213, 0.9363, 0.9954, 0.9940, 0.9538, 0.9755, 0.9465,\n",
      "         0.9318, 0.9357, 0.9600, 0.6277],\n",
      "        [0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9865, 0.9220, 0.9437, 0.9864, 0.9968, 0.9952, 0.9835, 0.9972, 0.9880,\n",
      "         0.9994, 0.9913, 0.9770, 0.9811]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[0.9999, 0.9983, 0.9972, 0.9985, 0.9984, 0.9985, 0.9972, 0.9974, 0.9972,\n",
      "         0.9982, 0.9972, 0.9984, 0.9962],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9997, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 0.9999, 1.0000],\n",
      "        [0.4607, 0.9915, 0.8949, 0.9891, 0.6545, 0.9765, 0.9429, 0.9597, 0.9598,\n",
      "         0.8496, 0.9395, 0.7834, 0.9397],\n",
      "        [0.9744, 0.9993, 0.9834, 0.9956, 0.9761, 0.9968, 0.9931, 0.9946, 0.9967,\n",
      "         0.9987, 0.9962, 0.9881, 0.9943],\n",
      "        [0.9986, 1.0000, 0.9999, 0.9999, 0.9997, 0.9992, 1.0000, 0.9999, 0.9999,\n",
      "         0.9998, 0.9993, 0.9999, 0.9999],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.9548, 0.9958, 0.9966, 0.9947, 0.9993, 0.9937, 0.9983, 0.9998, 0.9902,\n",
      "         0.9790, 0.9907, 0.9912, 0.9943],\n",
      "        [0.9918, 0.9982, 0.9987, 0.9979, 0.9990, 1.0000, 0.9967, 0.9954, 0.9957,\n",
      "         0.9953, 0.9944, 0.9978, 0.9981],\n",
      "        [0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999,\n",
      "         0.9999, 0.9997, 1.0000, 1.0000],\n",
      "        [0.9951, 0.9998, 0.9980, 0.9998, 0.9985, 0.9984, 0.9990, 0.9994, 0.9994,\n",
      "         0.9983, 0.9999, 0.9978, 0.9981],\n",
      "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         0.9999, 0.9999, 1.0000, 1.0000]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.6922e-02],\n",
      "        [1.1193e-03],\n",
      "        [1.1611e-03],\n",
      "        [1.4038e-03],\n",
      "        [2.4641e-03],\n",
      "        [4.3194e-05],\n",
      "        [1.6489e-06],\n",
      "        [2.3350e-04],\n",
      "        [2.4342e-04],\n",
      "        [1.8290e-03],\n",
      "        [2.5393e-02],\n",
      "        [1.9523e-05],\n",
      "        [3.6262e-03]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#recon+testerror\n",
    "for r in range(1,3):\n",
    "    src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=r)\n",
    "\n",
    "    #print(src1.size())\n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "    #transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "    transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "    Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    #print('L',val_loss)\n",
    "    a=0.1\n",
    "    #print(Ad)\n",
    "    #pp_A = complete_postprocess(Ad,d,a)\n",
    "    \n",
    "    #err_p=err_perc(pp_A,y)\n",
    "    #print('err',r,err_p)\n",
    "\n",
    "#print(src1.size())\n",
    "\n",
    "    print('y',y[6])\n",
    "    print('Ad',Ad[6])\n",
    "    #print('pp',pp_A[6])\n",
    "    #print('d',d[6])\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "    make_reconstructed_edgelist(Ad,run=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=95\n",
    "t= 8\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:]#[1:]\n",
    "src_t2=src_t2[:,t,:]#[1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:]#[1:]\n",
    "b=b[:,t,:]#[1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine',n_neighbors=4)\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
