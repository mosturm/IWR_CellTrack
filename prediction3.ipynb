{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,75) ##100 total data size\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(75,100)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    m_Ad = []\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        m_Ad.append(np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy()))\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ad = postprocess_MinCostAss(m_Ad,a)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2912,  0.7213, -0.2201,  ...,  0.8745,  0.8745,  0.8745],\n",
       "         [-0.5672, -0.1294, -0.4315,  ...,  0.8745,  0.8745,  0.8745],\n",
       "         ...,\n",
       "         [ 0.0420,  0.6094, -0.6926,  ...,  0.8667,  0.8745,  0.8745],\n",
       "         [-0.1685,  0.1963, -0.4332,  ...,  0.8745,  0.8745,  0.8745],\n",
       "         [-0.0815,  0.1846, -0.2623,  ...,  0.8745,  0.8745,  0.8745]]),\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2423,  0.3597,  0.0149,  ...,  0.8784,  0.8745,  0.8784],\n",
       "         [ 0.0688,  0.7041, -0.6287,  ...,  0.8745,  0.8745,  0.8745],\n",
       "         ...,\n",
       "         [-0.5149, -0.1651, -0.4450,  ...,  0.8745,  0.8745,  0.8745],\n",
       "         [ 0.2054, -0.0890, -0.2332,  ...,  0.8745,  0.8745,  0.8745],\n",
       "         [ 0.1949,  0.5957, -0.0874,  ...,  0.8745,  0.8745,  0.8745]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000],\n",
       "         [1.0000, 1.0000, 0.2641, 1.0000, 0.0516, 0.0677, 0.3235, 0.1128, 0.1355,\n",
       "          1.0000],\n",
       "         [1.0000, 0.3037, 0.0690, 0.1127, 0.1210, 0.1303, 0.2991, 0.9501, 0.0784,\n",
       "          0.1625],\n",
       "         [1.0000, 0.6391, 0.1255, 0.1882, 0.1397, 0.2296, 1.0000, 0.3561, 0.2089,\n",
       "          0.2280],\n",
       "         [1.0000, 1.0000, 0.1933, 0.4439, 0.0772, 0.1079, 0.9456, 0.1942, 0.1753,\n",
       "          0.7428],\n",
       "         [1.0000, 0.1321, 0.1973, 0.1304, 0.0629, 0.1118, 0.2395, 0.0628, 1.0000,\n",
       "          0.1003],\n",
       "         [1.0000, 0.7651, 0.6997, 1.0000, 0.0403, 0.0542, 0.2159, 0.0717, 0.1590,\n",
       "          1.0000],\n",
       "         [1.0000, 0.1797, 0.0551, 0.0797, 0.2096, 0.1987, 0.2574, 1.0000, 0.0736,\n",
       "          0.1033],\n",
       "         [1.0000, 0.2471, 1.0000, 0.6501, 0.0385, 0.0561, 0.1864, 0.0533, 0.3921,\n",
       "          0.2889],\n",
       "         [1.0000, 0.0623, 0.0350, 0.0395, 1.0000, 1.0000, 0.1175, 0.2202, 0.0660,\n",
       "          0.0425],\n",
       "         [1.0000, 0.1125, 0.0552, 0.0637, 0.8508, 1.0000, 0.2952, 0.3430, 0.1210,\n",
       "          0.0685]], dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = True, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05):\n",
    "        super(AdjacencyTransformer_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = src_t1\n",
    "        src2_emb = src_t2\n",
    "        #print('src1',src1_emb.size())\n",
    "        #print('src2',src2_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        #print('out1',out1.size())\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        #print('out_dec1',out_dec1.size())\n",
    "        out_dec1=self.lin2(out_dec1)\n",
    "        #print('out_dec1b',out_dec1.size())\n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        #print('z',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 6\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.886, Val loss: 4.515, Epoch time = 4.761s\n",
      "Epoch: 2, Train loss: 4.030, Val loss: 4.184, Epoch time = 5.001s\n",
      "Epoch: 3, Train loss: 4.274, Val loss: 4.702, Epoch time = 5.245s\n",
      "Epoch: 4, Train loss: 4.435, Val loss: 4.088, Epoch time = 4.789s\n",
      "Epoch: 5, Train loss: 4.806, Val loss: 3.661, Epoch time = 4.276s\n",
      "Epoch: 6, Train loss: 3.991, Val loss: 5.047, Epoch time = 4.185s\n",
      "Epoch: 7, Train loss: 4.093, Val loss: 4.606, Epoch time = 4.295s\n",
      "Epoch: 8, Train loss: 5.204, Val loss: 4.205, Epoch time = 3.966s\n",
      "Epoch: 9, Train loss: 3.486, Val loss: 5.300, Epoch time = 5.888s\n",
      "Epoch: 10, Train loss: 4.620, Val loss: 3.985, Epoch time = 4.018s\n",
      "Epoch: 11, Train loss: 4.515, Val loss: 3.480, Epoch time = 4.081s\n",
      "Epoch: 12, Train loss: 3.816, Val loss: 3.947, Epoch time = 3.815s\n",
      "Epoch: 13, Train loss: 4.252, Val loss: 4.929, Epoch time = 3.856s\n",
      "Epoch: 14, Train loss: 4.499, Val loss: 4.234, Epoch time = 3.933s\n",
      "Epoch: 15, Train loss: 3.810, Val loss: 4.103, Epoch time = 3.784s\n",
      "Epoch: 16, Train loss: 4.028, Val loss: 3.982, Epoch time = 5.004s\n",
      "Epoch: 17, Train loss: 4.172, Val loss: 3.855, Epoch time = 3.979s\n",
      "Epoch: 18, Train loss: 5.001, Val loss: 4.006, Epoch time = 3.722s\n",
      "Epoch: 19, Train loss: 3.539, Val loss: 4.737, Epoch time = 4.041s\n",
      "Epoch: 20, Train loss: 4.606, Val loss: 4.191, Epoch time = 3.853s\n",
      "Epoch: 21, Train loss: 3.454, Val loss: 3.644, Epoch time = 4.116s\n",
      "Epoch: 22, Train loss: 4.555, Val loss: 3.843, Epoch time = 3.647s\n",
      "Epoch: 23, Train loss: 5.201, Val loss: 4.157, Epoch time = 3.946s\n",
      "Epoch: 24, Train loss: 4.363, Val loss: 5.371, Epoch time = 3.808s\n",
      "Epoch: 25, Train loss: 4.164, Val loss: 4.217, Epoch time = 3.971s\n",
      "Epoch: 26, Train loss: 5.555, Val loss: 3.621, Epoch time = 3.620s\n",
      "Epoch: 27, Train loss: 3.920, Val loss: 4.654, Epoch time = 3.805s\n",
      "Epoch: 28, Train loss: 4.674, Val loss: 3.636, Epoch time = 3.759s\n",
      "Epoch: 29, Train loss: 4.197, Val loss: 4.076, Epoch time = 3.538s\n",
      "Epoch: 30, Train loss: 5.355, Val loss: 3.814, Epoch time = 3.764s\n",
      "Epoch: 31, Train loss: 4.461, Val loss: 4.765, Epoch time = 3.679s\n",
      "Epoch: 32, Train loss: 4.685, Val loss: 3.459, Epoch time = 3.729s\n",
      "Epoch: 33, Train loss: 5.318, Val loss: 4.421, Epoch time = 3.812s\n",
      "Epoch: 34, Train loss: 4.303, Val loss: 3.927, Epoch time = 3.874s\n",
      "Epoch: 35, Train loss: 3.982, Val loss: 3.632, Epoch time = 3.789s\n",
      "Epoch: 36, Train loss: 3.248, Val loss: 3.777, Epoch time = 3.704s\n",
      "Epoch: 37, Train loss: 4.424, Val loss: 4.020, Epoch time = 3.805s\n",
      "Epoch: 38, Train loss: 4.532, Val loss: 4.337, Epoch time = 3.800s\n",
      "Epoch: 39, Train loss: 3.967, Val loss: 4.450, Epoch time = 3.719s\n",
      "Epoch: 40, Train loss: 3.659, Val loss: 3.918, Epoch time = 4.052s\n",
      "Epoch: 41, Train loss: 3.754, Val loss: 4.826, Epoch time = 3.865s\n",
      "Epoch: 42, Train loss: 3.698, Val loss: 5.157, Epoch time = 3.757s\n",
      "Epoch: 43, Train loss: 5.112, Val loss: 5.162, Epoch time = 3.812s\n",
      "Epoch: 44, Train loss: 3.733, Val loss: 4.810, Epoch time = 3.798s\n",
      "Epoch: 45, Train loss: 4.286, Val loss: 3.706, Epoch time = 4.028s\n",
      "Epoch: 46, Train loss: 4.677, Val loss: 3.612, Epoch time = 3.636s\n",
      "Epoch: 47, Train loss: 4.359, Val loss: 4.445, Epoch time = 3.790s\n",
      "Epoch: 48, Train loss: 3.502, Val loss: 4.479, Epoch time = 3.914s\n",
      "Epoch: 49, Train loss: 4.507, Val loss: 4.252, Epoch time = 3.728s\n",
      "Epoch: 50, Train loss: 5.086, Val loss: 4.340, Epoch time = 3.623s\n",
      "Epoch: 51, Train loss: 3.704, Val loss: 4.550, Epoch time = 3.848s\n",
      "Epoch: 52, Train loss: 3.436, Val loss: 4.492, Epoch time = 3.628s\n",
      "Epoch: 53, Train loss: 3.818, Val loss: 3.929, Epoch time = 3.747s\n",
      "Epoch: 54, Train loss: 3.700, Val loss: 4.743, Epoch time = 3.946s\n",
      "Epoch: 55, Train loss: 4.762, Val loss: 5.126, Epoch time = 3.745s\n",
      "Epoch: 56, Train loss: 3.519, Val loss: 4.286, Epoch time = 3.891s\n",
      "Epoch: 57, Train loss: 3.995, Val loss: 4.281, Epoch time = 3.834s\n",
      "Epoch: 58, Train loss: 3.480, Val loss: 4.237, Epoch time = 3.974s\n",
      "Epoch: 59, Train loss: 3.413, Val loss: 3.211, Epoch time = 3.654s\n",
      "Epoch: 60, Train loss: 4.051, Val loss: 4.734, Epoch time = 3.905s\n",
      "Epoch: 61, Train loss: 6.070, Val loss: 6.086, Epoch time = 3.532s\n",
      "Epoch: 62, Train loss: 4.063, Val loss: 4.239, Epoch time = 3.824s\n",
      "Epoch: 63, Train loss: 4.853, Val loss: 3.973, Epoch time = 3.622s\n",
      "Epoch: 64, Train loss: 3.676, Val loss: 4.279, Epoch time = 3.840s\n",
      "Epoch: 65, Train loss: 3.689, Val loss: 3.738, Epoch time = 3.724s\n",
      "Epoch: 66, Train loss: 4.777, Val loss: 4.592, Epoch time = 3.772s\n",
      "Epoch: 67, Train loss: 3.995, Val loss: 3.401, Epoch time = 3.683s\n",
      "Epoch: 68, Train loss: 4.092, Val loss: 3.369, Epoch time = 3.803s\n",
      "Epoch: 69, Train loss: 3.894, Val loss: 4.471, Epoch time = 3.654s\n",
      "Epoch: 70, Train loss: 4.135, Val loss: 4.042, Epoch time = 3.854s\n",
      "Epoch: 71, Train loss: 3.877, Val loss: 4.397, Epoch time = 3.733s\n",
      "Epoch: 72, Train loss: 4.275, Val loss: 4.215, Epoch time = 3.921s\n",
      "Epoch: 73, Train loss: 3.631, Val loss: 4.310, Epoch time = 3.673s\n",
      "Epoch: 74, Train loss: 4.300, Val loss: 3.783, Epoch time = 3.501s\n",
      "Epoch: 75, Train loss: 4.972, Val loss: 4.301, Epoch time = 4.016s\n",
      "Epoch: 76, Train loss: 3.697, Val loss: 4.154, Epoch time = 3.802s\n",
      "Epoch: 77, Train loss: 4.074, Val loss: 3.900, Epoch time = 3.826s\n",
      "Epoch: 78, Train loss: 4.845, Val loss: 5.000, Epoch time = 3.837s\n",
      "Epoch: 79, Train loss: 4.248, Val loss: 3.713, Epoch time = 3.796s\n",
      "Epoch: 80, Train loss: 3.334, Val loss: 5.445, Epoch time = 3.702s\n",
      "Epoch: 81, Train loss: 4.068, Val loss: 4.087, Epoch time = 3.743s\n",
      "Epoch: 82, Train loss: 4.331, Val loss: 3.564, Epoch time = 3.849s\n",
      "Epoch: 83, Train loss: 4.580, Val loss: 4.992, Epoch time = 3.705s\n",
      "Epoch: 84, Train loss: 4.178, Val loss: 4.148, Epoch time = 3.761s\n",
      "Epoch: 85, Train loss: 5.190, Val loss: 4.691, Epoch time = 3.673s\n",
      "Epoch: 86, Train loss: 4.627, Val loss: 3.875, Epoch time = 3.704s\n",
      "Epoch: 87, Train loss: 4.122, Val loss: 3.395, Epoch time = 3.748s\n",
      "Epoch: 88, Train loss: 4.122, Val loss: 4.287, Epoch time = 4.010s\n",
      "Epoch: 89, Train loss: 3.972, Val loss: 4.220, Epoch time = 4.049s\n",
      "Epoch: 90, Train loss: 4.564, Val loss: 4.320, Epoch time = 3.754s\n",
      "Epoch: 91, Train loss: 3.888, Val loss: 4.223, Epoch time = 3.789s\n",
      "Epoch: 92, Train loss: 4.608, Val loss: 4.321, Epoch time = 3.728s\n",
      "Epoch: 93, Train loss: 3.934, Val loss: 3.806, Epoch time = 3.932s\n",
      "Epoch: 94, Train loss: 4.545, Val loss: 4.126, Epoch time = 3.542s\n",
      "Epoch: 95, Train loss: 3.411, Val loss: 3.589, Epoch time = 3.833s\n",
      "Epoch: 96, Train loss: 5.042, Val loss: 4.445, Epoch time = 3.702s\n",
      "Epoch: 97, Train loss: 3.754, Val loss: 3.507, Epoch time = 3.828s\n",
      "Epoch: 98, Train loss: 4.572, Val loss: 3.686, Epoch time = 3.870s\n",
      "Epoch: 99, Train loss: 4.568, Val loss: 3.974, Epoch time = 3.940s\n",
      "Epoch: 100, Train loss: 3.672, Val loss: 5.669, Epoch time = 3.617s\n",
      "Epoch: 101, Train loss: 5.182, Val loss: 4.494, Epoch time = 3.734s\n",
      "Epoch: 102, Train loss: 5.181, Val loss: 4.001, Epoch time = 3.893s\n",
      "Epoch: 103, Train loss: 4.104, Val loss: 4.920, Epoch time = 3.688s\n",
      "Epoch: 104, Train loss: 3.490, Val loss: 3.552, Epoch time = 3.797s\n",
      "Epoch: 105, Train loss: 4.305, Val loss: 4.534, Epoch time = 3.582s\n",
      "Epoch: 106, Train loss: 4.666, Val loss: 3.921, Epoch time = 3.549s\n",
      "Epoch: 107, Train loss: 4.578, Val loss: 4.656, Epoch time = 3.793s\n",
      "Epoch: 108, Train loss: 4.478, Val loss: 4.797, Epoch time = 3.532s\n",
      "Epoch: 109, Train loss: 4.510, Val loss: 4.139, Epoch time = 3.863s\n",
      "Epoch: 110, Train loss: 4.460, Val loss: 4.337, Epoch time = 3.719s\n",
      "Epoch: 111, Train loss: 4.014, Val loss: 3.451, Epoch time = 3.769s\n",
      "Epoch: 112, Train loss: 3.881, Val loss: 4.606, Epoch time = 3.704s\n",
      "Epoch: 113, Train loss: 3.998, Val loss: 4.678, Epoch time = 3.883s\n",
      "Epoch: 114, Train loss: 3.794, Val loss: 4.449, Epoch time = 3.843s\n",
      "Epoch: 115, Train loss: 3.831, Val loss: 5.181, Epoch time = 3.632s\n",
      "Epoch: 116, Train loss: 4.499, Val loss: 5.379, Epoch time = 4.133s\n",
      "Epoch: 117, Train loss: 3.667, Val loss: 4.419, Epoch time = 4.016s\n",
      "Epoch: 118, Train loss: 4.410, Val loss: 3.930, Epoch time = 3.843s\n",
      "Epoch: 119, Train loss: 3.852, Val loss: 5.055, Epoch time = 3.974s\n",
      "Epoch: 120, Train loss: 4.043, Val loss: 4.482, Epoch time = 3.768s\n",
      "Epoch: 121, Train loss: 3.414, Val loss: 4.279, Epoch time = 3.806s\n",
      "Epoch: 122, Train loss: 4.456, Val loss: 4.480, Epoch time = 3.857s\n",
      "Epoch: 123, Train loss: 4.827, Val loss: 4.926, Epoch time = 3.927s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train loss: 3.435, Val loss: 4.092, Epoch time = 3.852s\n",
      "Epoch: 125, Train loss: 3.179, Val loss: 3.267, Epoch time = 3.772s\n",
      "Epoch: 126, Train loss: 4.652, Val loss: 3.721, Epoch time = 3.674s\n",
      "Epoch: 127, Train loss: 3.835, Val loss: 4.603, Epoch time = 3.635s\n",
      "Epoch: 128, Train loss: 4.091, Val loss: 4.424, Epoch time = 3.930s\n",
      "Epoch: 129, Train loss: 4.030, Val loss: 4.282, Epoch time = 3.714s\n",
      "Epoch: 130, Train loss: 3.693, Val loss: 4.440, Epoch time = 3.795s\n",
      "Epoch: 131, Train loss: 3.775, Val loss: 4.306, Epoch time = 3.746s\n",
      "Epoch: 132, Train loss: 4.123, Val loss: 4.025, Epoch time = 3.644s\n",
      "Epoch: 133, Train loss: 4.000, Val loss: 4.128, Epoch time = 3.833s\n",
      "Epoch: 134, Train loss: 5.010, Val loss: 3.242, Epoch time = 3.877s\n",
      "Epoch: 135, Train loss: 4.659, Val loss: 4.128, Epoch time = 3.833s\n",
      "Epoch: 136, Train loss: 3.748, Val loss: 4.500, Epoch time = 3.924s\n",
      "Epoch: 137, Train loss: 4.040, Val loss: 4.303, Epoch time = 3.806s\n",
      "Epoch: 138, Train loss: 4.159, Val loss: 4.017, Epoch time = 3.948s\n",
      "Epoch: 139, Train loss: 3.741, Val loss: 4.035, Epoch time = 4.265s\n",
      "Epoch: 140, Train loss: 3.851, Val loss: 4.447, Epoch time = 4.353s\n",
      "Epoch: 141, Train loss: 5.075, Val loss: 4.410, Epoch time = 4.106s\n",
      "Epoch: 142, Train loss: 3.464, Val loss: 4.524, Epoch time = 3.873s\n",
      "Epoch: 143, Train loss: 3.419, Val loss: 3.962, Epoch time = 3.943s\n",
      "Epoch: 144, Train loss: 4.289, Val loss: 4.414, Epoch time = 3.962s\n",
      "Epoch: 145, Train loss: 4.505, Val loss: 3.757, Epoch time = 4.090s\n",
      "Epoch: 146, Train loss: 4.660, Val loss: 4.430, Epoch time = 3.947s\n",
      "Epoch: 147, Train loss: 3.325, Val loss: 4.192, Epoch time = 6.009s\n",
      "Epoch: 148, Train loss: 5.030, Val loss: 3.934, Epoch time = 3.999s\n",
      "Epoch: 149, Train loss: 4.081, Val loss: 4.107, Epoch time = 4.913s\n",
      "Epoch: 150, Train loss: 3.650, Val loss: 4.295, Epoch time = 4.080s\n",
      "Epoch: 151, Train loss: 3.801, Val loss: 4.293, Epoch time = 3.895s\n",
      "Epoch: 152, Train loss: 4.044, Val loss: 4.150, Epoch time = 3.896s\n",
      "Epoch: 153, Train loss: 4.431, Val loss: 4.505, Epoch time = 3.778s\n",
      "Epoch: 154, Train loss: 3.962, Val loss: 5.611, Epoch time = 3.812s\n",
      "Epoch: 155, Train loss: 3.701, Val loss: 4.736, Epoch time = 3.875s\n",
      "Epoch: 156, Train loss: 4.961, Val loss: 4.000, Epoch time = 3.654s\n",
      "Epoch: 157, Train loss: 3.423, Val loss: 4.490, Epoch time = 4.038s\n",
      "Epoch: 158, Train loss: 4.357, Val loss: 4.220, Epoch time = 3.788s\n",
      "Epoch: 159, Train loss: 3.398, Val loss: 4.167, Epoch time = 3.892s\n",
      "Epoch: 160, Train loss: 3.385, Val loss: 4.232, Epoch time = 4.009s\n",
      "Epoch: 161, Train loss: 3.957, Val loss: 3.820, Epoch time = 3.758s\n",
      "Epoch: 162, Train loss: 3.389, Val loss: 4.179, Epoch time = 4.007s\n",
      "Epoch: 163, Train loss: 4.512, Val loss: 4.694, Epoch time = 4.360s\n",
      "Epoch: 164, Train loss: 3.612, Val loss: 3.611, Epoch time = 4.045s\n",
      "Epoch: 165, Train loss: 3.384, Val loss: 3.780, Epoch time = 4.122s\n",
      "Epoch: 166, Train loss: 3.521, Val loss: 4.728, Epoch time = 3.818s\n",
      "Epoch: 167, Train loss: 3.600, Val loss: 4.263, Epoch time = 3.822s\n",
      "Epoch: 168, Train loss: 4.352, Val loss: 4.475, Epoch time = 4.023s\n",
      "Epoch: 169, Train loss: 4.255, Val loss: 3.750, Epoch time = 3.611s\n",
      "Epoch: 170, Train loss: 3.985, Val loss: 3.581, Epoch time = 3.979s\n",
      "Epoch: 171, Train loss: 5.093, Val loss: 4.546, Epoch time = 3.994s\n",
      "Epoch: 172, Train loss: 4.295, Val loss: 4.560, Epoch time = 4.152s\n",
      "Epoch: 173, Train loss: 3.506, Val loss: 3.562, Epoch time = 4.216s\n",
      "Epoch: 174, Train loss: 5.179, Val loss: 4.556, Epoch time = 3.761s\n",
      "Epoch: 175, Train loss: 4.220, Val loss: 4.231, Epoch time = 4.049s\n",
      "Epoch: 176, Train loss: 3.561, Val loss: 3.703, Epoch time = 3.728s\n",
      "Epoch: 177, Train loss: 3.234, Val loss: 4.061, Epoch time = 3.743s\n",
      "Epoch: 178, Train loss: 4.916, Val loss: 3.739, Epoch time = 3.640s\n",
      "Epoch: 179, Train loss: 4.716, Val loss: 4.169, Epoch time = 3.894s\n",
      "Epoch: 180, Train loss: 4.004, Val loss: 3.861, Epoch time = 3.818s\n",
      "Epoch: 181, Train loss: 3.389, Val loss: 3.749, Epoch time = 3.819s\n",
      "Epoch: 182, Train loss: 4.549, Val loss: 5.072, Epoch time = 3.891s\n",
      "Epoch: 183, Train loss: 3.605, Val loss: 4.060, Epoch time = 4.071s\n",
      "Epoch: 184, Train loss: 4.972, Val loss: 4.133, Epoch time = 3.886s\n",
      "Epoch: 185, Train loss: 4.882, Val loss: 3.306, Epoch time = 4.189s\n",
      "Epoch: 186, Train loss: 3.363, Val loss: 3.662, Epoch time = 3.780s\n",
      "Epoch: 187, Train loss: 3.595, Val loss: 4.594, Epoch time = 4.807s\n",
      "Epoch: 188, Train loss: 4.134, Val loss: 4.204, Epoch time = 4.037s\n",
      "Epoch: 189, Train loss: 4.658, Val loss: 4.722, Epoch time = 3.937s\n",
      "Epoch: 190, Train loss: 4.537, Val loss: 4.080, Epoch time = 3.921s\n",
      "Epoch: 191, Train loss: 4.671, Val loss: 4.070, Epoch time = 3.837s\n",
      "Epoch: 192, Train loss: 4.653, Val loss: 4.376, Epoch time = 3.750s\n",
      "Epoch: 193, Train loss: 4.052, Val loss: 3.691, Epoch time = 3.731s\n",
      "Epoch: 194, Train loss: 3.841, Val loss: 4.489, Epoch time = 4.010s\n",
      "Epoch: 195, Train loss: 4.467, Val loss: 4.052, Epoch time = 4.648s\n",
      "Epoch: 196, Train loss: 4.196, Val loss: 3.893, Epoch time = 3.712s\n",
      "Epoch: 197, Train loss: 3.232, Val loss: 3.330, Epoch time = 3.791s\n",
      "Epoch: 198, Train loss: 4.248, Val loss: 3.991, Epoch time = 3.805s\n",
      "Epoch: 199, Train loss: 3.707, Val loss: 5.565, Epoch time = 3.742s\n",
      "Epoch: 200, Train loss: 3.800, Val loss: 4.167, Epoch time = 3.907s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc04c8d2590>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydeViV1fbHP4tJBBFUcAIFB9DAWZxSU9NKyzIbNDNtMhusm3Ub7y1vt363+TbbaDZ4NbNJTVNLU3NWcAZFESdEBVTAiXn//tjn6OFwgAMyyv48D8973v3ud7/77V7P9+y11l5LlFIYDAaDwWCLS1VPwGAwGAzVDyMOBoPBYCiEEQeDwWAwFMKIg8FgMBgKYcTBYDAYDIVwq+oJlAf+/v4qJCSkqqdhMBgMNYro6OhUpVSAo2uXhTiEhIQQFRVV1dMwGAyGGoWIHCzqmjErGQwGg6EQRhwMBoPBUAinxEFEhopInIjEi8hzRfQZJSKxIhIjIrMsbcEiEi0iWy3tD9n07y4iOyxjfiAiYmlvKCJ/iMhey7FBebyowWAwGJynRHEQEVdgKjAMCAfGiEi4XZ9Q4Hmgr1IqAphsuXQUuFIp1QXoBTwnIs0t1z4BJgKhlr+hlvbngGVKqVBgmeXcYDAYDJWIMyuHnkC8UipBKZUNzAZG2PV5AJiqlDoFoJRKthyzlVJZlj51rM8TkWZAfaXUOqWTO30L3GzpNwL4xvL5G5t2g8FgMFQSzohDIHDY5jzR0mZLGBAmImtEZL2IWFcBiEgLEdluGeMNpVSS5f7EIsZsopQ6CmA5NnY0KRGZKCJRIhKVkpLixGsYDAaDwVmcEQdx0GafytUNbRoaCIwBpomIH4BS6rBSqhPQFrhbRJo4OWaxKKU+V0pFKqUiAwIchukaDAaDoYw4Iw6JQAub8yAgyUGfeUqpHKXUfiAOLRYXsKwYYoD+lv5BRYx53GJ2spqfkp17FUNtZEPCCVbtNStHg6G8cUYcNgGhItJKRDyAO4D5dn3mAoMARMQfbWZKEJEgEalraW8A9AXiLOai0yLS2xKlNB6YZxlrPnC35fPdNu0GQwHmbT3CndM2cPf0jSyJOVbV0zEYLitKFAelVC7wKLAE2AXMUUrFiMjLInKTpdsS4ISIxALLgaeVUieAK4ANIrINWAm8rZTaYbnnYWAaEA/sAxZZ2l8HrhGRvcA1lnNDJXI2K5fZGw+Rn199C0F9t/EQk7/fSmRwAzoF+fHYrC2siU912PdI2nnWFnHNYDA4Ri6HSnCRkZHKpM8oP979Yw/vL9vL/+7vRb9Q/6qeTiH+t/4gL8zdycB2AXx6V3cyc/IY/dl6Dp86x8wJveja8uLWmLx8xY0frmbXsQym392DQe0dxjcYDLUSEYlWSkU6umZ2SBsKkJ2bz6yNhwBYs6/6/do+n53HG4t306+tP5+Pi8TT3RU/Lw9m3N8T/3p1eODbKJJPZ17o/1N0IrFHM2jo5cHfZm/hQOrZKpy9wVBzMOJgKMCSmGOknM7Cp45bpZhilFL8a95OZlsEqSR+3ZbE6cxc/jY4FA+3i//3bVzfk2l3R3I6M5e/z9lGfr7ibFYub/0eR9eWfsyd1BdXF+HBGdGczcqtqNcxGC4bjDgYCvDtugO0bOjFPX1D2HEknfTzORX6vFV7U/lm3UGe+3kHn6zYV2L/mRsOEtq4Hj1CCmdVCWviw5Qbw1m1N5VpqxP4dOU+Uk5n8eLwcFo09OLDMV3Zm3ya537e4WBkg8FgixEHwwViktLZdOAU4/sE0z80gHwF6xNOlMvYSik+WbGPXUczCrR/vCKeJvXrMLxTM95YvJt3/thDUX6wHYnpbEtMZ2yvllhScRXizp4tGdahKW8ujuPzvxK4qXNzull8EP1DA5g8JIxftyUV6bw2GAwaIw6GC8xYdxBPdxdu796CLi38qOvuWm6mpeVxybyxeDcPfBtFRqZejWw+dIr1CSd5oH9r3r+jK7d1D+KDZXuZvuaAwzFmbTxIXXdXbuke5PA6gIjw+i2daFLfE4BnhrYrcH3iVa0J9KvLa4t2VetoLIOhqjHiYAAg/VwOc7ce4eYugfh6uePh5kLPVg1ZXU7i8MmKfTT09iAp7TwvzY+50Obn5c6Yni1xdRHevLUTfVo34stVCYW+uDMyc5i7JYmbOjenvqd7sc/y9XJn9sTezJ7Ym6AGXgWuebq78tR1Yew8ksGv2+33choMBitGHAwAfLfpEJk5+YzrE3yhrW/bRuxLOcux9Mxi7iyZTQdOsunAKf52dVsevTqUnzcf4d0/9vBH7HHu7hOCdx1dkNDFRRjTqyVJ6ZmsszNnzd1yhPM5eYzt3dKpZ7Zo6FUgpNWWEZ0DCW9Wn7eWxJGVm3dJ72YwXK4YcTCQmZPHl6v306+tPxHNfS+0X9lG73FYe4khrdZVw+geLfnb1W3p0sKP95ftpa67K/dcGVKg77XhTfDxdOOn6It5GXPy8vl6zQE6BvrSKcjvkuYCWoSev749iafOM2NdkVUSDYZajREHA79sOULK6SweGtCmQHt4s/o08HJnTXzZndK7jmbw5+5k7r0yhLoerri5uvDe6C7U93Tjnr4hNPD2KNDf092V4Z2as2jnMc5YQk6/23iIhNSzPD441NEjykT/0AD6h/rz0fJ4snPzy21cg+FywYhDLScvX/H5Xwl0DPSlb9tGBa65uAhXtvFn7b7UIiOISuLTlfvw9nBlfJ+QC20h/t6se34wz1zXzuE9t3UP5HxOHr/tOEpGZg7vLd1Ln9aNGHxF+e5uHtsrmLRzOWw5dKpcx60qTp3NJu1cdlVPw3CZYMShlvN7zDH2p57loQFtHIaHXtm2EUfTM0ko5c7irNw8Xl+0m/nbkhjbOxhfr4JOZO86bkWGo3Zr2YBW/t78FJ3Ix8v3cepcNv+84Yoi+5eVPm0a4SKUm9O9qpk4I4pbPllLZo7xoxguHSMOtRilFJ+u3EdIIy+GdmjqsM9VobpWxqo9zqfF3nU0gxEfreHTlfsYHdmCyUNKZw4SEW7tFsiG/SeZvno/I7sG0iHQt+QbS4lvXXc6t/Bj1d6aLw5p57KJOniKhJSzfPRnfIU/Lycvn+iDJyv8OYaqw4hDLWZdwgm2JaYz8ao2uLo4/lXeoqEXrfy9WemkOBxNP8/Ij9eQeiabL++O5PVbO+Hl4VbquY3sFoQIiMBT1zo2P5UH/dv6sz0xjfRzFbsTvKJZE38CpaBDYH0+XbmP3ccySr7pEvh+02Fu/WQdmw4YgbhcMeJQi5m2aj/+9Ty4pZt91deCDAgLYF3CCafMFb9sOUJmTj5zHuzN4CualHlugX51mdi/NS8OD6e5X90yj1MS/Sw7wZ2JyMrNy+evPSnk5lU/B/aqvSn4eLrx1T098a3rznM/7SCvAjf5Wf97zVxvor0uV4w41FLik8/w5+5kxvUOwdPdtdi+A8ICyMzJJ+pA8Y5bpRS/bD5C9+AGtA6od8lzfP76K7ird3DJHS+Bri398PZwZZUTfofvNh5i/PSN3PdNVIXnnCoNSin+2pNC3zb+BPjUYcqN4Ww9nMa36w5UyPPy8xXrE07iIvDbjmOcPGuc4JcjRhxqKdPX7MfDzYW7nNhU1qt1QzxcXVi5p/iKrTFJGexNPlPiSqQ64e7qQp82jVjthN9h7tYkGnl7sG5fKiOnrmFfyplKmGHJ7Es5S1J6Jv3D9L6Umzo3p0dIA75zMtNtadmbfIaTZ7O5r28rsvPy+TH6cIU8x1C1OCUOIjJUROJEJF5EniuizygRiRWRGBGZZWnrIiLrLG3bRWS0Tf9VIrLV8pckInMt7QNFJN3m2pTyeFHDRU6ezean6ERu7RZIo3p1Suzv5eFGz1YNS/Q7/Lz5CB6uLgzv2Ly8plop9Gvrz6GT5zh04lyRfQ6fPEf0wVPc378VMyf0Ju18DjdPXcP+cq4PMWfTYWKTSucvsNbQtgYPiAjdghtwIPVchZiWrMkY774yxCJCh02eqsuQEsVBRFyBqcAwIBwYIyLhdn1CgeeBvkqpCGCy5dI5YLylbSjwnoj4ASil+iuluiilugDrgJ9thlxlvaaUevnSXtFgz8z1B8nKzee+vq2cvueqMH/2HD/D0fTzDq/n5uUzf9sRrm7fuFDYanWnnzUiK75o8bPmYbqxU3N6tmrIvEl9yc9XvL5oV7nNIzYpg2d+2s6d09YTn3za6ftW7U2llb83LRpezCPVxr8e2Xn5JJ4qWvDKyrp9Jwj0q0uLhl6M7RXM/tSzhdKdGGo+zqwcegLxSqkEpVQ2MBsYYdfnAWCqUuoUgFIq2XLco5Taa/mcBCQDAbY3iogPcDUw91JexOAcWbl5fLPuIAPCAght4uP0fQPC9Aa0v4pYPazam0rqmWxG1iCTkpU2Ad408/Us1rQ0f2sS3YMbXPgCbtHQiwcHtGFJzPFyi9j5Zu0B6rq74ubiwvgvNxYpxLZk5eaxbt8J+tuVc20d4A1AQkr5rmzy8xUb9p+gd2u9YXJoh6Y08HJn5gbjmL7ccEYcAgFbo2Kipc2WMCBMRNaIyHoRGWo/iIj0BDwA+4ouI4FlSinbtXQfEdkmIotEJMLRpERkoohEiUhUSorzMfi1nc9XJpB6JosJ/Z1fNQCENalH0/qeF0xLSinik89c2JH785Yj+Hm5M6hdzavRLCL0a+vPmvjUCyk7bIk7dprdx04zoktBc9mE/q1o7FOHV3/bVeYd5FZOnc3WWXG7BvL1vT3IyMzl7ukbSwyx3XwwjfM5efQPLfCb60JAQHn7RfYkn+bUuRz6tNHi4Onuyu2RLfg95jhH0ooWs+zcfBZsT3KY6HDv8dOknM4q13kaLh1nxMFRALz9vwQ3IBQYCIwBplnNRwAi0gyYAdyrlLKPAxwDfGdzvhkIVkp1Bj6kiBWFUupzpVSkUioyICDAUReDDUop3lu6h//+sYfrOzalX1v/km+yQUS4Ksyf1XtTmbo8nkFvr2DIOyvp8vIfdH/lDxbtOMqNnZoXKN1ZkxjdowVns/OYPHtLITv9/G1HcHURru/YrEC7l4cbf782jC2H0vhtx7FLev73UYfJys3n7iuD6RDoy+fjunMg9Rzjpm8olBJjX8oZVu5JYfOhUyzYnoSbi9C7dcMCfRp6e9DAy5195bxyWLdPm49snze+TzCuLsJrvzk2seXnK576YRuPztpSKNFhfPJprn3vL3r8ZykD3lrOk3O2cvhk+ZvCDKXHmX/JiUALm/MgwD4RfiIwTymVo5TaD8ShxQIRqQ8sBF5QSq23vUlEGqHNVgutbUqpDKXUGcvn3wB3ESndN5mhAPn5ilcW7OK9pXt1QZ07upYpFcWAsMZkZOby1pI4mvp68p+RHfjn9VdwTXgT+rb1556+IeU/+UoiMqQhU4aHs3RXMm8u2X2hXSnF/G1J9G3rj78D5/1t3VvQrokPbyze7XDV4Qx5+YoZ6w7Su3VD2jetD8CVbf355K5u7D56mjs+X0/qmSyycvN4c/Furn33L+6evpFbPl7LzA2H6B7cAB8HNS5aB9QjoZxXDusTTtCiYd0CdTKCGnjx0IA2LNh+9IJ4WFFK8crCWOZvS6JeHTd+jE4ssMr6ftNhXEV4+rp2tGviw6Idx3ji+63GwV0NcGbr6iYgVERaAUeAO4A77frMRa8AvrZ8kYcBCSLiAfwCfKuU+sHB2LcDC5RSFwoGiEhT4LhSSllMUS6A8XaVkZy8fJ77aQc/bU7knitDmDI8HJcidkOXxHURTfjv7Z2JDGlAcCPvcp5p1TO+TzB7jp/ms5UJNPbxxL+eB1EHTnH45HkeHxzm8B5XF+GfN1zB+Okb6ffGn9zXtxV3XxmCb13nnfJLd2mTzIvDryjQPviKJky7O5KJM6IY/dk6XF2EPcfPMCoyiNsjW3AmK5czmbl0aeE4jXlrf29WlCLtSUlof8NJrg0vvLnx4YFt+DE6kX//GsOCx/rh5qp/d36ych9frTnAvX1DaOXvzZR5McQkZdAh0JecvHx+3nyEIVc0YdKgtgDMiTrMMz9uZ07UYe7o6VztDkPFUKI4KKVyReRRYAngCkxXSsWIyMtAlFJqvuXatSISC+QBTyulTojIXcBVQCMRuccy5D1Kqa2Wz3cAr9s98jbgYRHJBc4Dd6hLNejWEpRS5OSpC6ad89l5TJq1mT93JzN5SCiPDw69pOR1bq4u3FpMic6ajojw0k0R7E89yysLYgHwcHWhf6g/w4rIPQVwVVgAcyf15cNle3nnjz18tnIf4c3r08rfm7aN6zEqsgV+Xh5F3v/N2gM09/VkiIMd5VeFBfD1vT257+tN1Pd056t7ezjt12nTuB4/RCeSkZlTYvU8Z9h97DRp53IuOKNt8XR35cXhV/DQ/zbzv/UHCWvqw6crE/hrTwojujTnxRvCycjM4f8W7OLH6EQ6BPqybFcyJ85mM6rHxf9P3d49iB+jE3lt0W6GhDfBv14dlFKs23eC8Ob1i/3vaChf5HL43o2MjFRRUVFVPY0q51/zdjJr4yG6tmxAv7b+rIhLZuvhNF65uQNje1XsTuPLibNZuayOT6VlQy/aNq6Hu6vzfpSdR9KZtfEQ8cfPkJB6ltQzWQQ38uKL8ZGEOYgO23LoFCM/XsszQ9vxyMC2RY6bnJGJdx23C1XznOH3mGNMnBHN3El9i1xdlIbP/9rHq7/tZt3zV9PMt3BKE6UU477cyJp9qSgF/vXqcF+/ECb0a33hB8ukmZtZuy+VDf8YwkP/iyYmKZ01z159YaUB2g8x7P1VDO/UnLt6B/PKgli2Hk5jfJ9gXh7R4ZLfw3AREYlWSkU6ulb6jGiGasn+1LP8b8Mhurbw43x2Hu8u3YO7iwsfj+3G0A7NSh7AcAHvOm5cF1H0SqE4OgT68urIjhfOow+e5MEZmxk5dQ3vju7CtXbjvv17HI28PQrUu3BE4/qepZ6LNWIpIeVMuYjD7zHHCW9W36EwgF55vXJzB6bM28nQDk25tVtQodQst0UGsXDHUWZvOsSKuGQeGtCmgDAAtG3sw0MD2vDhn/H8suUIjX3q0Mrfu5A/w1CxGHG4THj3jz14uLrwyV3dCfCpw6mz2eQp5dCJaqg8ugc35NfH+vLgjGgmzojmtVs6MsZiS18bn8qa+BO8ODyceqVYEThLy4ZeuLpIuex1SDmdRfShU0wuwvdipZW/NzPu71Xk9f5t/WnsU4f/W7iLfAWjIls47DdpUFv2Hj9DWFMfHhrQmq/WHOCtJXGcOJPl1K5+w6VTM+MODQWITcpg/rYk7usXQoCP/ofTwNvDCEM1oZlvXeY82IeB7QL45y87WBJzDKUUb/0eRzNfT8b2qhjHq4ebC8ENvYrc65Cbl8/5bOcKA/0Rexyl4LoOZc+0C9pvNbJbINm5+fRs1ZAQf8eBDZ7urnw6rjtPXhOGl4fbhdDZjftNivDKwojDZcA7f8RR39ONif3blNzZUCV4urvy8dhudAry47HvtvDWkji2HErjb4NDS8yKeym0DvAucuXwyoJYhryz0uHGNHuWxBwjuJEX7Uqxq74obu/eAndXYXwf5/1gHQP98HR3YYMRh0rDiEMNJ/rgKZbuSubBAW1qXE6j2oaXhxtf3dODFg3q8vEKXYHvtgqO/modUI/9J84W2tiXdi6b76MOcyTtPIt3Fr+BLyMzh7X7Urkuomm5lGpt27geUS9cw/BOzido9HBzoXtwAyMO6JDiyggkMuJQw/l67QEaentwbw3egFabaODtwbf396JnSENeuimiVJFQZaG1vzfZufkk2aW2+H7TYTJz8vGv58E3aw8UO8by3cnk5Cmui7g0k5ItpdkHYqVXq0bsPpZR46v2XQpZuXn0fm0ZsyooHbstRhyK4sgRqOZhvkopNiTopGtlKcVpqBoC/eoy56E+DKyEPFRtGuuIpXgbv0NevuJby47sRwa2ZfOhNHYkphc5xu8xxwnwqUPXFg0qfL7F0atVQ5SCjZZEhzl5+Yz7cgPTViVU6bwqk11HT5N8Ootlu4qvrVIeGHFwxOrVEBQEgwbBli3lN+4338APP0Cec07Akjh08hzJp7PoEdKw5M6GWklr/8LZWa07su+5shW3dg/Cy8O1yKpxmTl5rIhL5prwJmXeWV9edG7hh4ebCxss6cG/XnOAVXtTWbjjaJXOqzLZdjgN0Obkik4xYsTBEXPngrs7xMRA9+5w//2QmVnyfcVx4gTcey+MGgUdO8LMmZcsEtbIjZ6tjDgYHNPQ2wPfuu4Fcix9veYAgX51GXJFY3zrujOyayDztiVxyq7cZ2ZOHtPX7Odsdp7DlBmVjae7K11b+LFh/0mOpJ3n3aV7cHURYpIyyKmGdb0rAqs4pJ/PKbAarAiMODhi0SIYMAD27oUnnoDp0+Hbby9tzKVLtZnqX/8CV1e46y6YMOGShtx04CR+Xu60LYd6zYbLExGhTYA3a+JT+TE6kT9ij7Mu4QTj+gRf2Hw2vk8I2bn5fLfpEImnzrFyTwr/WRhL79eW8ebiODoH+XJlm+qR+7JX60bEJKXz3E/byVeKp65tR3ZuPnHHii6OtHD7URZst88VWjPZmphGqMVUWFJN90vFiIM9hw5BbCwMGwZ+fvD22xASAgsWXNq4S5ZAgwbw4ouwbRv8/e/w9dewcmWZh9x04BSRwQ2qfLlvqN7c0Kk5yaezeOqHbTzwbRR13FwYbbP5rF1TH3q3bsibi+Po98Zy7p6+ka/WHODKNo2YNaEXcyf1rTap2Hu3aki+0sWlHh8cxg2WNOrbEtOKvOetJbuZMi+mxq8u0s/lkJBylhFdmtPI24OocioyVRTGi2nPokX6OGyYPorAjTfCtGlw/jzUdZw6oABK6ftsz5csgSFD9KoB4OWX4aef4JFHYOtWbcYqBcmnM9mfepY7ejjeYWowWLm/XyvuuTKE/aln2J6YToBPHRp4F0xgN2V4BPO2HSG4oTdtArxp19SnWia569qyAR6uLoT4ezGhfyvcXIQGXu5sP5zOWAcbs9PP5XDAUht87b4TDAirubVfth/RAtilRQMiQ9KJOmhWDpXLokUQHAzt219sGz5cC8Off5Z8//HjEBGhnc9WYmIgKQmuu+5im5cXfPCBXqW8916pp2ldUvYw/gaDE7i6CG0b+3BLt6BCVeMAwpvX5/lhV3Bnr5b0at2oWgoDQF0PVz4d143PxkXi7uqCiNAxyK/IlcPOpItRWL9uq9mmJau/oVMLXyKDG+qAlIxL9IUWgxEHW7KzYdkyGDq04C//AQOgXj3nTEuPPw67dsE//wlZltKHS5boo604gF6R3HQTvPSSdoK/8gpcfz1MmVKis3rj/pN4urvQobmv8+9nMFwGXN2+Ca1s0m50DvJlb/IZzmUXLra044gWhyFXNGHJzmNk5pRPpGBVsPVwGm0CvKnv6U5kiA4rrsjVgxEHW9asgTNnLpqUrNSpA9deq8WhuL0PCxfC999rEThyBGbM0O1LlkB4uA6Ptef99/WYI0dqZ/WePVokRo7UcymCqIMn6dqiQbWxBRsMVUXnID/y8hUxSRmFru1ITKdlQy/G9QnmdFbuhRro1Qn7KDFHKKXYejidzpbsuhHNffF0d6lQp7RT3ywiMlRE4kQkXkSeK6LPKBGJFZEYEZllaesiIussbdtFZLRN/69FZL+IbLX8dbG0i4h8YHnWdhHpVh4v6hSLFmnb/9VXF742fDgkJmpnsiNOn4aHH9YmpXnzdAjs66/r9r/+KrxqsBISolcrv/6qw13j4+Gjj7TQDBigzVH2j8rMITYpw5iUDAa0mQUuml1s2X4kjY5BvvRt04iG3h7Mr2ampRVxyXT/vz/4Iepwsf2S0jNJPZNFV4s4eLi50DnIj6iDFeeULlEcRMQVmAoMA8KBMSISbtcnFHge6KuUigAmWy6dA8Zb2oYC74mIbWL5p5VSXSx/1upww9D1p0OBicAnZX670rJoEfTvDz4Okotdf702NRVlWnrhBS0eX3yhVxr/+Afs2wePPabNS0WJA0CfPlp8Glh2oE6apMVizx4YPbpQ982H0shX0NNsfjMYaOzjSTNfT7bb7fI+dTabwyfP0zHQFzdXF67v2JRlu45ztoy1vsub7Nx8/v1rLPlK1/UoLkOuVfg629Tl6BHSkJikDIfmtPLAmZVDTyBeKZWglMoGZgMj7Po8AExVSp0CUEolW457lFJ7LZ+TgGSgpHCBEeia00optR7wE5GKr1Zz+DDs3FnYpGSlSRPo2VN/adsTFwcffqgjj/r00W0336yd2t98A56ecNVVpZvP9ddr89Lq1YV2aW/afxJXF6Fry0sv4GIwXA50CvJlu51T2upv6BSoVxY3dQ4kMyefpbuOV/r8HDF9zX72p57l0UFtOZ6RxdfF5LjaejgND1cX2jetf6Gte0gD8vIVWw8VHcZ7KTgjDoGA7Zon0dJmSxgQJiJrRGS9iAy1H0REegIewD6b5v9YTEfvioi1+IAzz0NEJopIlIhEpaSUgx1x2TJ9vPbaovsMHw4bN+qIJFv++1+9Wpgy5WKbiws8/7z+PGCAcyGw9txzj45qmjq1QPPq+FQ6BPrqkpHvvad3XlfzPFAGQ0XSuYUfB06cI+3cRfu9VRwiLOIQGdyAZr6evDQ/hhFT13DnF+vLnJcpL19xxC6ZYWk4npHJh8v2MuSKxjx1XTsGt2/MxyviC8zflq2H0whvXr+Aj7FbywaIVJxT2hlxcLTDyv6byA1tBhoIjAGm2ZqPLL/8ZwD3KqWsO1GeB9oDPYCGwLOleB5Kqc+VUpFKqciAgHKIXV6xAvz9oUMxNWpvukkfP/74Ytvx43r39N13Q2O7RGpjxuiVyMSJZZuTn5/eST1zJpzUtsXk05lsPZzGkPaNISrq4ma6X34p2zMMhsuAzkH668bWtLQ9MY1W/t4XMsC6uAj/ujGcPm0a4VvXnQOpZ3l/2d4y5Sh6ZUEs/d74kw/LeP/ri3aTk6d4cbi20D8ztD1nsnL5eMW+Qn1PZ+aw80h6oVKvvnXdeezq0AqzIDgjDomA7U6rIMDeq5MIzFNK5Sil9gNxaLFAROoDC4EXLGYiAJRSRy2moyzgK7T5yk3M0awAACAASURBVNnnlT8rV2rTj0sx/0k6dYKxY+G117QJCrQ5KTsbnnyycH93d/jtN7jllrLPa9Ikndfpq68ALmRjvCaskc751KSJNl89+6yeh8FQC+lgWR3YmpZ2JKbTMbBgqPfQDs34eGx3vr2vJ48PCeV0Zi4HT54r1bOiD57im3U6P9V//9jDIzM3c6YUfoxlu47zy5YjPHBVK4Ib6ZDcdk19uLVbEF+vPUB88sUoxazcPB76XzTZufnc2Lmwdf3Ja8Ic7lspD5wRh01AqIi0EhEP4A5gvl2fucAgABHxR5uZEiz9f0H7EH6wvcHqRxBdPeRmwPJty3xgvCVqqTeQrpSq2LSLBw/CgQMwcGDJfd97D3x94b77ICNDryJGjICw4mvrlplOnbST/OOPIT+fpbHHCWpQl3bffgrbt+v2//5XRzl99lnFzMFgqOb41nWnfVMfvo86zKmz2aScziIpPbOQONjSMdC62nDeZp+dm8/zP2+nWX1PFk++ihduuILfY48x4qPVLN+dXGIRntikDB77bgsdAuvz6KDQAteeuCYMLw9XRny0mp83J5Kfr3jqh+2siT/Bm7d1ontw5QaglCgOSqlc4FFgCbALmKOUihGRl0XEYmdhCXBCRGKB5egopBPAKOAq4B77kFVgpojsAHYA/sD/Wdp/AxKAeOAL4JHyeNFiseY3GjCg5L7+/vDhh6TE7CXyX4tY3qANPP10xc5v0iRISODcgkWsjk9lSIN85OV/w+23a8f3sGE6/Pbf/4b0ovPyGwyXM6/d0pHjGVk8PDOaLYe0Hb5jUNHiENqkHnXcXApFOdlyNiuXHYnp5FryMn22ch97jp/hlZs7UK+OGxP6t+Z/9/ciOy+fe7/exK2frOXXbUl8tWY/z/20nfu/3sRP0Ylk5eaRnJHJ/d9sor6nO1/e3YO6HgXLwwb61WXh3/oT0dyXJ+dsY9j7q/h1WxLPDm3PLd0qtmKgI6Qyys1VNJGRkSoqKqrsA9x3n96bkJJSvFnJilJ8e88/mNKsHyOP7+Ddrxxu/Sg/srMhOJglPiE8eMsLzPruH1x5+rDeid3Ekkp582a9t+LZZ/X+CoOhFvLLlkSe+H4b/vXqcOJsFtv/dS0+nkXnLbvl4zW4ubgw56E+Dq+/+tsuPv8rAZ86bvRp04gVe1K4JrwJU+8suP0qJy+fOVGH+XBZPMcsKS0aeLnj5eHGkbTz+NfzoL6nO8cyMpnzYJ8LZjBH5OUrPvoznveX7eHuK0OYMjy8XMqzOkJEopVSkY6u1erEe5k5eZzPzqP+ypW4luRvsEWEhZHXwZGz/NWiE/n5qmIzo3p4wLffsnTpIepLHj1eegKu6n9RGAC6dYNx47TZ66GH9OY6g6GWMbJrEHHHzvDpyn20CfAuVhgAOgX5MSfqMHn5ClcH/4bXxKfSrokP3YL9+GtPKvU93fjXjeGF+rm7ujC2VzC3dgtix5F0ght6EeCjAzBXx6fy9ZoDrI5P5aM7uxUrDKDzYD0+JJR7+4XgU8etwoShJGq1OCzfnczDMzfD7R/gI3k0eWclkwa14eYugYgISil+iE7km7UHeOPWThf+R03OyGRj0llCG9djb/IZdial0ymoYvcc5A0ewp/rlzIo1B/3O25y3OnVV+HHH/Xq4fvvL7ZHR+torCefLJgzymC4DHn6unakncsukH+pKDoG+vL12gPsSzlDWJOCm19PZ+aw62gGj14dypPXhKGUIi9fXaiD4QhPd9dClRn7hwbQPzSg1D8i65cgbBVNrU7M075ZfaY0Pcvjq2dxa6gvdd1deeL7bdz15QZW7U3hzi828MyP24lJyuCNxbsv3Lc45hhKwSs3d0AEVsQVvc9i55F00s9fekH0LYdOceJsNkOuKKYiV1AQPPMMzJmj80SB3qV97bXw1FMw3z6OwGC4/HB1EV6/tRMPDmhTYt9OQdYop8J+B2smgh6WJHciUqwwlERNq7tSq8Whlb839+1YwhOxi3jpnv7MndSXV0ZEsP1wOuO+3MjOpHReHdmR54a1Z9XeVDZZimss2H6UsCb16N26ER0DfYtM5hWffIYRU9fw7h97nJ7TT9GJzNl0uFDUw6/bknB3FQa0KyFs7emnITAQJk/WeyOGD9ftbdroFUVu9UgdYDBUB1oH1MPbw5UdDiKWog5YMxE0qIKZVT21WhyAAvsbXF2EcX1CWPb3Afzj+vYse3IAd/Zqyd19QvCvV4d3/9hDckYmmw6c5HpLBaqBYQFsOXTK4c7GNxfvJi9f8acTIW6gd00+//MOnvlpOxNnRJN2Lpv08zlMnr2Fb9Yd5LqIpiUvNb299T6MqCjth9i3D37+WYe7xsXBl1+W6T+TwXA54uoiRAT6ss3BymHj/pOEN6tPvTq10/peu8UhMVF/edqFsDau78nEq9rQuL4noAuMPDywDWv3neDfv8aiFBfKEw5oF0C+0k4nW6IOnOT32OO0bVyPQyfPkZB6tsTpTF+zn9z8fCYNasOKuGSGvb+Koe/9xa/bjzJ5SCjvju5S4hiA3qjXo4fev/HZZ/r9broJ+vXTacGLSQVuMNQ2OgX6Ens0o0AZ0ezcfLYeTivkP6hN1G5xsO5vcGLz29heLWlSvw4Ld2iTUqjFedU5yA/fuu6stPE7KKV4bdFuGvvU4ZOxOuRt+e7kYsfPyMxh1vpDXN+xGU9f155fHulLXXdXvDxc+fnhK5k8JAx3Z+2dLi7aMT1vns67BNoR/dZbOt3H2287N47BUAvoGORLdm4+e46fvtC240g6Wbn5F/wNtZHaLQ7Dh2snbadOJXb1dHdl0qC2ABdMSgBuri70C/Vn5Z6UC6aj32OPE33wFJOHhBHaxIfQxvWKdVoDzNpwiNNZuTxkcaJ1CPTljycH8McTAwqk6XWali0v5oKy0rs33Hab3izXsqWuePfss1okjS/CUEtxlJcpyuJfjKzFK4faaUyz4uurS3U6yR09WnI6M5cxPVsWaB8QFsDC7Ud5d+leMs7nsHjnMdoEeDMqUu9qHNS+MV+t2c/ZrFydSdWOrNw8pq/eT7+2/gVioB3FXV8yn3+uN8vt3HmxfvWbb0LDhjoH1DvvOK5nYTBcpgQ38sLH043tiemMsWR423TgJK38vS/sVaiN1G5xKCUebi4XVg+2DAwLwN1V+GDZXrw9XGnTuB5ThodfCHsb2C6Az/9KYE18KtdGNAV08q7DJ8/R3K8u2w6nkXw6i/+O6lzxL9GgATxns6P79Gn4/XdtgvrqK52j6bffypZi3GCogYgInYJ8WbsvlfRzOfh4uhF18BTXhhcTNl4LMOJQDjSu78mffx+Ih5sLjX3qFNrRGBnckHp13Fgel8K1EU1ZvjuZ+7/ZhG2m34jm9enX1r+SZ45eJdx6q/4bNkw7s2+5BebO1TUqDIZawPg+ITw6azM3frSav18bRtq5nFptUgIjDuVGi4ZeRV7zcHOhX1t/VsQls/uYzsp4RbP6vH17Z1JOZ3EsPZPIkAZVtk3+AmPGwLlzMGGC/vzjj86nFDEYajDXRTRl9sQ+PDIzmsdn64rFtb0MrxGHSmJQ+wAWxxzjzi824F3HlS/v7kFTX0+uqPgCqKXj/vvhxAntqF6+HAYPruoZGQyVQvfgBvz6WD/+9t0WTp7NJrhR0T/4agPmZ2ElMbCdrhJ3LjuXaeO1MFRbJk3SJiVH9bINhsuYxj6ezJ7Yh0WPX1X1K/kqxohDJdGkvicv3HAFX97do9gc89UCb2+9Yvj1V1Ob2lArqZBIwRqGEYdKZEL/1vStCqdzWRg+HBISYPfukvsaDIbLDqfEQUSGikiciMSLiMPKNiIySkRiRSRGRGZZ2rqIyDpL23YRGW3Tf6ZlzJ0iMl1E3C3tA0Uk3aZy3JTyeFFDKbEm7FuwoGrnYTAYqoQSxUFEXIGpwDAgHBgjIuF2fUKB54G+SqkIYLLl0jlgvKVtKPCeiFi3+84E2gMdgbrABJshVymlulj+Xi7z2xnKTosW0Lmz8TsYDLUUZ1YOPYF4pVSCUiobmA2MsOvzADBVKXUKQCmVbDnuUUrttXxOApKBAMv5b8oCsBGo/CKphuK58UZdF+LkyaqeicFgqGScEYdA4LDNeaKlzZYwIExE1ojIehEZaj+IiPQEPIB9du3uwDhgsU1zHxHZJiKLRCTC0aREZKKIRIlIVEpK8XmLDGVk+HDIz4dFi6p6JgaDoZJxRhwcue3tQ1jcgFBgIDAGmGZjPkJEmgEzgHuVUvl2934M/KWUWmU53wwEK6U6Ax8Ccx1NSin1uVIqUikVGRBQQgEcQ9no0QMaNzZ+B4OhFuKMOCQCLWzOg4AkB33mKaVylFL7gTi0WCAi9YGFwAtKqfW2N4nIv9BmpietbUqpDKXUGcvn3wB3EakhIT6XGS4ucMMNsHgx5Fx6qVODwVBzcEYcNgGhItJKRDyAOwD7YsRzgUEAli/yMCDB0v8X4Ful1A+2N4jIBOA6YIztakJEmopl94nFFOUCnCjLyxnKgeHDIS0N1q2r6pkYDIZKpERxUErlAo8CS4BdwBylVIyIvCwi1oIBS4ATIhILLAeeVkqdAEYBVwH32ISmWsuZfQo0AdbZhazeBuwUkW3AB8Adypkam4aK4eqr9Qpi6dKqnonBYKhE5HL43o2MjFRRUVFVPY3Ll969wdVVRy4ZDIbLBhGJVkpFOrpmdkgbSmbIENiwATIyqnomBoOhkjDiYCiZwYMhL+9izW2DwXDZY8TBUDJ9+ujKcMbvYDDUGow4GErG0xP69zfiYDDUIow4GJxjyBCIjYWjR6t6JgaDoRIw4mBwjiFD9HHZsqqdh8FgqBSMOBico3NnaNTImJYMhlqCEQeDc7i46A1xS5ea6nAGQy3AiIPBeYYMgSNHTHU4g6EWYMTB4Dw33KBXEDNmVPVMDAZDBWPEweA8gYFaIKZPN1laDYbLHCMOhtLx4INw/DjMm1fVMzEYDBWIEQdD6Rg6FFq2hM8+q+qZGAyGCsSIg6F0uLrChAk6aik+vqpnYzAYKggjDobSc//9WiS++KKqZ2IwGCoIp8RBRIaKSJyIxIvIc0X0GSUisSISIyKzLG1dRGSdpW27iIy26d9KRDaIyF4R+d5SNQ4RqWM5j7dcD7n01zSUK82bw003wVdfQWZmVc/GYDBUACWKg4i4AlOBYUA4MEZEwu36hALPA32VUhHAZMulc8B4S9tQ4D0R8bNcewN4VykVCpwC7re03w+cUkq1Bd619DNUNx55BFJSoFkzGDsWfvwR8vNLvs9gMNQInFk59ATilVIJSqlsYDYwwq7PA8BUpdQpAKVUsuW4Rym11/I5CUgGAiw1oq8GfrTc/w1ws+XzCMs5luuDrTWlDdWIIUNg0SK4+Wb4/Xe4/XZ4552qnpXBYCgnnBGHQOCwzXmipc2WMCBMRNaIyHoRGWo/iIj0BDyAfUAjIM1Sn9p+zAvPs1xPt/S3H2+iiESJSFRKSooTr2Eod4YO1aalY8f05//8B06dqupZGQyGcsAZcXD0q90+uY4bEAoMBMYA02zMR4hIM2AGcK9SKr+EMZ15Hkqpz5VSkUqpyICAgBJfwlCBuLrC669Dejq8YayABsPlgDPikAi0sDkPApIc9JmnlMpRSu0H4tBigYjUBxYCLyil1lv6pwJ+IuLmYMwLz7Nc9wVOlualDFVA587a9/D++zr/ksFgqNE4Iw6bgFBLdJEHcAcw367PXGAQgIj4o81MCZb+vwDfKqV+sHZWSilgOXCbpeluwLrldr7lHMv1Py39DdWdl1/WtaZfeqmqZ2IwGC6REsXBYvd/FFgC7ALmKKViRORlEbnJ0m0JcEJEYtFf+k8rpU4Ao4CrgHtEZKvlr4vlnmeBJ0UkHu1T+NLS/iXQyNL+JOAwdNZQDWnVSkcxTZ8OK1ZU9WwMBsMlIJfDj/LIyEgVFRVV1dMwgA5vjYyEQ4dg9GjtgwgOrupZGQwGB4hItFIq0tE1s0PaUL4EBOha01OmwPz50K4drFpV1bMyGAylxIiDofzx9oZ//xvi4nQk048/lnyPwWCoVhhxMFQcLVroKKYtW6p6JgaDoZQYcTBULF27anEwqTUMhhqFEQdDxdKtG5w5A/v2VfVMDAZDKTDiYKhYunbVR2NaMhhqFEYcDBVLRAS4uRlxMBhqGEYcDBVLnTrQoQNs3lzVMzEYDKXAiIOh4rE6pS+DDZcGQ23BiIOh4unaVe+cTrLP12gwGKorRhwMFY9xShsMNQ4jDoaKp3NnEDF+B4OhBmHEwVDx+PhAaKhZORgMNQgjDobKweqUNhgMNQIjDobKoVs3OHgQTpqifgZDTcCIg6FyME5pg6FG4ZQ4iMhQEYkTkXgRcViZTURGiUisiMSIyCyb9sUikiYiC+z6r7KpDpckInMt7QNFJN3m2pRLeUFDNcEqDuvWVe08duyAt9+GnJyqnYfBUM0pURxExBWYCgwDwoExIhJu1ycUeB7oq5SKACbbXH4LGGc/rlKqv1Kqi1KqC7AO+Nnm8irrNaXUy6V9KUM1xN8fBg6EadN0nemq4j//gaefhltugfPnq24eBkM1x5mVQ08gXimVoJTKBmYDI+z6PABMVUqdAlBKJVsvKKWWAaeLGlxEfICrgbmlnLuhpvHYY9rv8OuvVfN8pXRVupAQWLgQrr0W0tKqZi7lxeHDul731q1w4ADk5lb1jAyXCc6IQyBw2OY80dJmSxgQJiJrRGS9iAwtxRxGAsuUUhk2bX1EZJuILBKRCEc3ichEEYkSkaiUlJRSPM5QZdx0E7RsCR98UDXPP3BA79J+6in4/nvYsAGuuabmpvVQSgvcoEHabNeqFYyw/91mMJQNZ8RBHLTZ/2tyA0KBgcAYYJqI+Dk5hzHAdzbnm4FgpVRn4EOKWFEopT5XSkUqpSIDAgKcfJShSnFzg0cegeXLYefOyn++tZZ1//5w++3w+usQFaV/fddEYmNh92545hn46ScYPx5++w3i46t6ZobLAGfEIRFoYXMeBNgnyUkE5imlcpRS+4E4tFgUi4g0QputFlrblFIZSqkzls+/Ae4i4u/EPA01gQkTwNMTPvyw7GP8/LP+Ky2rVoGfn84SC9Crlz7u2FH2uVQW+/cXNoH9/LPeeT55svahvPqqrtk9bVrVzNFwWeGMOGwCQkWklYh4AHcA8+36zAUGAVi+yMOABCfGvh1YoJTKtDaISFMREcvnnpY5nnBiLENNoFEjGDsWZsyAU6dKf39yMtx1l/7l/+efpbt39Wro2xdcLP+3t4rE9u2ln0dlopRe7dx1V8H2X36BPn2gWTN9HhgIw4fDV19Bdnblz9NwWVGiOCilcoFHgSXALmCOUipGRF4WkZss3ZYAJ0QkFlgOPK2UOgE6ZBX4ARgsIokicp3N8HdQ0KQEcBuwU0S2AR8AdyhVU43CBoc89piOFPr009Lf+9ZbkJWl7eujR8OhQ87dl5KiTTD9+l1s8/WF4ODqv3I4eBCOHNFOdOtc9+/Xe0ZGjizYd+JELaDz7X+/GQylRClV4/+6d++uDDWMG25QytdXqZQU5+85flypunWVuusupXbvVsrHR6kePZQ6f77ke3/+WSlQavXqgu3DhysVEVG6uduTna3Ul18qdfbspY1TFHPm6LmLKDVunG575x3dFh9fsG9urlItWih1zTUVMxfDZQUQpYr4XjU7pA1Vw5tvwpkz8O9/O3/P22/rVcMLL0C7dvDtt7BpEzz6aMkRR6tX66p0kZEF2zt1grg4PW5ZWbIE7r9f2/4vlRUrCpvbNm0CDw94+GH47ju9WvrlFz33Nm0K9nV11X6dP/6ABGcsuwaDY4w4GKqG8HB44AFtWoqLK7l/cjJMnQp33qmFAeDmm+Gf/4Qvvyw5PHbVKujZUwuELR076r0Bu3eX7T0AoqP18YsvdNRQWVAKXn5Zh6U+Z5eEYONG6NIFnn1Wnz/3nBY7e5OSlfvu034V45g2XApFLSlq0p8xK9VQjh/XpqGbbiq579NPK+Xios1JtuTlKXXzzfraokWO7z1zRilXV6X+8Y/C12JitHlmxozSz9/KjTcqFRqqTVwNGih16FDp7s/PV+qZZ/Q8vLy0WSg/X1/LzVWqXj2lHn1Un48bp/uBUtu2FT3mNddcurnMcNmDMSsZqiWNG8Pzz2vn6YoVRfdLS4NPPoE77ri4arDi4qIjnzp21A5qRyuA9et1yo7+/QtfCw3VJpuiIpYWLIBhw/QGvrFj9a93+0igzZt1WOysWTpn0113lS5FyBNPaDPbQw/Bu+/qfRexsfpaXJw2v/Xooc+fflofW7fW71wU3bvDnj0mh5ShzBhxMFQtkydDQAB89lnRfT77TH9BPvOM4+v16mmBqVtX73i2rTiXlwf/+5/eD9CnT+F73d3hiisKRyydOaMjf268EXbt0nb+1av1l/gff1zsd/y4jiTq1g3attXmrb/+cj5FyNq18P772m/y8cdw/fW6ffFifdy4UR+t4tCxI/zjHzBlin6noggP18Kwd69z8zAY7ClqSVGT/oxZqYZzxx1KNWt20ZRiS1aWvuZM9M22bUq1bKkjmmbPVurgQaX699cmmEceKfq+ceOUat784vmePUq1aaOjg555RqnMTN1+7pxSdeoo9cQTF/suXKjHX7lSn2dmKuXhoc1gznD99Ur5+2vTl5UOHZQaPFh/fuQRbXrLy3NuPCvR0Xpec+aU7j5DrQJjVjJUawYNgqNHHf/KnTVLX3vqqZLH6dRJR/Z066ZNUOHhOiHdjBnamV0UHTvqnEvWQkRPPgmpqdrU9cYbF53YdevqfRJLl16817pK6dJFH+vU0Z83bCh5vlu26HQXkyeDt/fF9qFDtQP9zBn9PpGRFzfuOUv79nplERNTuvsMBgtGHAxVz8CB+rh8ecF2pXT4aqdO2lzkDI0b653TkyZpM9LWrYV3FtvTqZM+7tihv9QXLNAmrKuuKtx38GDd7/hxfR4dDWFhUL/+xT69e+ucTSVlSH31VX3fpEkF24cN036NJUv0/K0mpdLg5aX9ElbfhcFQSow4GKqe0FCdAsLeKb1kif7l+9RTxdvX7fHwgI8+0r6B1q1L7m917G7fDi++qGtP/O1vjvsOGaKP1tQdmzfrlYotvXrBuXPF/2rftUuHvT76qM73ZEvfvnol8cYb2m9QFnEAvXIyKwdDGTHiYKh6RPTqYcWKgpvZ3npL5wsaPbpin9+smc759MUXWlCef147uR3RrZv+Ml+2TJueDh3SkUG2WBP6rV9f9DNff12bqRxtnKtTR69QNm3S5z17lv6dACIidMSSybNkKANGHAzVg4ED4dgx/WUG+ovxzz/1l6eHR8U+W0SvHnbs0ELx8MNF93V11T6SpUsvbn6zXzm0bq1XH0X5HQ4dgpkzdTRUUenmh1pKojRuDC1aOO5TEhER2rRlIpYMZcCIg6F6MGiQPlr9Dq+/rn+hP/hg5Tzfalr65z/1L/riGDxYJ8P74Qd9bi8OIvrXflHiYHWOP/FE0c+wikOPHqUzqdkSYamTZfwOhjJgxMFQPWjbFpo316al3bt17qBHHwUfn8p5/pgx+m/ChJL7Wv0OM2bo3Eb2PgPQpqVduyAjo2D72bPw+ee6/kLLlkU/o1UrLYz33+/8O9jTrp2JWDKUGbeqnoDBAFz0OyxbpjeaeXoW7RSuCPr0cbxJzhFhYRAUBImJhVcNVnr31v6TTZv0SsPKN9/oHd/OJOkrS0pzW6wRS0YcDGXArBwM1YeBA3WI6Ndf61/w1bX8q8jFL3x7Z7QVqxPZ1rSUn693Q/fo4bwQXSoREUYcDGXCKXEQkaEiEici8SLyXBF9RolIrIjEiMgsm/bFIpImIgvs+n8tIvtFZKvlr4ulXUTkA8uztotIET/NDJcdVr+Diwv8/e9VO5eSsO67sE8BbsXPT5t1bMVh8WLtcH/iibL7EUpLRIR2SJuIJUMpKdGsJCKuwFTgGnSt6E0iMl8pFWvTJxR4HuirlDolIo1thngL8AIceRafVkr9aNc2DF1/OhToBXxiORoud9q00V+o/fvrCm3VmdGjdcipVdAc0auXFgRreO6772q/ym23Vc4coWDEktVBbTA4gTMrh55AvFIqQSmVDcwGRtj1eQCYqpQ6BaCUSrZeUEotA06XYk4jgG8tqT/WA34i0qwU9xtqKiI6pcQnn1T1TErGzU1/yReX1qJXL12H4skn9Ua/pUu1H8XdvfLmGR6uj8a0ZCglzohDIHDY5jzR0mZLGBAmImtEZL2IDHXy+f+xmI7eFRFrFRZnnoeITBSRKBGJSklJcfJxhmpP3br6i/dy4Mor9fH99/VK6KuvnMsRVZ60b68FzIiDoZQ486/QkXHUviajG9oMNBAIAlaJSAelVFox4z4PHAM8gM+BZ4GXnXweSqnPLfcRGRlZQo1Ig6EK6NIFfv9dpwQPCqqaOdStayKWDGXCmZVDImC7RTMISHLQZ55SKkcptR+IQ4tFkSiljlpMR1nAV2jzlbPPMxhqBtdcU3XCYKVjR10XIj+/audhqFE4Iw6bgFARaSUiHsAdwHy7PnOBQQAi4o82MxVb3dzqRxARAW4GdlouzQfGW6KWegPpSqmjTr6PwWCwZ9QoXV1u2bKqnomhBlGiOCilcoFHgSXALmCOUipGRF4WkZss3ZYAJ0QkFliOjkI6ASAiq4AfgMEikigi11numSkiO4AdgD/wf5b239DCEg98ATxSDu9pMNRebr4ZGjbUiQUNBicRpWq+uT4yMlJFRUVV9TQMhurLE0/onE5HjlTfzYWGSkdEopVSDjfrmB3SBkNt4IEHdG2Ib76p6pkYaghGHAyG2kB4uA6tnTatYM0Mg6EIjDgYDLWFBx6AuDhdn9pgKAEjDgZDbeH223XN6s8+q+qZGGoARhwMhtqCt7dePcyaBR9+WNWzMVRzjDgYDLWJOwifxgAAEb1JREFU116DkSN1jqfqkMMqNxdefBEOHKjqmRjsuEyS2BgMBqdwd4fZs3XSwEcegdOnYdw4XTu7Kli4EP7v/3TZ1W+/rZo5GBxiVg4GQ23Dw0PXv77+enj2WZ1GPCxM18+u7BQb06fr4+zZcOxY5T7bUCxGHAyG2kidOvDrr7qM6dtv6+R8r74KU6ZU3hyOHdMrh1tu0eal6mDmMlzAiIPBUFtxcdGV7P7+d1i0SJdm/c9/tMO6MpgxA/Ly9DNvuEGLQ2Zm5TzbUCJGHAwGgy60NHUqXHUV3HefzuJakSilTUp9++qaE5MnQ0qKNi+VlZwcLTaGcsGIg8Fg0Hh4wE8/aee01dRzqeTnw6lTkJAAsbEXfRrr18Pu3VqIAK6+Gjp0gPfeK9sObqWgd2+9+jGUC0YcDAbDRfz9te/hyBHYvv3SxoqL05vuGjbU9cEjInQBpB9+0Gk8vL31xjzQK5fHH4dt2/S10jrG166FzZu1SSw19dLmbQCMOBgMBnv699fH1asvbZx58+DsWXj9dV0i9ZNPtOln1ChtUho1Cnx8LvYfOxa6dYOJE7UvZNEi51cR06ZpJ3t2tvZlGC4Zk7LbYDAUJjgYevWCOXPKPsZ11+kiQ7GxF9vy8uDHH/UX+Jtv6oSAtuTlwcyZ8NJLsH+/FpSHHir+OenpOhz3rrv0yuP0adi5U69GDMViUnYbDIbS0a+fXjmU9cdjVpZO8Dd4cMF2V1cYPRoWLCgsDNbr48drf0SnTs6tAmbPhnPntL9hwgQtRuvXl23ehgs4JQ4iMlRE4kQkXkSeK6LPKBGJFZEYEZll075YRNJEZIFd/5mWMXeKyHQRcbe0DxSRdBHZavmrxMBrg8EAaHE4elT/ei8L69fD+fOFxcFZPDzg1lth3To4frz4vtOmaSGJjNTC4+2t26xERcGuXWWbRy2mRHEQEVdgKjAMCAfGiEi4XZ9Q4Hmgr1IqAphsc/ktYJyDoWcC7YGOQF3ANsxglVKqi+Xv5VK8j8FgKA/69dPHsvodli3T+ygGDiz7HEaM0CuXBQuK7rN1q/7ynzBBm5F8fGDMGL2aSE7WIbI9elx0fBucxpmVQ08gXimVoJTKBmYDI+z6PABMVUqdAlBKJVsvKKWWAaftB1VK/aYsABuBoDK+g8FgKG8iIsDXF9asKdv9y5ZB9+7g51f2OXTqpH0fc+cW3efLL7UjeuzYi20TJmgzU1gYvP8+dO4MMTE6nNbgNM6IQyBw2OY80dJmSxgQJiJrRGS9iAx1dgIWc9I4+P/27j64qvpM4Pj3SQIrkGajEBcXotghorSdbiWwEAQVtKACYabQkO4iUCIznVKpqPjS6foyQ6tWi7C6zFDQ0plW6bKtoUChSmFk+kKJZYsFdU3AxghKqmAireXFZ/94zmkuufeSm5v7Ivc+n5k7yT3n3PP7HX7hPPf8XtkSsXmMiPxBRH4uIp+K87kFItIgIg2tra2JJuecS0RBgQ1QS+bJob3dBtFdd13P8iAC06fDCy9Yr6fODhywXlAzZlh32dCoUVbFVFwMW7fa2A2wqToiPf+8TV2e6fmkzhGJBIdYTf6dW6mKgArgGqAWWC0iiX5l+C/gRVUNl6f6PXCJqn4W+E8g5tcGVV2lqpWqWlnmC6Y7l3pXXWWNu+++273PvfiiDaBLtr0hUnW1Tanxi1+cuf30aZg71xqwv/3tM/eJwI4d0NQEn/+8jbG4/HKbSyry83V1NnX5rFnWPpItH9PglEhwaAHKI94PBg7FOKZeVU+q6kHgNSxYnJWI3AeUAYvDbarapqofBL9vBnqJyIAE8umcS6Ww3eHXv+7e57Zts6qeqqqe52HcODj//Oiqpccft95QK1ZAeXn05/r1szyEpkyxgNEe1HBv3gzNzTYSfP16uPbarhu+06GpydpJtm3LfNpdSCQ47AYqRORSEekNzAI2dDrmOeBagOBGfhlw1go+EakDJgG1qvpRxPaBItZBWURGBXns5lcX51yPjRxpvYa6qlratMm+nT/4ILS12Y1u7Fjo06fneSgqskn5Nm7smM5j3z6bXry62rq9JmLqVBuAFz6BrFxpYyPWrbNqp7177ZhM27TJ2kc+jivzqWqXL+BG4P+AJuAbwbYHgWnB7wJ8F9gPvAzMivjsTqAV+Cv2hDEp2H4qON//Bq//CLYvBPYBfwB+C1R1lb8RI0aocy4NqqrsFc/+/aqf+ITqgAGqoNq/v/1cujR1eVi/3s55992qCxaoDh5s6b3zTuLnOHlStbRUde5c1aYmVRHV++7r2L90qaXR2pq6fCdi6lRLt7BQ9fDhzKatqkCDxrvvx9txLr08ODiXJkuWqPbqpfrWW9H7jh5VrahQvfBC1eZm1d27VSdPthvd3r2py0N7u2q/fna7Ki1VnThRdceO7p+ntla1rEz1jjssjy0tHft27LDz/+xnqct3V06csMA6caKl/dBDmUs7cLbg4COknXPx1dVZ1c78+WeOlj592rqPHjxodfbl5R3zIbW3w2c+k7o8FBfDnj3Q2AjvvWe9l66+uvvnmTrVpgVfvhymTYNBEZ0uR460xu3f/CZ1+Q61tdl06DNn2tiL0O7d9m/1la9Y28qaNcmPSE8DX0PaORdfRYXNgfS1r8H3vmeT4n34IcybZ426K1d2TNQXSkVbQ6x89NSkSRYATp60G3Kkvn1txtjuNr7HcuiQzUh74ADs2mUzxYZdca+4wtpmwIKciDWGHz8Oc+ZYI/v48T3PQyrEe6Q4l15ereRcGp0+bVUf/fqp7tqlOnasVYM8/HC2c9Z9EyeqDhtm19TZwoWqffta+0Sy6utVCwrs3wdU+/RRnTdP9Xe/U73pJquC+/BDO3bcONXw3nX8uGpJiers2cmnnQS8Wsk5l7SCAhtsVlRkM7U2NNhsrUuWZDtn3bdunXVpLYhx66uqsp5DL7+c/PmXLbNR3S+8YFVubW02PfnIkfb0deSIVcN98IFVYYUDBfv2hS99yda6OHYs+fRTyIODc65r5eVWrTR8OGzffu7OVdS/PwwcGHvfmDH2M9mqpcZGCzy33GIDAIcMsYAauv56qx574gmrPjp16sxR5HV1VmX39NPJpZ9iHhycc4mZOdPGGIQ30VxzySUWOJJtlH7qKWvTmDMn9v6CAli40GasfeQRG6Q3dmzH/hEjrKH9scdsyvMs8+DgnHNgjcNVVck9OZw6Zd/4b7zRBtfFM3eu9b7asSP2QMF777UlWj8Gq9l5cHDOudCYMdZW0N2pNDZvhrfftqqhsykp6RjVHWtiwuuvtyeIhx7qGBGeJR4cnHMuFFaZdbdqac0auOgie3Loym23WbfZGTOi94nY00NTkzVcZ5GPc3DOudCIEdCrl1UtTZ8e/7jmZgsIpaX2NLBpk/XeKkrgljp0qA3qi2f6dJtF9lvfspXtsrQWtgcH55wLnXceXHll108Od91lq82FCgvhy19OTR4KCuCee6xhe+1aa6fIAq9Wcs65SBMmWHB4/fXY+5ubbTzC4sU2ncerr9pr6NDU5aG21kae19XBT3+auvN2gwcH55yLdOut1s30/vtj71+xwn4uWmRrTQwbltrAAFa1tXGjzVdVUxO9il0GeHBwzrlIAwdagHjmmejR0m1tNhhw5ky4+OL05qOkBLZssbW0v/AFGx+RQR4cnHOuszvvtJvzN7955vbVqy1A3H57ZvJRWmoLFBUXdzyxZEhCwUFEJovIayLSKCJ3xznmiyKyX0T2iciPIrZvEZFjIrKx0/GXisguEXldRNYFq8whIv8QvG8M9g9J/vKccy4JF1xgAaK+3mZWBRt3sHy5zZpaWZnZvMyYYXkJZ3fNgC6Dg4gUAk8CNwDDgVoRGd7pmArgHmCsqn4K+HrE7u8As2Oc+mFgmapWAEeB+cH2+cBRVR0KLAuOc865zFq0CMrKYPZsW2v66qutMTpTTw2RampsUsAMtj0k8uQwCmhU1QOqegJ4FqjudMwtwJOqehRAVf++ooWqbgPaIw8O1oieAISjPNYCYafi6uA9wf6J4ZrSzjmXMcXF8Oij8NFH1nPp9GnrXjplSubzMn68tYWsW5exJBMZ5zAIeDPifQvwr52OuQxARH4FFAL3q+qWs5yzP3BMVcPx4S1BOmekp6qnROT94Pg/R55ARBYACwAuTnfDkHMuP918c8d0F9lUWGiN4KtWWZtHSUnak0zkySHWt/bOa9kVARXANUAtsFpESpM8ZyLpoaqrVLVSVSvLysrOkpRzzuWAWbNsttb6+owkl0hwaAHKI94PBg7FOKZeVU+q6kHgNSxYxPNnoFREwieXyHP+Pb1g/z8C7yWQT+ecy12jR9u6GpFVSzt32joSaZBIcNgNVAS9i3oDs4ANnY55DrgWQEQGYNVMB+KdMFiebjsQzjw1BwjD4YbgPcH+XwbHO+dc/ioosIbprVttJb5x46wt4rHH0pNcVwcE7QILga3AK8CPVXWfiDwoItOCw7YC74rIfuymf6eqvgsgIjuB/8YalltEZFLwmbuAxSLSiLUprAm2rwH6B9sXAzG7zjrnXN6pqbEutTU18MYbNvYhTcFBcuFLeWVlpTY0NGQ7G845l16q8MADVr00ezb07t2j04nIS6oac9CGz8rqnHPnCpH4cz6lmE+f4ZxzLooHB+ecc1E8ODjnnIviwcE551wUDw7OOeeieHBwzjkXxYODc865KB4cnHPORcmJEdIi0gr8KcmPD6DTdOB5wK85P/g154eeXPMlqhpzWuucCA49ISIN8YaP5yq/5vzg15wf0nXNXq3knHMuigcH55xzUTw4wKpsZyAL/Jrzg19zfkjLNed9m4Nzzrlo/uTgnHMuigcH55xzUfI6OIjIZBF5TUQaRSQnlyMVkXIR2S4ir4jIPhFZFGy/QESeF5HXg5/nZzuvqSQihSKyR0Q2Bu8vFZFdwfWuC9ZDzxkiUioi60Xk1aCsx+RBGd8W/E3/UUSeEZHzcq2cReQpETkiIn+M2BazXMWsCO5ne0Xkyp6knbfBQUQKgSeBG4DhQK2IDM9urtLiFHC7ql4BjAa+Glzn3cA2Va0AtpF7a3UvwtY8Dz0MLAuu9ygwPyu5Sp/lwBZVvRz4LHbtOVvGIjIIuBWoVNVPA4XALHKvnL8PTO60LV653gBUBK8FwMqeJJy3wQEYBTSq6gFVPQE8C1RnOU8pp6qHVfX3we/t2E1jEHata4PD1gLTs5PD1BORwcBNwOrgvQATgPXBIbl2vSXAeGANgKqeUNVj5HAZB4qAPiJSBPQFDpNj5ayqLwLvddocr1yrgR+o+S1QKiIXJZt2PgeHQcCbEe9bgm05S0SGAJ8DdgH/pKqHwQIIcGH2cpZyjwNLgI+C9/2BY6p6Knifa2X9SaAVeDqoSlstIv3I4TJW1beAR4FmLCi8D7xEbpdzKF65pvSels/BQWJsy9l+vSJSDPwP8HVVbct2ftJFRKYAR1T1pcjNMQ7NpbIuAq4EVqrq54Dj5FAVUixBPXs1cCnwz0A/rFqls1wq566k9O88n4NDC1Ae8X4wcChLeUkrEemFBYYfqupPgs3vhI+cwc8j2cpfio0FponIG1hV4QTsSaI0qH6A3CvrFqBFVXcF79djwSJXyxjgOuCgqraq6kngJ0AVuV3OoXjlmtJ7Wj4Hh91ARdC7oTfWmLUhy3lKuaC+fQ3wiqp+N2LXBmBO8PscoD7TeUsHVb1HVQer6hCsTH+pqv8GbAdmBIflzPUCqOrbwJsiMizYNBHYT46WcaAZGC0ifYO/8fCac7acI8Qr1w3AzUGvpdHA+2H1UzLyeoS0iNyIfassBJ5S1aVZzlLKichVwE7gZTrq4O/F2h1+DFyM/UebqaqdG77OaSJyDXCHqk4RkU9iTxIXAHuAf1fVv2Uzf6kkIv+CNcD3Bg4A87AvfzlbxiLyAFCD9cjbA9Rhdew5U84i8gxwDTYt9zvAfcBzxCjXIEg+gfVu+gswT1Ubkk47n4ODc8652PK5Wsk551wcHhycc85F8eDgnHMuigcH55xzUTw4OOeci+LBwTnnXBQPDs4556L8P8LGWli00WyPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=100\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AdjacencyTransformer_2:\n\tMissing key(s) in state_dict: \"decoder.layers.3.self_attn.in_proj_weight\", \"decoder.layers.3.self_attn.in_proj_bias\", \"decoder.layers.3.self_attn.out_proj.weight\", \"decoder.layers.3.self_attn.out_proj.bias\", \"decoder.layers.3.multihead_attn.in_proj_weight\", \"decoder.layers.3.multihead_attn.in_proj_bias\", \"decoder.layers.3.multihead_attn.out_proj.weight\", \"decoder.layers.3.multihead_attn.out_proj.bias\", \"decoder.layers.3.linear1.weight\", \"decoder.layers.3.linear1.bias\", \"decoder.layers.3.linear2.weight\", \"decoder.layers.3.linear2.bias\", \"decoder.layers.3.norm1.weight\", \"decoder.layers.3.norm1.bias\", \"decoder.layers.3.norm2.weight\", \"decoder.layers.3.norm2.bias\", \"decoder.layers.3.norm3.weight\", \"decoder.layers.3.norm3.bias\", \"decoder.layers.4.self_attn.in_proj_weight\", \"decoder.layers.4.self_attn.in_proj_bias\", \"decoder.layers.4.self_attn.out_proj.weight\", \"decoder.layers.4.self_attn.out_proj.bias\", \"decoder.layers.4.multihead_attn.in_proj_weight\", \"decoder.layers.4.multihead_attn.in_proj_bias\", \"decoder.layers.4.multihead_attn.out_proj.weight\", \"decoder.layers.4.multihead_attn.out_proj.bias\", \"decoder.layers.4.linear1.weight\", \"decoder.layers.4.linear1.bias\", \"decoder.layers.4.linear2.weight\", \"decoder.layers.4.linear2.bias\", \"decoder.layers.4.norm1.weight\", \"decoder.layers.4.norm1.bias\", \"decoder.layers.4.norm2.weight\", \"decoder.layers.4.norm2.bias\", \"decoder.layers.4.norm3.weight\", \"decoder.layers.4.norm3.bias\", \"decoder.layers.5.self_attn.in_proj_weight\", \"decoder.layers.5.self_attn.in_proj_bias\", \"decoder.layers.5.self_attn.out_proj.weight\", \"decoder.layers.5.self_attn.out_proj.bias\", \"decoder.layers.5.multihead_attn.in_proj_weight\", \"decoder.layers.5.multihead_attn.in_proj_bias\", \"decoder.layers.5.multihead_attn.out_proj.weight\", \"decoder.layers.5.multihead_attn.out_proj.bias\", \"decoder.layers.5.linear1.weight\", \"decoder.layers.5.linear1.bias\", \"decoder.layers.5.linear2.weight\", \"decoder.layers.5.linear2.bias\", \"decoder.layers.5.norm1.weight\", \"decoder.layers.5.norm1.bias\", \"decoder.layers.5.norm2.weight\", \"decoder.layers.5.norm2.bias\", \"decoder.layers.5.norm3.weight\", \"decoder.layers.5.norm3.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"encoder.layers.4.self_attn.in_proj_weight\", \"encoder.layers.4.self_attn.in_proj_bias\", \"encoder.layers.4.self_attn.out_proj.weight\", \"encoder.layers.4.self_attn.out_proj.bias\", \"encoder.layers.4.linear1.weight\", \"encoder.layers.4.linear1.bias\", \"encoder.layers.4.linear2.weight\", \"encoder.layers.4.linear2.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.5.self_attn.in_proj_weight\", \"encoder.layers.5.self_attn.in_proj_bias\", \"encoder.layers.5.self_attn.out_proj.weight\", \"encoder.layers.5.self_attn.out_proj.bias\", \"encoder.layers.5.linear1.weight\", \"encoder.layers.5.linear1.bias\", \"encoder.layers.5.linear2.weight\", \"encoder.layers.5.linear2.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"lin2.0.weight\", \"lin2.0.bias\". \n\tUnexpected key(s) in state_dict: \"positional_encoding.pos_embedding\". \n\tsize mismatch for decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for decoder.layers.0.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for decoder.layers.0.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm3.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm3.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for decoder.layers.1.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for decoder.layers.1.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm3.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm3.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.multihead_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.2.multihead_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.2.multihead_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.2.multihead_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for decoder.layers.2.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for decoder.layers.2.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm3.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm3.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for encoder.layers.0.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for encoder.layers.0.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for encoder.layers.1.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for encoder.layers.1.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for encoder.layers.2.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for encoder.layers.2.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1fcb3e326643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#a=[0.1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AttTrack24.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconvert_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1605\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AdjacencyTransformer_2:\n\tMissing key(s) in state_dict: \"decoder.layers.3.self_attn.in_proj_weight\", \"decoder.layers.3.self_attn.in_proj_bias\", \"decoder.layers.3.self_attn.out_proj.weight\", \"decoder.layers.3.self_attn.out_proj.bias\", \"decoder.layers.3.multihead_attn.in_proj_weight\", \"decoder.layers.3.multihead_attn.in_proj_bias\", \"decoder.layers.3.multihead_attn.out_proj.weight\", \"decoder.layers.3.multihead_attn.out_proj.bias\", \"decoder.layers.3.linear1.weight\", \"decoder.layers.3.linear1.bias\", \"decoder.layers.3.linear2.weight\", \"decoder.layers.3.linear2.bias\", \"decoder.layers.3.norm1.weight\", \"decoder.layers.3.norm1.bias\", \"decoder.layers.3.norm2.weight\", \"decoder.layers.3.norm2.bias\", \"decoder.layers.3.norm3.weight\", \"decoder.layers.3.norm3.bias\", \"decoder.layers.4.self_attn.in_proj_weight\", \"decoder.layers.4.self_attn.in_proj_bias\", \"decoder.layers.4.self_attn.out_proj.weight\", \"decoder.layers.4.self_attn.out_proj.bias\", \"decoder.layers.4.multihead_attn.in_proj_weight\", \"decoder.layers.4.multihead_attn.in_proj_bias\", \"decoder.layers.4.multihead_attn.out_proj.weight\", \"decoder.layers.4.multihead_attn.out_proj.bias\", \"decoder.layers.4.linear1.weight\", \"decoder.layers.4.linear1.bias\", \"decoder.layers.4.linear2.weight\", \"decoder.layers.4.linear2.bias\", \"decoder.layers.4.norm1.weight\", \"decoder.layers.4.norm1.bias\", \"decoder.layers.4.norm2.weight\", \"decoder.layers.4.norm2.bias\", \"decoder.layers.4.norm3.weight\", \"decoder.layers.4.norm3.bias\", \"decoder.layers.5.self_attn.in_proj_weight\", \"decoder.layers.5.self_attn.in_proj_bias\", \"decoder.layers.5.self_attn.out_proj.weight\", \"decoder.layers.5.self_attn.out_proj.bias\", \"decoder.layers.5.multihead_attn.in_proj_weight\", \"decoder.layers.5.multihead_attn.in_proj_bias\", \"decoder.layers.5.multihead_attn.out_proj.weight\", \"decoder.layers.5.multihead_attn.out_proj.bias\", \"decoder.layers.5.linear1.weight\", \"decoder.layers.5.linear1.bias\", \"decoder.layers.5.linear2.weight\", \"decoder.layers.5.linear2.bias\", \"decoder.layers.5.norm1.weight\", \"decoder.layers.5.norm1.bias\", \"decoder.layers.5.norm2.weight\", \"decoder.layers.5.norm2.bias\", \"decoder.layers.5.norm3.weight\", \"decoder.layers.5.norm3.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\", \"encoder.layers.4.self_attn.in_proj_weight\", \"encoder.layers.4.self_attn.in_proj_bias\", \"encoder.layers.4.self_attn.out_proj.weight\", \"encoder.layers.4.self_attn.out_proj.bias\", \"encoder.layers.4.linear1.weight\", \"encoder.layers.4.linear1.bias\", \"encoder.layers.4.linear2.weight\", \"encoder.layers.4.linear2.bias\", \"encoder.layers.4.norm1.weight\", \"encoder.layers.4.norm1.bias\", \"encoder.layers.4.norm2.weight\", \"encoder.layers.4.norm2.bias\", \"encoder.layers.5.self_attn.in_proj_weight\", \"encoder.layers.5.self_attn.in_proj_bias\", \"encoder.layers.5.self_attn.out_proj.weight\", \"encoder.layers.5.self_attn.out_proj.bias\", \"encoder.layers.5.linear1.weight\", \"encoder.layers.5.linear1.bias\", \"encoder.layers.5.linear2.weight\", \"encoder.layers.5.linear2.bias\", \"encoder.layers.5.norm1.weight\", \"encoder.layers.5.norm1.bias\", \"encoder.layers.5.norm2.weight\", \"encoder.layers.5.norm2.bias\", \"lin2.0.weight\", \"lin2.0.bias\". \n\tUnexpected key(s) in state_dict: \"positional_encoding.pos_embedding\". \n\tsize mismatch for decoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.multihead_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.0.multihead_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.0.multihead_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.0.multihead_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for decoder.layers.0.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for decoder.layers.0.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm3.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.0.norm3.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.multihead_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.1.multihead_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.1.multihead_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.1.multihead_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for decoder.layers.1.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for decoder.layers.1.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm3.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.1.norm3.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.multihead_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for decoder.layers.2.multihead_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for decoder.layers.2.multihead_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for decoder.layers.2.multihead_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for decoder.layers.2.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for decoder.layers.2.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm3.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for decoder.layers.2.norm3.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for encoder.layers.0.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for encoder.layers.0.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for encoder.layers.0.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for encoder.layers.0.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.0.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for encoder.layers.1.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for encoder.layers.1.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for encoder.layers.1.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for encoder.layers.1.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.1.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_weight: copying a param with shape torch.Size([72, 24]) from checkpoint, the shape in current model is torch.Size([450, 150]).\n\tsize mismatch for encoder.layers.2.self_attn.in_proj_bias: copying a param with shape torch.Size([72]) from checkpoint, the shape in current model is torch.Size([450]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([24, 24]) from checkpoint, the shape in current model is torch.Size([150, 150]).\n\tsize mismatch for encoder.layers.2.self_attn.out_proj.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.linear1.weight: copying a param with shape torch.Size([512, 24]) from checkpoint, the shape in current model is torch.Size([512, 150]).\n\tsize mismatch for encoder.layers.2.linear2.weight: copying a param with shape torch.Size([24, 512]) from checkpoint, the shape in current model is torch.Size([150, 512]).\n\tsize mismatch for encoder.layers.2.linear2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm1.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm1.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm2.weight: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150]).\n\tsize mismatch for encoder.layers.2.norm2.bias: copying a param with shape torch.Size([24]) from checkpoint, the shape in current model is torch.Size([150])."
     ]
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon\n",
    "run=14\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "a=0.1\n",
    "pp_A = complete_postprocess(Ad,d,a)\n",
    "\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
