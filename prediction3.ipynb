{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src1_batch[3])\n",
    "    #print('src2_batch',src2_batch[3])\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:],src1_batch[:,0,:].size())\n",
    "    #print('src2',src2_batch[:,0,:],src2_batch[:,0,:].size())\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,75) ##100 total data size\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(75,100)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    m_Ad = []\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        m_Ad.append(np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy()))\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ad = postprocess_MinCostAss(m_Ad,a)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.1878,  0.0051, -1.4109,  ..., -0.9412, -0.5882, -0.0784],\n",
       "         [-0.8047,  0.0157,  0.2383,  ...,  0.0000,  0.0000, -0.0784],\n",
       "         ...,\n",
       "         [-0.3630,  0.1452,  0.4475,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.2601,  0.2395, -1.2828,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0675,  0.3647, -0.0084,  ...,  0.0000,  0.0000, -0.0392]]),\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0992,  0.2476, -1.2270,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.7703, -0.2403,  0.1330,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.2172,  0.0921, -1.2554,  ..., -0.2745, -0.0392,  0.0000],\n",
       "         [-0.1938,  0.8784, -0.2188,  ..., -0.0784,  0.3137, -1.8039],\n",
       "         [-0.4520,  0.1002,  0.0051,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0.]]),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 0.0676, 0.1691, 0.1457, 0.1342, 1.0000, 0.0917, 0.0488],\n",
       "         [1.0000, 0.0469, 0.2205, 0.0760, 0.1181, 0.0475, 0.0472, 0.0278, 1.0000],\n",
       "         [1.0000, 0.0844, 1.0000, 0.3050, 0.7305, 0.1406, 0.0715, 0.0599, 0.2402],\n",
       "         [1.0000, 0.2811, 0.2227, 1.0000, 1.0000, 1.0000, 0.1664, 0.2087, 0.0702],\n",
       "         [1.0000, 0.1734, 0.0631, 0.2156, 0.1213, 0.6732, 0.1047, 1.0000, 0.0303],\n",
       "         [1.0000, 0.1594, 0.7826, 1.0000, 1.0000, 0.2957, 0.1217, 0.0943, 0.1366],\n",
       "         [1.0000, 1.0000, 0.0667, 0.2210, 0.1540, 0.2242, 1.0000, 0.1673, 0.0412],\n",
       "         [1.0000, 0.3220, 0.1257, 1.0000, 0.3756, 1.0000, 0.1689, 0.4587, 0.0494]],\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = False, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05):\n",
    "        super(AdjacencyTransformer_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = src_t1\n",
    "        src2_emb = src_t2\n",
    "        #print('src1',src1_emb.size())\n",
    "        #print('src2',src2_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        #print('out1',out1.size())\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        #print('out_dec1',out_dec1.size())\n",
    "        #out_dec1=self.lin2(out_dec1)\n",
    "        #print('out_dec1b',out_dec1.size())\n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        #print('z',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.776, Val loss: 4.257, Epoch time = 3.062s\n",
      "Epoch: 2, Train loss: 4.502, Val loss: 4.518, Epoch time = 3.326s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9379743c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack_2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack_2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=10000\n",
    "plt.plot(np.convolve(loss_over_time, np.ones(N)/N, mode='valid'),c='red',label='train loss')\n",
    "plt.plot(np.convolve(test_error, np.ones(N)/N, mode='valid'),label='test loss')\n",
    "plt.title('Running mean of loss over epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k--- 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f937be88190>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 31, 150])\n",
      "L 2.725292682647705\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.INFEASIBLE\n",
      "y tensor([[1., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "Ad tensor([[1.0000e+00, 5.0761e-23, 1.0000e+00, 5.7588e-15, 1.4653e-17],\n",
      "        [1.0000e+00, 5.4674e-21, 2.0984e-19, 2.9731e-19, 1.5716e-20],\n",
      "        [2.3864e-22, 2.6208e-19, 4.8172e-12, 1.0000e+00, 7.1485e-20],\n",
      "        [5.3894e-16, 1.0000e+00, 2.4381e-14, 2.1126e-22, 5.3363e-20],\n",
      "        [8.6388e-20, 2.1492e-22, 1.7103e-12, 2.9883e-19, 1.0000e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "pp [[1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "d tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 0.0588, 1.0000, 0.0487, 0.0654],\n",
      "        [1.0000, 1.0000, 0.0572, 1.0000, 0.0549],\n",
      "        [1.0000, 1.0000, 0.0987, 1.0000, 0.0664],\n",
      "        [1.0000, 0.0518, 0.0846, 0.0569, 1.0000]], dtype=torch.float64)\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "0 [2, 3, 4]\n",
      "e [5 6 7]\n",
      "1 [2, 3, 4]\n",
      "e [5 6 7]\n",
      "2 [2, 3, 4]\n",
      "e [5 6 7]\n",
      "mid [5 6 7]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "M1 [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "0 [5 6 7]\n",
      "e [ 8  9 10]\n",
      "1 [5 6 7]\n",
      "e [ 8  9 10]\n",
      "2 [5 6 7]\n",
      "e [ 8  9 10]\n",
      "mid [ 8  9 10]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "M1 [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "0 [ 8  9 10]\n",
      "e [11 12 13]\n",
      "1 [ 8  9 10]\n",
      "e [11 12 13]\n",
      "2 [ 8  9 10]\n",
      "e [11 12 13]\n",
      "mid [11 12 13]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "M1 [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "0 [11 12 13]\n",
      "e [14 15 16]\n",
      "1 [11 12 13]\n",
      "e [14 15 16]\n",
      "2 [11 12 13]\n",
      "e [14 15 16]\n",
      "mid [14 15 16]\n",
      "M0 [[1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "M1 [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "0 [14 15 16]\n",
      "e [17 18 19 20]\n",
      "1 [14 15 16]\n",
      "e [17 18 19 20]\n",
      "2 [14 15 16]\n",
      "e [17 18 19 20]\n",
      "mid [17 18 19 20]\n",
      "M0 [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "0 [17 18 19 20]\n",
      "e [21 22 23 24]\n",
      "1 [17 18 19 20]\n",
      "e [21 22 23 24]\n",
      "2 [17 18 19 20]\n",
      "e [21 22 23 24]\n",
      "mid [21 22 23 24]\n",
      "M0 [[1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "M1 [[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "1 [21 22 23 24]\n",
      "e [25 26 27 28]\n",
      "2 [21 22 23 24]\n",
      "e [25 26 27 28]\n",
      "3 [21 22 23 24]\n",
      "e [25 26 27 28]\n",
      "mid [25 26 27 28]\n",
      "M0 [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [25 26 27 28]\n",
      "e [29 30 31 32]\n",
      "2 [25 26 27 28]\n",
      "e [29 30 31 32]\n",
      "3 [25 26 27 28]\n",
      "e [29 30 31 32]\n",
      "mid [29 30 31 32]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "1 [29 30 31 32]\n",
      "e [33 34 35]\n",
      "2 [29 30 31 32]\n",
      "e [33 34 35]\n",
      "3 [29 30 31 32]\n",
      "e [33 34 35]\n",
      "mid [33 34 35]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "0 [33 34 35]\n",
      "e [36 37 38]\n",
      "1 [33 34 35]\n",
      "e [36 37 38]\n",
      "2 [33 34 35]\n",
      "e [36 37 38]\n",
      "mid [36 37 38]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "M1 [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "0 [36 37 38]\n",
      "e [39 40 41]\n",
      "1 [36 37 38]\n",
      "e [39 40 41]\n",
      "2 [36 37 38]\n",
      "e [39 40 41]\n",
      "mid [39 40 41]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "0 [39 40 41]\n",
      "e [42 43 44]\n",
      "1 [39 40 41]\n",
      "e [42 43 44]\n",
      "2 [39 40 41]\n",
      "e [42 43 44]\n",
      "mid [42 43 44]\n",
      "M0 [[1. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "M1 [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "0 [42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "1 [42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "2 [42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "mid [45 46 47 48 49]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "1 [45 46 47 48 49]\n",
      "e [50 51 52 53]\n",
      "1 [45 46 47 48 49]\n",
      "e [50 51 52 53]\n",
      "2 [45 46 47 48 49]\n",
      "e [50 51 52 53]\n",
      "4 [45 46 47 48 49]\n",
      "e [50 51 52 53]\n",
      "mid [50 51 52 53]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "0 [50 51 52 53]\n",
      "e [54 55 56 57 58]\n",
      "0 [50 51 52 53]\n",
      "e [54 55 56 57 58]\n",
      "1 [50 51 52 53]\n",
      "e [54 55 56 57 58]\n",
      "2 [50 51 52 53]\n",
      "e [54 55 56 57 58]\n",
      "3 [50 51 52 53]\n",
      "e [54 55 56 57 58]\n",
      "mid [54 55 56 57 58]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "0 [54 55 56 57 58]\n",
      "e [59 60 61 62 63 64]\n",
      "1 [54 55 56 57 58]\n",
      "e [59 60 61 62 63 64]\n",
      "2 [54 55 56 57 58]\n",
      "e [59 60 61 62 63 64]\n",
      "3 [54 55 56 57 58]\n",
      "e [59 60 61 62 63 64]\n",
      "3 [54 55 56 57 58]\n",
      "e [59 60 61 62 63 64]\n",
      "4 [54 55 56 57 58]\n",
      "e [59 60 61 62 63 64]\n",
      "mid [59 60 61 62 63 64]\n",
      "M0 [[1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "1 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "2 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "3 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "4 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "4 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "5 [59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72]\n",
      "mid [65 66 67 68 69 70 71 72]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "1 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "2 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "4 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "5 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "6 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "7 [65 66 67 68 69 70 71 72]\n",
      "e [73 74 75 76 77 78 79 80 81]\n",
      "mid [73 74 75 76 77 78 79 80 81]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "1 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "2 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "3 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "4 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "6 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "8 [73 74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87 88]\n",
      "mid [82 83 84 85 86 87 88]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "M1 [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "0 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "1 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "2 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "3 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "3 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "4 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "5 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "6 [82 83 84 85 86 87 88]\n",
      "e [89 90 91 92 93 94 95 96]\n",
      "mid [89 90 91 92 93 94 95 96]\n",
      "M0 [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "0 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "1 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "2 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "3 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "4 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "5 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "6 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "7 [89 90 91 92 93 94 95 96]\n",
      "e [ 97  98  99 100 101 102 103 104 105]\n",
      "mid [ 97  98  99 100 101 102 103 104 105]\n",
      "M0 [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "1 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "2 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "3 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "4 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "5 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "6 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "7 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "8 [ 97  98  99 100 101 102 103 104 105]\n",
      "e [106 107 108 109 110 111 112 113 114]\n",
      "mid [106 107 108 109 110 111 112 113 114]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "1 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "2 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "3 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "4 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "5 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "6 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "7 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "8 [106 107 108 109 110 111 112 113 114]\n",
      "e [115 116 117 118 119 120 121 122 123]\n",
      "mid [115 116 117 118 119 120 121 122 123]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "1 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "2 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "3 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "4 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "5 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "7 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "8 [115 116 117 118 119 120 121 122 123]\n",
      "e [124 125 126 127 128 129 130 131]\n",
      "mid [124 125 126 127 128 129 130 131]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "0 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "1 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "2 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "3 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "4 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "5 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "6 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "6 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "7 [124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140]\n",
      "mid [132 133 134 135 136 137 138 139 140]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "0 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "1 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "2 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "3 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "4 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "5 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "6 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "7 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "8 [132 133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "mid [141 142 143 144 145 146 147 148 149]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "1 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "2 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "3 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "3 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "4 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "5 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "6 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "7 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "8 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159]\n",
      "mid [150 151 152 153 154 155 156 157 158 159]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "1 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "2 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "3 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "4 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "5 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "6 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "7 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "8 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "9 [150 151 152 153 154 155 156 157 158 159]\n",
      "e [160 161 162 163 164 165 166 167 168 169]\n",
      "mid [160 161 162 163 164 165 166 167 168 169]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "1 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "1 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "2 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "2 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "3 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "4 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "4 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "5 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "5 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "6 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "7 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "8 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "9 [160 161 162 163 164 165 166 167 168 169]\n",
      "e [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "mid [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "1 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "2 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "2 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "3 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "4 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "5 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "6 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "7 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "9 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "10 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "11 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "12 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "13 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "14 [170 171 172 173 174 175 176 177 178 179 180 181 182 183 184]\n",
      "e [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n",
      "mid [185 186 187 188 189 190 191 192 193 194 195 196 197 198 199]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recon\n",
    "run=83\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "val_loss = evaluate(transformer,loss_fn)\n",
    "print('L',val_loss)\n",
    "a=0.1\n",
    "pp_A = complete_postprocess(Ad,d,a)\n",
    "\n",
    "#print(src1.size())\n",
    "\n",
    "print('y',y[6])\n",
    "print('Ad',Ad[6])\n",
    "print('pp',pp_A[6])\n",
    "print('d',d[6])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e2c9c290de89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.990, Val loss: 2.800, Epoch time = 3.435s\n",
      "Epoch: 2, Train loss: 3.094, Val loss: 2.641, Epoch time = 3.707s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f937bcec590>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 31, 150])\n",
      "(12, 150)\n",
      "[[ 9.86416     2.0014303 ]\n",
      " [10.312352   -0.24273808]\n",
      " [10.51631     1.3104519 ]\n",
      " [10.872507    0.05946213]\n",
      " [10.891018    0.7087565 ]\n",
      " [ 9.138768    2.0187724 ]\n",
      " [ 8.462985    1.5162205 ]\n",
      " [ 8.660129    0.8742414 ]\n",
      " [ 9.163463    2.523605  ]\n",
      " [ 8.086525    0.22588328]\n",
      " [ 7.731403    1.2439451 ]\n",
      " [ 7.738533    0.66740245]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[11.102701   9.834718 ]\\n [10.975245  11.376655 ]\\n [11.55883   10.9941   ]\\n [10.942158  10.440168 ]\\n [10.304249  10.682447 ]\\n [10.096922  10.017049 ]\\n [10.49952   12.192604 ]\\n [ 8.663966  11.4105625]\\n [ 9.177266  12.255981 ]\\n [ 8.936496  10.613881 ]\\n [10.011719  11.911004 ]\\n [ 9.29462   11.477478 ]\\n [ 9.607173  10.698044 ]]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=95\n",
    "t= 8\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:]#[1:]\n",
    "src_t2=src_t2[:,t,:]#[1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:]#[1:]\n",
    "b=b[:,t,:]#[1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine',n_neighbors=4)\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(src)\n",
    "\n",
    "\n",
    "print(pca.singular_values_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
