{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import copy\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src1_batch[3])\n",
    "    #print('src2_batch',src2_batch[3])\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:],src1_batch[:,0,:].size())\n",
    "    #print('src2',src2_batch[:,0,:],src2_batch[:,0,:].size())\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,3) #!!!!!!!!!!##100 total data size\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,3) #!!!!!!!!\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "      \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        #print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        #print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    #print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    #print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        #print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    Ad_n = []\n",
    "    #Ad_n=copy.deepcopy(Ad)\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        \n",
    "        A_t,ill_flag=treshold(Ad[h],t=0.5)\n",
    "        \n",
    "        #print('ill_flag',ill_flag)\n",
    "        #print(Ad[h],A_t)\n",
    "        if ill_flag==True:\n",
    "            Ad[h]=np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy())\n",
    "            A_t = postprocess_MinCostAss(np.array([Ad[h]]),a)[0]\n",
    "            \n",
    "        #print(Ad[h],A_t)\n",
    "        Ad_n.append(A_t)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad_n\n",
    "\n",
    "def treshold(matrix, t):\n",
    "    z=np.where(matrix >= t, 1, 0)\n",
    "    \n",
    "    ill_flag=False\n",
    "\n",
    "      \n",
    "    if (~z.any(axis=0)).any() or any(np.sum(z[:,1:], axis=0)>1):\n",
    "        ill_flag=True\n",
    "          \n",
    "    return z,ill_flag\n",
    "\n",
    "def err_perc(a,b):\n",
    "    w=0\n",
    "    s=0\n",
    "    for i in range(len(a)):\n",
    "        m=a[i]-b[i].detach().numpy()\n",
    "        w=w+0.5*np.sum(np.abs(m))\n",
    "        s=s+np.size(m)\n",
    "    \n",
    "    \n",
    "    print('w,s',w,s)\n",
    "    \n",
    "    return w*100/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n",
      "[[0.87 0.05 0.08 0.77 0.11]\n",
      " [0.12 0.19 0.11 0.14 0.93]\n",
      " [0.12 0.45 0.89 0.23 0.05]\n",
      " [0.1  0.97 0.65 0.34 0.02]]\n",
      "True\n",
      "0.2222222222222222\n",
      "tensor(0.8848)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0938, -0.3220,  0.5815,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.6416, -0.4797,  0.2852,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.6139, -0.4950,  0.2619,  ..., -1.8431, -1.3333, -0.7059],\n",
       "         [ 0.4061, -0.5147,  0.3480,  ..., -1.0196, -1.4902, -0.5882],\n",
       "         [ 0.6263, -0.5565,  0.3212,  ...,  0.1176, -0.0392,  0.0000]]),\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.4657, -0.3383,  0.5594,  ..., -0.0784, -0.3922, -0.5490],\n",
       "         [ 0.0359, -0.5056,  0.5996,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.6449, -0.2314,  0.5671,  ..., -1.2157, -1.4902, -1.7647],\n",
       "         [ 0.4589, -0.4443,  0.4574,  ..., -0.0392,  0.0000,  0.0000],\n",
       "         [ 0.4465, -0.3453,  0.4494,  ..., -1.0980, -2.1961, -1.0980]]),\n",
       " tensor([[1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 0.1199, 1.0000, 0.5677, 1.0000, 0.3960, 0.1005, 0.2490, 0.1610,\n",
       "          0.2703, 1.0000, 0.1175, 0.1946, 0.0869],\n",
       "         [1.0000, 0.6778, 0.3225, 0.3206, 0.4699, 0.0980, 0.4548, 1.0000, 0.1944,\n",
       "          0.5756, 0.2172, 0.3724, 1.0000, 0.4102],\n",
       "         [1.0000, 1.0000, 0.0911, 0.1327, 0.1293, 0.0515, 0.3152, 0.4575, 0.1604,\n",
       "          0.2789, 0.0835, 0.8217, 0.6552, 1.0000],\n",
       "         [1.0000, 1.0000, 0.1110, 0.1965, 0.1771, 0.0648, 0.2330, 0.9143, 0.2649,\n",
       "          0.5226, 0.1088, 1.0000, 0.7654, 1.0000],\n",
       "         [1.0000, 0.2217, 1.0000, 1.0000, 1.0000, 0.2494, 0.1464, 0.6540, 0.2628,\n",
       "          0.6731, 1.0000, 0.2137, 0.4087, 0.1427],\n",
       "         [1.0000, 0.4482, 0.1346, 0.1180, 0.1487, 0.0535, 1.0000, 0.3069, 0.0948,\n",
       "          0.1820, 0.0939, 0.2060, 1.0000, 0.5861],\n",
       "         [1.0000, 0.2280, 0.1254, 0.5486, 0.2581, 0.1406, 0.0719, 0.3979, 1.0000,\n",
       "          0.8242, 0.1988, 0.4817, 0.1675, 0.1353],\n",
       "         [1.0000, 1.0000, 0.1620, 0.4548, 0.3320, 0.1004, 0.1592, 1.0000, 0.7248,\n",
       "          1.0000, 0.1841, 1.0000, 0.6246, 0.4375],\n",
       "         [1.0000, 0.2949, 0.3208, 1.0000, 1.0000, 0.2308, 0.1073, 1.0000, 1.0000,\n",
       "          1.0000, 0.5645, 0.4332, 0.3195, 0.1621],\n",
       "         [1.0000, 0.6293, 0.3496, 0.9658, 0.9866, 0.1449, 0.1899, 1.0000, 0.4852,\n",
       "          1.0000, 0.3574, 0.6461, 0.9764, 0.2909]], dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "#print(threshold_matrix(a, 0.2))\n",
    "print(a[:,1:])\n",
    "print(any(np.sum(a[:,1:], axis=0)>1))\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "c = np.array([[1, 0,0], [0, 1,0],[0, 0,1]])\n",
    "d = np.array([[1, 0,0], [0, 0,1],[0, 0,1]])\n",
    "\n",
    "\n",
    "m=c-d\n",
    "print(np.sum(np.abs(m))/np.size(m))\n",
    "\n",
    "\n",
    "c = torch.from_numpy(c).float()\n",
    "d = torch.from_numpy(d).float()\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = cross_entropy_loss(c, d)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = False, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05):\n",
    "        super(AdjacencyTransformer_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        self.lin2 = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = src_t1\n",
    "        src2_emb = src_t2\n",
    "        #print('src1',src1_emb.size())\n",
    "        #print('src2',src2_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        #print('out1',out1.size())\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        #print('out_dec1',out_dec1.size())\n",
    "        #out_dec1=self.lin2(out_dec1)\n",
    "        #print('out_dec1b',out_dec1.size())\n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        #print('z',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.053, Val loss: 4.422, Epoch time = 4.507s\n",
      "Epoch: 2, Train loss: 5.076, Val loss: 4.388, Epoch time = 4.170s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0a98040110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack_2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack_2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=10000\n",
    "plt.plot(np.convolve(loss_over_time, np.ones(N)/N, mode='valid'),c='red',label='train loss')\n",
    "plt.plot(np.convolve(test_error, np.ones(N)/N, mode='valid'),label='test loss')\n",
    "plt.title('Running mean of loss over epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k--- 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-60d97047ee88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAd_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mlo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w,s 14.0 2205\n",
      "err 1 0.6349206349206349\n",
      "w,s 5.5 2186\n",
      "err 2 0.2516010978956999\n"
     ]
    }
   ],
   "source": [
    "#recon+testerror\n",
    "for r in range(1,3):\n",
    "    src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=r)\n",
    "\n",
    "    #print(src1.size())\n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "    transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "    transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "    Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    #print('L',val_loss)\n",
    "    a=0.1\n",
    "    pp_A = complete_postprocess(Ad,d,a)\n",
    "    \n",
    "    err_p=err_perc(pp_A,y)\n",
    "    print('err',r,err_p)\n",
    "\n",
    "#print(src1.size())\n",
    "\n",
    "    #print('y',y[6])\n",
    "    #print('Ad',Ad[6])\n",
    "    #print('pp',pp_A[6])\n",
    "    #print('d',d[6])\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "    make_reconstructed_edgelist(pp_A,run=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=95\n",
    "t= 8\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:]#[1:]\n",
    "src_t2=src_t2[:,t,:]#[1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:]#[1:]\n",
    "b=b[:,t,:]#[1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine',n_neighbors=4)\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
