{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import copy\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #src1_batch=torch.stack(src1_batch)  \n",
    "    #src2_batch=torch.stack(src2_batch) \n",
    "        \n",
    "    #print('src_batch',src1_batch[3])\n",
    "    #print('src2_batch',src2_batch[3])\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=-100)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=-100)\n",
    "    \n",
    "    #print('src1size',src1_batch.size())\n",
    "    #print('src1',src1_batch[:,0,:],src1_batch[:,0,:].size())\n",
    "    #print('src2',src2_batch[:,0,:],src2_batch[:,0,:].size())\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,75) #!!!!!!!!!!##100 total data size\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed_n2v.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(75,100) #!!!!!!!!\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed_n2v.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "    #print('src_padding',src)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        \n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "\n",
    "class makeLinEmb:\n",
    "    def __init__(self,save=False):\n",
    "        self.save=save\n",
    "        \n",
    "    def forward(self,s:Tensor,\n",
    "                mask1:Tensor):\n",
    "        \n",
    "        #print('s',s[0],s.size())\n",
    "        #print('mask',mask1[0],mask1.size())\n",
    "        emb = []\n",
    "        for i in range(s.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            emb.append(s[i,0:n])\n",
    "        \n",
    "        #print('emb',emb[0],emb[0].size())\n",
    "        if self.save == True:\n",
    "            result = torch.cat(emb, dim=0)\n",
    "            print('res',result.size())\n",
    "            result_np = result.detach().numpy()\n",
    "            np.savetxt('./'+str(1)+'/'+'embed_n2v_lin.txt', result_np)\n",
    "        return emb\n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "                \n",
    "            #if i==10:\n",
    "             #   print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "      \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        #print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        #print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    #print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    #print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        #print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    Ad_n = []\n",
    "    #Ad_n=copy.deepcopy(Ad)\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        \n",
    "        A_t,ill_flag=treshold(Ad[h],t=0.5)\n",
    "        \n",
    "        #print('ill_flag',ill_flag)\n",
    "        #print(Ad[h],A_t)\n",
    "        if ill_flag==True:\n",
    "            Ad[h]=np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy())\n",
    "            A_t = postprocess_MinCostAss(np.array([Ad[h]]),a)[0]\n",
    "            \n",
    "        #print(Ad[h],A_t)\n",
    "        Ad_n.append(A_t)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad_n\n",
    "\n",
    "def treshold(matrix, t):\n",
    "    z=np.where(matrix >= t, 1, 0)\n",
    "    \n",
    "    ill_flag=False\n",
    "\n",
    "      \n",
    "    if (~z.any(axis=0)).any() or any(np.sum(z[:,1:], axis=0)>1):\n",
    "        ill_flag=True\n",
    "          \n",
    "    return z,ill_flag\n",
    "\n",
    "def err_perc(a,b):\n",
    "    w=0\n",
    "    s=0\n",
    "    for i in range(len(a)):\n",
    "        m=a[i]-b[i].detach().numpy()\n",
    "        w=w+np.sum(np.abs(m))\n",
    "        s=s+np.size(m)\n",
    "    \n",
    "    \n",
    "    print('w,s',w,s)\n",
    "    \n",
    "    return w*100/s\n",
    "\n",
    "\n",
    "\n",
    "'''mport numpy as np\n",
    "\n",
    "# Define a matrix with duplicates of the first row\n",
    "matrix = np.array([[1, 2, 3], [1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]])\n",
    "\n",
    "# Find all duplicates of the first row, except the first row\n",
    "duplicates = [row for row in matrix if np.array_equal(row, matrix[0]) and row is not matrix[0]]\n",
    "\n",
    "# Keep all rows that are not duplicates of the first row\n",
    "new_matrix = [row for row in matrix if row not in duplicates]\n",
    "\n",
    "# Convert the list of rows back to a matrix\n",
    "new_matrix = np.array(new_matrix)\n",
    "\n",
    "print(new_matrix)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 dim_out: int = 400,\n",
    "                 out = False, \n",
    "                 dropout: float = 0.1):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        self.Linemb=makeLinEmb()\n",
    "        \n",
    "        self.lina = nn.Sequential(\n",
    "            nn.Linear(input_dim, int(dim_out/2)),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "        self.lina_2 = nn.Sequential(\n",
    "            nn.Linear(int(dim_out/2), dim_out),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "        self.lina_3 = nn.Sequential(\n",
    "            nn.Linear(dim_out, dim_out),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.linb = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, int(dim_out/2)),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.linb_2 = nn.Sequential(\n",
    "        #    nn.Linear(int(dim_out/2), dim_out),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.linb_3 = nn.Sequential(\n",
    "        #    nn.Linear(dim_out, dim_out),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor,\n",
    "                run=0):\n",
    "        \n",
    "        #print('sizes1',src_t1.size())\n",
    "        \n",
    "        src_t1 =torch.transpose(src_t1,0,1)\n",
    "        src_t2 =torch.transpose(src_t2,0,1) \n",
    "        \n",
    "        #print('sizes1a',src_t1.size())\n",
    "        #print('sizes2a',src_t2.size())\n",
    "        \n",
    "        src_t1 = self.lina(src_t1)\n",
    "        src_t2 = self.lina(src_t2)\n",
    "        \n",
    "        src_t1 = self.lina_2(src_t1)\n",
    "        src_t2 = self.lina_2(src_t2)\n",
    "        \n",
    "        src_t1 = self.lina_3(src_t1)\n",
    "        src_t2 = self.lina_3(src_t2)\n",
    "        \n",
    "        src_t2 = torch.transpose(src_t2,1,2)\n",
    "        \n",
    "        #print('sizes1',src_t1.size())\n",
    "        #print('sizes2',src_t2.size())\n",
    "\n",
    "        \n",
    "        z=self.sig(torch.bmm(src_t1,src_t2))\n",
    "        \n",
    "        #print('sizesz',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "        \n",
    "        emb=self.Linemb.forward(src_t1,src_padding_mask1)\n",
    "\n",
    "       \n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=50\n",
    "\n",
    "dim_out= 400\n",
    "\n",
    "\n",
    "\n",
    "lin = LinearLayer(input_dim,dim_out)\n",
    "\n",
    "\n",
    "lin = lin.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(lin.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.794, Val loss: 4.890, Epoch time = 2.936s\n",
      "Epoch: 2, Train loss: 3.443, Val loss: 4.379, Epoch time = 3.155s\n",
      "Epoch: 3, Train loss: 3.386, Val loss: 4.000, Epoch time = 3.021s\n",
      "Epoch: 4, Train loss: 3.236, Val loss: 4.666, Epoch time = 2.892s\n",
      "Epoch: 5, Train loss: 3.752, Val loss: 4.687, Epoch time = 2.870s\n",
      "Epoch: 6, Train loss: 4.165, Val loss: 3.760, Epoch time = 2.827s\n",
      "Epoch: 7, Train loss: 3.677, Val loss: 3.828, Epoch time = 2.757s\n",
      "Epoch: 8, Train loss: 4.642, Val loss: 3.897, Epoch time = 2.804s\n",
      "Epoch: 9, Train loss: 4.195, Val loss: 4.100, Epoch time = 2.726s\n",
      "Epoch: 10, Train loss: 5.355, Val loss: 3.841, Epoch time = 2.741s\n",
      "Epoch: 11, Train loss: 4.003, Val loss: 5.184, Epoch time = 2.940s\n",
      "Epoch: 12, Train loss: 3.918, Val loss: 3.837, Epoch time = 3.071s\n",
      "Epoch: 13, Train loss: 3.617, Val loss: 4.254, Epoch time = 3.135s\n",
      "Epoch: 14, Train loss: 4.276, Val loss: 3.953, Epoch time = 2.776s\n",
      "Epoch: 15, Train loss: 4.334, Val loss: 4.093, Epoch time = 2.749s\n",
      "Epoch: 16, Train loss: 3.384, Val loss: 3.782, Epoch time = 2.866s\n",
      "Epoch: 17, Train loss: 3.315, Val loss: 3.948, Epoch time = 3.014s\n",
      "Epoch: 18, Train loss: 3.842, Val loss: 4.212, Epoch time = 2.693s\n",
      "Epoch: 19, Train loss: 4.492, Val loss: 4.155, Epoch time = 2.905s\n",
      "Epoch: 20, Train loss: 3.476, Val loss: 4.064, Epoch time = 3.039s\n",
      "Epoch: 21, Train loss: 4.009, Val loss: 3.920, Epoch time = 2.966s\n",
      "Epoch: 22, Train loss: 4.038, Val loss: 3.992, Epoch time = 2.891s\n",
      "Epoch: 23, Train loss: 4.271, Val loss: 4.140, Epoch time = 3.128s\n",
      "Epoch: 24, Train loss: 4.355, Val loss: 3.758, Epoch time = 3.775s\n",
      "Epoch: 25, Train loss: 4.219, Val loss: 4.549, Epoch time = 3.518s\n",
      "Epoch: 26, Train loss: 4.368, Val loss: 4.879, Epoch time = 2.919s\n",
      "Epoch: 27, Train loss: 4.398, Val loss: 4.402, Epoch time = 2.733s\n",
      "Epoch: 28, Train loss: 4.348, Val loss: 4.199, Epoch time = 2.823s\n",
      "Epoch: 29, Train loss: 3.807, Val loss: 4.437, Epoch time = 2.643s\n",
      "Epoch: 30, Train loss: 4.843, Val loss: 4.116, Epoch time = 2.591s\n",
      "Epoch: 31, Train loss: 3.753, Val loss: 4.244, Epoch time = 2.756s\n",
      "Epoch: 32, Train loss: 3.873, Val loss: 4.416, Epoch time = 2.479s\n",
      "Epoch: 33, Train loss: 3.621, Val loss: 3.966, Epoch time = 2.814s\n",
      "Epoch: 34, Train loss: 4.018, Val loss: 3.903, Epoch time = 2.671s\n",
      "Epoch: 35, Train loss: 3.616, Val loss: 4.742, Epoch time = 2.947s\n",
      "Epoch: 36, Train loss: 3.765, Val loss: 4.306, Epoch time = 2.715s\n",
      "Epoch: 37, Train loss: 4.221, Val loss: 5.092, Epoch time = 2.785s\n",
      "Epoch: 38, Train loss: 4.488, Val loss: 3.963, Epoch time = 2.580s\n",
      "Epoch: 39, Train loss: 3.832, Val loss: 4.022, Epoch time = 2.573s\n",
      "Epoch: 40, Train loss: 4.055, Val loss: 4.911, Epoch time = 2.747s\n",
      "Epoch: 41, Train loss: 4.384, Val loss: 4.133, Epoch time = 2.689s\n",
      "Epoch: 42, Train loss: 5.377, Val loss: 4.679, Epoch time = 2.698s\n",
      "Epoch: 43, Train loss: 4.781, Val loss: 4.346, Epoch time = 2.684s\n",
      "Epoch: 44, Train loss: 3.831, Val loss: 3.833, Epoch time = 2.558s\n",
      "Epoch: 45, Train loss: 4.431, Val loss: 4.025, Epoch time = 2.641s\n",
      "Epoch: 46, Train loss: 4.351, Val loss: 4.485, Epoch time = 2.465s\n",
      "Epoch: 47, Train loss: 3.971, Val loss: 4.222, Epoch time = 2.556s\n",
      "Epoch: 48, Train loss: 4.035, Val loss: 3.983, Epoch time = 2.651s\n",
      "Epoch: 49, Train loss: 3.670, Val loss: 4.076, Epoch time = 2.535s\n",
      "Epoch: 50, Train loss: 3.240, Val loss: 3.842, Epoch time = 2.526s\n",
      "Epoch: 51, Train loss: 3.866, Val loss: 4.346, Epoch time = 2.499s\n",
      "Epoch: 52, Train loss: 4.227, Val loss: 3.684, Epoch time = 2.469s\n",
      "Epoch: 53, Train loss: 3.911, Val loss: 4.141, Epoch time = 2.602s\n",
      "Epoch: 54, Train loss: 2.995, Val loss: 4.118, Epoch time = 2.592s\n",
      "Epoch: 55, Train loss: 3.838, Val loss: 4.653, Epoch time = 2.535s\n",
      "Epoch: 56, Train loss: 3.951, Val loss: 4.329, Epoch time = 2.621s\n",
      "Epoch: 57, Train loss: 3.912, Val loss: 4.083, Epoch time = 2.624s\n",
      "Epoch: 58, Train loss: 3.550, Val loss: 3.984, Epoch time = 2.844s\n",
      "Epoch: 59, Train loss: 4.283, Val loss: 3.760, Epoch time = 2.722s\n",
      "Epoch: 60, Train loss: 3.744, Val loss: 3.568, Epoch time = 2.680s\n",
      "Epoch: 61, Train loss: 3.858, Val loss: 3.995, Epoch time = 2.775s\n",
      "Epoch: 62, Train loss: 4.324, Val loss: 4.390, Epoch time = 3.425s\n",
      "Epoch: 63, Train loss: 4.439, Val loss: 3.922, Epoch time = 2.728s\n",
      "Epoch: 64, Train loss: 3.011, Val loss: 3.965, Epoch time = 2.631s\n",
      "Epoch: 65, Train loss: 3.655, Val loss: 3.898, Epoch time = 2.488s\n",
      "Epoch: 66, Train loss: 3.954, Val loss: 4.342, Epoch time = 2.490s\n",
      "Epoch: 67, Train loss: 4.148, Val loss: 4.017, Epoch time = 2.521s\n",
      "Epoch: 68, Train loss: 4.646, Val loss: 3.704, Epoch time = 2.332s\n",
      "Epoch: 69, Train loss: 3.979, Val loss: 4.137, Epoch time = 2.489s\n",
      "Epoch: 70, Train loss: 3.623, Val loss: 3.595, Epoch time = 2.576s\n",
      "Epoch: 71, Train loss: 4.149, Val loss: 4.492, Epoch time = 2.333s\n",
      "Epoch: 72, Train loss: 4.136, Val loss: 4.437, Epoch time = 2.512s\n",
      "Epoch: 73, Train loss: 3.027, Val loss: 4.111, Epoch time = 2.405s\n",
      "Epoch: 74, Train loss: 3.889, Val loss: 4.511, Epoch time = 2.509s\n",
      "Epoch: 75, Train loss: 4.428, Val loss: 3.870, Epoch time = 2.435s\n",
      "Epoch: 76, Train loss: 4.252, Val loss: 3.833, Epoch time = 2.495s\n",
      "Epoch: 77, Train loss: 3.572, Val loss: 3.882, Epoch time = 2.613s\n",
      "Epoch: 78, Train loss: 3.422, Val loss: 4.283, Epoch time = 2.462s\n",
      "Epoch: 79, Train loss: 3.999, Val loss: 4.201, Epoch time = 2.477s\n",
      "Epoch: 80, Train loss: 3.262, Val loss: 3.729, Epoch time = 2.795s\n",
      "Epoch: 81, Train loss: 4.099, Val loss: 3.740, Epoch time = 2.501s\n",
      "Epoch: 82, Train loss: 4.083, Val loss: 4.444, Epoch time = 2.484s\n",
      "Epoch: 83, Train loss: 3.626, Val loss: 4.271, Epoch time = 2.409s\n",
      "Epoch: 84, Train loss: 3.353, Val loss: 3.625, Epoch time = 2.410s\n",
      "Epoch: 85, Train loss: 3.972, Val loss: 3.775, Epoch time = 2.372s\n",
      "Epoch: 86, Train loss: 3.481, Val loss: 4.191, Epoch time = 2.442s\n",
      "Epoch: 87, Train loss: 4.126, Val loss: 3.618, Epoch time = 2.534s\n",
      "Epoch: 88, Train loss: 3.819, Val loss: 3.490, Epoch time = 2.401s\n",
      "Epoch: 89, Train loss: 3.397, Val loss: 3.971, Epoch time = 2.356s\n",
      "Epoch: 90, Train loss: 3.993, Val loss: 3.693, Epoch time = 2.513s\n",
      "Epoch: 91, Train loss: 3.921, Val loss: 3.503, Epoch time = 2.389s\n",
      "Epoch: 92, Train loss: 3.932, Val loss: 4.242, Epoch time = 2.252s\n",
      "Epoch: 93, Train loss: 3.933, Val loss: 4.340, Epoch time = 2.341s\n",
      "Epoch: 94, Train loss: 4.059, Val loss: 4.486, Epoch time = 2.438s\n",
      "Epoch: 95, Train loss: 4.675, Val loss: 3.381, Epoch time = 2.447s\n",
      "Epoch: 96, Train loss: 3.614, Val loss: 4.102, Epoch time = 2.292s\n",
      "Epoch: 97, Train loss: 3.814, Val loss: 3.920, Epoch time = 2.326s\n",
      "Epoch: 98, Train loss: 5.207, Val loss: 4.099, Epoch time = 2.314s\n",
      "Epoch: 99, Train loss: 3.872, Val loss: 3.533, Epoch time = 2.316s\n",
      "Epoch: 100, Train loss: 3.306, Val loss: 3.538, Epoch time = 2.381s\n",
      "Epoch: 101, Train loss: 3.889, Val loss: 4.298, Epoch time = 2.403s\n",
      "Epoch: 102, Train loss: 3.823, Val loss: 4.262, Epoch time = 2.282s\n",
      "Epoch: 103, Train loss: 3.889, Val loss: 4.141, Epoch time = 2.321s\n",
      "Epoch: 104, Train loss: 3.077, Val loss: 4.248, Epoch time = 2.324s\n",
      "Epoch: 105, Train loss: 3.781, Val loss: 3.543, Epoch time = 2.532s\n",
      "Epoch: 106, Train loss: 3.765, Val loss: 4.076, Epoch time = 2.441s\n",
      "Epoch: 107, Train loss: 3.650, Val loss: 4.291, Epoch time = 2.341s\n",
      "Epoch: 108, Train loss: 3.900, Val loss: 3.890, Epoch time = 2.366s\n",
      "Epoch: 109, Train loss: 4.081, Val loss: 3.867, Epoch time = 2.369s\n",
      "Epoch: 110, Train loss: 3.581, Val loss: 3.665, Epoch time = 2.542s\n",
      "Epoch: 111, Train loss: 3.747, Val loss: 3.896, Epoch time = 2.414s\n",
      "Epoch: 112, Train loss: 4.140, Val loss: 3.933, Epoch time = 2.307s\n",
      "Epoch: 113, Train loss: 4.122, Val loss: 3.609, Epoch time = 2.566s\n",
      "Epoch: 114, Train loss: 3.766, Val loss: 3.720, Epoch time = 2.478s\n",
      "Epoch: 115, Train loss: 3.604, Val loss: 3.726, Epoch time = 2.323s\n",
      "Epoch: 116, Train loss: 4.119, Val loss: 4.216, Epoch time = 2.611s\n",
      "Epoch: 117, Train loss: 3.345, Val loss: 3.879, Epoch time = 2.486s\n",
      "Epoch: 118, Train loss: 3.497, Val loss: 4.819, Epoch time = 2.410s\n",
      "Epoch: 119, Train loss: 4.074, Val loss: 3.935, Epoch time = 2.293s\n",
      "Epoch: 120, Train loss: 5.292, Val loss: 3.813, Epoch time = 2.378s\n",
      "Epoch: 121, Train loss: 3.845, Val loss: 3.205, Epoch time = 2.487s\n",
      "Epoch: 122, Train loss: 3.448, Val loss: 5.119, Epoch time = 2.480s\n",
      "Epoch: 123, Train loss: 3.920, Val loss: 3.605, Epoch time = 2.508s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train loss: 3.321, Val loss: 3.561, Epoch time = 2.557s\n",
      "Epoch: 125, Train loss: 3.866, Val loss: 3.488, Epoch time = 2.437s\n",
      "Epoch: 126, Train loss: 4.092, Val loss: 4.263, Epoch time = 2.413s\n",
      "Epoch: 127, Train loss: 3.981, Val loss: 3.866, Epoch time = 2.366s\n",
      "Epoch: 128, Train loss: 3.964, Val loss: 3.667, Epoch time = 2.421s\n",
      "Epoch: 129, Train loss: 4.161, Val loss: 4.370, Epoch time = 2.556s\n",
      "Epoch: 130, Train loss: 4.077, Val loss: 3.672, Epoch time = 2.443s\n",
      "Epoch: 131, Train loss: 3.927, Val loss: 4.055, Epoch time = 2.587s\n",
      "Epoch: 132, Train loss: 4.072, Val loss: 4.123, Epoch time = 2.519s\n",
      "Epoch: 133, Train loss: 4.320, Val loss: 3.778, Epoch time = 2.516s\n",
      "Epoch: 134, Train loss: 4.955, Val loss: 3.715, Epoch time = 2.807s\n",
      "Epoch: 135, Train loss: 3.247, Val loss: 4.168, Epoch time = 2.683s\n",
      "Epoch: 136, Train loss: 3.547, Val loss: 4.353, Epoch time = 2.491s\n",
      "Epoch: 137, Train loss: 3.961, Val loss: 4.160, Epoch time = 2.564s\n",
      "Epoch: 138, Train loss: 3.647, Val loss: 4.352, Epoch time = 2.486s\n",
      "Epoch: 139, Train loss: 3.631, Val loss: 3.992, Epoch time = 2.587s\n",
      "Epoch: 140, Train loss: 3.297, Val loss: 4.587, Epoch time = 2.738s\n",
      "Epoch: 141, Train loss: 4.534, Val loss: 3.885, Epoch time = 2.728s\n",
      "Epoch: 142, Train loss: 3.503, Val loss: 4.146, Epoch time = 2.541s\n",
      "Epoch: 143, Train loss: 3.817, Val loss: 3.757, Epoch time = 2.425s\n",
      "Epoch: 144, Train loss: 3.757, Val loss: 4.042, Epoch time = 2.391s\n",
      "Epoch: 145, Train loss: 4.038, Val loss: 3.867, Epoch time = 2.453s\n",
      "Epoch: 146, Train loss: 3.426, Val loss: 4.049, Epoch time = 2.585s\n",
      "Epoch: 147, Train loss: 4.213, Val loss: 3.942, Epoch time = 2.522s\n",
      "Epoch: 148, Train loss: 3.541, Val loss: 3.961, Epoch time = 2.466s\n",
      "Epoch: 149, Train loss: 3.779, Val loss: 3.953, Epoch time = 2.591s\n",
      "Epoch: 150, Train loss: 3.178, Val loss: 3.811, Epoch time = 2.433s\n",
      "Epoch: 151, Train loss: 3.466, Val loss: 4.318, Epoch time = 2.506s\n",
      "Epoch: 152, Train loss: 4.491, Val loss: 4.401, Epoch time = 2.790s\n",
      "Epoch: 153, Train loss: 3.467, Val loss: 3.604, Epoch time = 2.648s\n",
      "Epoch: 154, Train loss: 3.311, Val loss: 3.902, Epoch time = 2.632s\n",
      "Epoch: 155, Train loss: 3.861, Val loss: 4.281, Epoch time = 2.404s\n",
      "Epoch: 156, Train loss: 3.955, Val loss: 4.511, Epoch time = 2.649s\n",
      "Epoch: 157, Train loss: 3.846, Val loss: 3.870, Epoch time = 2.591s\n",
      "Epoch: 158, Train loss: 2.984, Val loss: 3.660, Epoch time = 2.661s\n",
      "Epoch: 159, Train loss: 4.091, Val loss: 3.951, Epoch time = 2.518s\n",
      "Epoch: 160, Train loss: 4.485, Val loss: 3.662, Epoch time = 2.710s\n",
      "Epoch: 161, Train loss: 3.643, Val loss: 4.427, Epoch time = 2.721s\n",
      "Epoch: 162, Train loss: 3.626, Val loss: 4.045, Epoch time = 3.180s\n",
      "Epoch: 163, Train loss: 4.105, Val loss: 4.850, Epoch time = 2.824s\n",
      "Epoch: 164, Train loss: 3.233, Val loss: 4.271, Epoch time = 2.816s\n",
      "Epoch: 165, Train loss: 3.425, Val loss: 3.675, Epoch time = 2.642s\n",
      "Epoch: 166, Train loss: 4.004, Val loss: 3.773, Epoch time = 2.526s\n",
      "Epoch: 167, Train loss: 4.027, Val loss: 4.332, Epoch time = 2.686s\n",
      "Epoch: 168, Train loss: 3.607, Val loss: 3.662, Epoch time = 3.167s\n",
      "Epoch: 169, Train loss: 3.527, Val loss: 4.366, Epoch time = 2.591s\n",
      "Epoch: 170, Train loss: 3.418, Val loss: 3.955, Epoch time = 2.487s\n",
      "Epoch: 171, Train loss: 4.292, Val loss: 4.020, Epoch time = 2.594s\n",
      "Epoch: 172, Train loss: 3.884, Val loss: 3.843, Epoch time = 2.427s\n",
      "Epoch: 173, Train loss: 3.149, Val loss: 3.569, Epoch time = 2.666s\n",
      "Epoch: 174, Train loss: 3.449, Val loss: 4.321, Epoch time = 2.533s\n",
      "Epoch: 175, Train loss: 4.198, Val loss: 3.825, Epoch time = 2.412s\n",
      "Epoch: 176, Train loss: 5.283, Val loss: 3.679, Epoch time = 2.498s\n",
      "Epoch: 177, Train loss: 4.251, Val loss: 4.539, Epoch time = 2.517s\n",
      "Epoch: 178, Train loss: 3.592, Val loss: 4.341, Epoch time = 2.553s\n",
      "Epoch: 179, Train loss: 3.926, Val loss: 4.377, Epoch time = 2.500s\n",
      "Epoch: 180, Train loss: 3.929, Val loss: 4.265, Epoch time = 2.447s\n",
      "Epoch: 181, Train loss: 3.822, Val loss: 3.920, Epoch time = 2.385s\n",
      "Epoch: 182, Train loss: 3.649, Val loss: 3.836, Epoch time = 2.618s\n",
      "Epoch: 183, Train loss: 3.591, Val loss: 3.697, Epoch time = 2.483s\n",
      "Epoch: 184, Train loss: 3.278, Val loss: 3.449, Epoch time = 2.571s\n",
      "Epoch: 185, Train loss: 2.913, Val loss: 3.750, Epoch time = 2.462s\n",
      "Epoch: 186, Train loss: 4.092, Val loss: 4.389, Epoch time = 2.590s\n",
      "Epoch: 187, Train loss: 3.124, Val loss: 4.339, Epoch time = 2.598s\n",
      "Epoch: 188, Train loss: 5.112, Val loss: 4.506, Epoch time = 3.220s\n",
      "Epoch: 189, Train loss: 4.028, Val loss: 4.131, Epoch time = 2.689s\n",
      "Epoch: 190, Train loss: 4.490, Val loss: 4.343, Epoch time = 2.643s\n",
      "Epoch: 191, Train loss: 4.065, Val loss: 4.832, Epoch time = 2.586s\n",
      "Epoch: 192, Train loss: 2.955, Val loss: 4.925, Epoch time = 2.574s\n",
      "Epoch: 193, Train loss: 2.937, Val loss: 3.583, Epoch time = 2.608s\n",
      "Epoch: 194, Train loss: 3.213, Val loss: 3.847, Epoch time = 2.704s\n",
      "Epoch: 195, Train loss: 4.106, Val loss: 3.864, Epoch time = 2.461s\n",
      "Epoch: 196, Train loss: 3.997, Val loss: 4.191, Epoch time = 2.683s\n",
      "Epoch: 197, Train loss: 3.828, Val loss: 4.203, Epoch time = 2.517s\n",
      "Epoch: 198, Train loss: 4.257, Val loss: 4.460, Epoch time = 2.536s\n",
      "Epoch: 199, Train loss: 4.057, Val loss: 4.547, Epoch time = 2.517s\n",
      "Epoch: 200, Train loss: 4.467, Val loss: 4.007, Epoch time = 2.555s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(lin, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(lin,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1031e99d10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=100\n",
    "plt.plot(np.convolve(loss_over_time, np.ones(N)/N, mode='valid'),c='red',label='train loss')\n",
    "plt.plot(np.convolve(test_error, np.ones(N)/N, mode='valid'),label='test loss')\n",
    "plt.title('Running mean of loss over epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-60d97047ee88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#a=[0.1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AttTrack_2.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconvert_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon+testerror\n",
    "for r in range(1,3):\n",
    "    src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=r)\n",
    "\n",
    "    #print(src1.size())\n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "    transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "    transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "    Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    #print('L',val_loss)\n",
    "    a=0.1\n",
    "    pp_A = complete_postprocess(Ad,d,a)\n",
    "    \n",
    "    err_p=err_perc(pp_A,y)\n",
    "    print('err',r,err_p)\n",
    "\n",
    "#print(src1.size())\n",
    "\n",
    "    #print('y',y[6])\n",
    "    #print('Ad',Ad[6])\n",
    "    #print('pp',pp_A[6])\n",
    "    #print('d',d[6])\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "    make_reconstructed_edgelist(pp_A,run=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=95\n",
    "t= 8\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:]#[1:]\n",
    "src_t2=src_t2[:,t,:]#[1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:]#[1:]\n",
    "b=b[:,t,:]#[1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine',n_neighbors=4)\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
