{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    m_Ad = []\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        m_Ad.append(np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy()))\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ad = postprocess_MinCostAss(m_Ad,a)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99, 0.87, 0.05, 0.08, 0.77, 0.11],\n",
       "       [0.05, 0.12, 0.19, 0.11, 0.14, 0.93],\n",
       "       [0.07, 0.12, 0.45, 0.89, 0.23, 0.05],\n",
       "       [0.04, 0.1 , 0.97, 0.65, 0.34, 0.02],\n",
       "       [1.  , 1.  , 1.  , 1.  , 1.  , 1.  ],\n",
       "       [1.  , 0.75, 0.07, 0.1 , 0.08, 0.8 ],\n",
       "       [1.  , 0.69, 0.07, 0.88, 0.34, 0.02],\n",
       "       [1.  , 0.1 , 0.9 , 0.05, 0.84, 0.02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1,\n",
    "                 out = False):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n",
    "    \n",
    "\n",
    "    \n",
    "class AdjacencyTransformer_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = True, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05):\n",
    "        super(AdjacencyTransformer_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        #self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "        self.out=out \n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        #src1_emb = self.positional_encoding(src_t1)\n",
    "        #src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src_t1,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src_t2,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec1 = self.lin(out_dec1)\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class AdjacencyNonlearn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyNonlearn, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "      \n",
    "        \n",
    "        out_dec2=torch.transpose(src_t1,0,1) \n",
    "        out_dec1=torch.transpose(src_t2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 24 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.417, Val loss: 5.068, Epoch time = 2.658s\n",
      "Epoch: 2, Train loss: 5.761, Val loss: 3.708, Epoch time = 4.127s\n",
      "Epoch: 3, Train loss: 4.235, Val loss: 4.139, Epoch time = 3.787s\n",
      "Epoch: 4, Train loss: 4.747, Val loss: 4.179, Epoch time = 3.748s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_xyr.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_xyr.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2beec45d0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack24.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack24.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1000\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k--- 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2b5a24050>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 31, 24])\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.INFEASIBLE\n",
      "y tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Ad tensor([[1.0000e+00, 4.1419e-09, 6.1942e-11, 1.0000e+00, 1.4290e-10],\n",
      "        [3.1586e-12, 6.4225e-11, 2.4955e-15, 2.0046e-11, 1.0000e+00],\n",
      "        [1.2560e-11, 9.4233e-13, 9.9979e-01, 3.4208e-14, 2.6567e-15],\n",
      "        [2.4875e-07, 1.0000e+00, 1.2060e-17, 1.1076e-09, 4.4607e-12]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "pp [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M0 [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "0 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "1 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "2 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "mid [5 6 7 8]\n",
      "M0 [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "1 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "3 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "mid [ 9 10 11 12]\n",
      "M0 [[1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "1 [ 9 10 11 12]\n",
      "e [13 14 15 16]\n",
      "2 [ 9 10 11 12]\n",
      "e [13 14 15 16]\n",
      "3 [ 9 10 11 12]\n",
      "e [13 14 15 16]\n",
      "mid [13 14 15 16]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n",
      "0 [13 14 15 16]\n",
      "e [17 18 19]\n",
      "1 [13 14 15 16]\n",
      "e [17 18 19]\n",
      "2 [13 14 15 16]\n",
      "e [17 18 19]\n",
      "mid [17 18 19]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "1 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "1 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "2 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "mid [20 21 22 23]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "1 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "2 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "3 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "mid [24 25 26 27]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "1 [24 25 26 27]\n",
      "e [28 29 30]\n",
      "2 [24 25 26 27]\n",
      "e [28 29 30]\n",
      "3 [24 25 26 27]\n",
      "e [28 29 30]\n",
      "mid [28 29 30]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 1. 0.]]\n",
      "0 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "1 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "2 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "2 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "mid [31 32 33 34]\n",
      "M0 [[1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "0 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "1 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "2 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "3 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "mid [35 36 37 38 39]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "0 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "1 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "2 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "3 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "4 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "mid [40 41 42 43 44]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "1 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "2 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "3 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "3 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "mid [45 46 47 48 49]\n",
      "M0 [[1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "0 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "1 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "2 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "3 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "4 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "mid [50 51 52 53 54 55]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "1 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "2 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "3 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "3 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "4 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "5 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "mid [56 57 58 59 60 61 62]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]]\n",
      "M1 [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]]\n",
      "0 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "2 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "3 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "4 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "5 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "6 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "6 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "mid [63 64 65 66 67 68 69]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "1 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "2 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "3 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "4 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "5 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "6 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "mid [70 71 72 73 74 75 76]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "1 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "2 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "3 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "4 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "4 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "5 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "6 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "mid [77 78 79 80 81 82 83 84]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "2 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "3 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "4 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "5 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "6 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "7 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "mid [85 86 87 88 89 90 91]\n",
      "M0 [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0.]]\n",
      "0 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "1 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "2 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "3 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "4 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "5 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "6 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "6 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "mid [ 92  93  94  95  96  97  98  99 100]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "1 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "2 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "3 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "4 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "5 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "7 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "8 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "mid [101 102 103 104 105 106 107]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "M1 [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "0 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "1 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "2 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "3 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "4 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "5 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "6 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "6 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "mid [108 109 110 111 112 113 114 115]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "1 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "2 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "3 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "4 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "5 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "7 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "mid [116 117 118 119 120 121 122]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "1 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "1 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "2 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "3 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "4 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "5 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "5 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "6 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "mid [123 124 125 126 127 128 129 130 131]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "0 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "2 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "3 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "4 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "5 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "6 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "6 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "7 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "8 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "mid [132 133 134 135 136 137 138 139 140 141]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "1 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "2 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "3 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "4 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "5 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "6 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "7 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "8 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "9 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "mid [142 143 144 145 146 147 148 149 150 151]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "1 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "2 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "3 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "4 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "5 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "6 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "7 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "8 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "9 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "mid [152 153 154 155 156 157 158 159 160 161]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "1 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "2 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "3 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "3 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "4 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "5 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "6 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "7 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "8 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "9 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "mid [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "1 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "2 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "3 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "4 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "5 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "5 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "6 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "7 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "9 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "10 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "11 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "mid [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "1 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "2 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "2 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "3 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "4 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "5 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "6 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "7 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "8 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "9 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "10 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "11 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "mid [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "1 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "1 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "2 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "3 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "4 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "5 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "6 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "7 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "8 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "9 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "10 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "11 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "12 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "mid [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "1 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "2 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "3 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "4 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "4 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "5 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "6 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "7 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "8 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "9 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "10 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "11 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "12 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "13 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "mid [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recon\n",
    "run=14\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "a=0.1\n",
    "pp_A = complete_postprocess(Ad,d,a)\n",
    "\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.897, Val loss: 2.730, Epoch time = 2.658s\n",
      "Epoch: 2, Train loss: 2.766, Val loss: 3.751, Epoch time = 3.278s\n",
      "Epoch: 3, Train loss: 3.015, Val loss: 2.989, Epoch time = 3.127s\n",
      "Epoch: 4, Train loss: 2.783, Val loss: 2.852, Epoch time = 3.004s\n",
      "Epoch: 5, Train loss: 2.655, Val loss: 2.588, Epoch time = 3.406s\n",
      "Epoch: 6, Train loss: 3.170, Val loss: 2.690, Epoch time = 3.338s\n",
      "Epoch: 7, Train loss: 2.897, Val loss: 2.928, Epoch time = 3.229s\n",
      "Epoch: 8, Train loss: 3.001, Val loss: 3.588, Epoch time = 3.210s\n",
      "Epoch: 9, Train loss: 2.661, Val loss: 2.642, Epoch time = 3.229s\n",
      "Epoch: 10, Train loss: 2.918, Val loss: 3.270, Epoch time = 3.342s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2b541ed50>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 24 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 31, 24])\n",
      "(13, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo/anaconda3/lib/python3.7/site-packages/umap/umap_.py:2345: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  \"n_neighbors is larger than the dataset size; truncating to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.102701   9.834718 ]\n",
      " [10.975245  11.376655 ]\n",
      " [11.55883   10.9941   ]\n",
      " [10.942158  10.440168 ]\n",
      " [10.304249  10.682447 ]\n",
      " [10.096922  10.017049 ]\n",
      " [10.49952   12.192604 ]\n",
      " [ 8.663966  11.4105625]\n",
      " [ 9.177266  12.255981 ]\n",
      " [ 8.936496  10.613881 ]\n",
      " [10.011719  11.911004 ]\n",
      " [ 9.29462   11.477478 ]\n",
      " [ 9.607173  10.698044 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=12\n",
    "t= 16\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:][1:]\n",
    "src_t2=src_t2[:,t,:][1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:][1:]\n",
    "b=b[:,t,:][1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine')\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 24)\n",
      "[[ -54.034996  126.53963 ]\n",
      " [ -59.16308  -105.41777 ]\n",
      " [ 122.20688    66.08065 ]\n",
      " [  59.08812   165.54088 ]\n",
      " [  44.324303  -57.763687]\n",
      " [ 239.39311    51.164993]\n",
      " [  44.51854  -171.7471  ]\n",
      " [ 157.55492  -146.32303 ]\n",
      " [ 143.82439   -33.385307]\n",
      " [ 182.88907   161.30112 ]\n",
      " [ -84.980606    7.409606]\n",
      " [ 252.54732   -67.98878 ]\n",
      " [  23.166214   42.472095]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[  63.499702 -287.3096  ]\\n [ 291.37546  -239.85983 ]\\n [ 145.31387   319.93018 ]\\n [ -42.45917   -69.62382 ]\\n [-253.67783   -41.07141 ]\\n [-157.32066  -250.51523 ]\\n [ 157.56784   106.591965]\\n [-240.93646   175.38722 ]\\n [ 148.04788   -84.04315 ]\\n [ -33.991825  122.39672 ]\\n [ -71.97189   332.94437 ]\\n [ 346.61322   206.81287 ]\\n [ 360.40482   -18.146132]]'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)\n",
    "\n",
    "'''[[ -54.034996  126.53963 ]\n",
    " [ -59.16308  -105.41777 ]\n",
    " [ 122.20688    66.08065 ]\n",
    " [  59.08812   165.54088 ]\n",
    " [  44.324303  -57.763687]\n",
    " [ 239.39311    51.164993]\n",
    " [  44.51854  -171.7471  ]\n",
    " [ 157.55492  -146.32303 ]\n",
    " [ 143.82439   -33.385307]\n",
    " [ 182.88907   161.30112 ]\n",
    " [ -84.980606    7.409606]\n",
    " [ 252.54732   -67.98878 ]\n",
    " [  23.166214   42.472095]]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lott\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
