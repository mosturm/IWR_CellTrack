{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch = [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            print('recon')\n",
    "        else:\n",
    "            E1,E2,A=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    return src1_batch, src2_batch,y_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(14)\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "        \n",
    "        A=A[id1-1]\n",
    "        #print(A)\n",
    "        A=A[:,id2-1]\n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "    elif recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "        \n",
    "        A=A[id1-1]\n",
    "        #print(A)\n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "       \n",
    "        #print(E1,E2)\n",
    "    else:\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = np.random.randint(14)\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "        \n",
    "        A=A[id1-1]\n",
    "        #print(A)\n",
    "        A=A[:,id2-1]\n",
    "        #print(run,t,id1,id2)\n",
    "        #print(E1,E2)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A) \n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y = collate_fn(26,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen):\n",
    "        self.pen=pen\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y = collate_fn(26,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            z2 = np.zeros(Ad[h].shape)\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            ind=torch.argmax(Ad[h][:,zero_col], dim=0)\n",
    "            for k,l in zip(ind,zero_col):\n",
    "                z2[k,l]=1\n",
    "            pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[1,2,3]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size= 24\n",
    "nhead= 6\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.120, Val loss: 3.399, Epoch time = 1.344s\n",
      "Epoch: 2, Train loss: 5.322, Val loss: 3.900, Epoch time = 1.650s\n",
      "Epoch: 3, Train loss: 4.733, Val loss: 3.428, Epoch time = 1.564s\n",
      "Epoch: 4, Train loss: 4.289, Val loss: 5.594, Epoch time = 1.392s\n",
      "Epoch: 5, Train loss: 4.611, Val loss: 3.796, Epoch time = 1.500s\n",
      "Epoch: 6, Train loss: 3.919, Val loss: 3.753, Epoch time = 1.457s\n",
      "Epoch: 7, Train loss: 4.217, Val loss: 3.452, Epoch time = 1.276s\n",
      "Epoch: 8, Train loss: 4.978, Val loss: 3.998, Epoch time = 1.506s\n",
      "Epoch: 9, Train loss: 4.627, Val loss: 3.778, Epoch time = 1.628s\n",
      "Epoch: 10, Train loss: 4.905, Val loss: 3.851, Epoch time = 1.479s\n",
      "Epoch: 11, Train loss: 4.678, Val loss: 3.748, Epoch time = 1.506s\n",
      "Epoch: 12, Train loss: 4.094, Val loss: 3.593, Epoch time = 1.405s\n",
      "Epoch: 13, Train loss: 4.767, Val loss: 4.025, Epoch time = 1.572s\n",
      "Epoch: 14, Train loss: 4.569, Val loss: 3.684, Epoch time = 1.476s\n",
      "Epoch: 15, Train loss: 4.287, Val loss: 3.428, Epoch time = 1.681s\n",
      "Epoch: 16, Train loss: 4.355, Val loss: 3.404, Epoch time = 1.379s\n",
      "Epoch: 17, Train loss: 4.476, Val loss: 3.649, Epoch time = 1.532s\n",
      "Epoch: 18, Train loss: 4.202, Val loss: 3.793, Epoch time = 1.437s\n",
      "Epoch: 19, Train loss: 4.033, Val loss: 3.429, Epoch time = 1.388s\n",
      "Epoch: 20, Train loss: 4.502, Val loss: 3.834, Epoch time = 1.602s\n",
      "Epoch: 21, Train loss: 4.049, Val loss: 3.915, Epoch time = 1.487s\n",
      "Epoch: 22, Train loss: 4.624, Val loss: 3.579, Epoch time = 1.497s\n",
      "Epoch: 23, Train loss: 4.258, Val loss: 3.583, Epoch time = 1.724s\n",
      "Epoch: 24, Train loss: 5.050, Val loss: 3.260, Epoch time = 1.723s\n",
      "Epoch: 25, Train loss: 4.346, Val loss: 3.485, Epoch time = 1.517s\n",
      "Epoch: 26, Train loss: 4.733, Val loss: 3.958, Epoch time = 1.398s\n",
      "Epoch: 27, Train loss: 5.148, Val loss: 3.471, Epoch time = 1.510s\n",
      "Epoch: 28, Train loss: 5.054, Val loss: 4.663, Epoch time = 1.645s\n",
      "Epoch: 29, Train loss: 4.227, Val loss: 3.278, Epoch time = 1.563s\n",
      "Epoch: 30, Train loss: 4.121, Val loss: 3.896, Epoch time = 1.481s\n",
      "Epoch: 31, Train loss: 4.209, Val loss: 3.393, Epoch time = 1.416s\n",
      "Epoch: 32, Train loss: 3.924, Val loss: 3.557, Epoch time = 1.561s\n",
      "Epoch: 33, Train loss: 5.506, Val loss: 3.370, Epoch time = 1.558s\n",
      "Epoch: 34, Train loss: 4.340, Val loss: 3.648, Epoch time = 1.317s\n",
      "Epoch: 35, Train loss: 4.309, Val loss: 3.895, Epoch time = 1.297s\n",
      "Epoch: 36, Train loss: 3.942, Val loss: 3.356, Epoch time = 1.269s\n",
      "Epoch: 37, Train loss: 4.545, Val loss: 3.931, Epoch time = 1.323s\n",
      "Epoch: 38, Train loss: 4.125, Val loss: 3.348, Epoch time = 1.529s\n",
      "Epoch: 39, Train loss: 4.349, Val loss: 3.531, Epoch time = 1.416s\n",
      "Epoch: 40, Train loss: 4.237, Val loss: 3.755, Epoch time = 1.346s\n",
      "Epoch: 41, Train loss: 5.829, Val loss: 3.734, Epoch time = 1.435s\n",
      "Epoch: 42, Train loss: 4.840, Val loss: 3.445, Epoch time = 1.526s\n",
      "Epoch: 43, Train loss: 4.964, Val loss: 4.116, Epoch time = 1.383s\n",
      "Epoch: 44, Train loss: 4.364, Val loss: 4.030, Epoch time = 1.599s\n",
      "Epoch: 45, Train loss: 4.359, Val loss: 3.542, Epoch time = 1.546s\n",
      "Epoch: 46, Train loss: 4.543, Val loss: 3.463, Epoch time = 1.511s\n",
      "Epoch: 47, Train loss: 4.560, Val loss: 3.506, Epoch time = 1.510s\n",
      "Epoch: 48, Train loss: 4.705, Val loss: 3.606, Epoch time = 1.561s\n",
      "Epoch: 49, Train loss: 4.667, Val loss: 3.540, Epoch time = 1.494s\n",
      "Epoch: 50, Train loss: 4.285, Val loss: 4.901, Epoch time = 1.369s\n",
      "Epoch: 51, Train loss: 4.639, Val loss: 3.700, Epoch time = 1.569s\n",
      "Epoch: 52, Train loss: 4.357, Val loss: 3.755, Epoch time = 1.451s\n",
      "Epoch: 53, Train loss: 4.758, Val loss: 3.768, Epoch time = 1.468s\n",
      "Epoch: 54, Train loss: 4.545, Val loss: 3.627, Epoch time = 1.624s\n",
      "Epoch: 55, Train loss: 4.848, Val loss: 3.722, Epoch time = 1.422s\n",
      "Epoch: 56, Train loss: 4.766, Val loss: 3.717, Epoch time = 1.527s\n",
      "Epoch: 57, Train loss: 4.469, Val loss: 4.944, Epoch time = 1.396s\n",
      "Epoch: 58, Train loss: 4.273, Val loss: 3.416, Epoch time = 1.415s\n",
      "Epoch: 59, Train loss: 3.960, Val loss: 3.334, Epoch time = 1.643s\n",
      "Epoch: 60, Train loss: 4.621, Val loss: 4.088, Epoch time = 1.548s\n",
      "Epoch: 61, Train loss: 4.410, Val loss: 3.456, Epoch time = 1.590s\n",
      "Epoch: 62, Train loss: 4.436, Val loss: 3.176, Epoch time = 1.512s\n",
      "Epoch: 63, Train loss: 4.849, Val loss: 4.179, Epoch time = 1.473s\n",
      "Epoch: 64, Train loss: 4.461, Val loss: 3.550, Epoch time = 1.472s\n",
      "Epoch: 65, Train loss: 4.369, Val loss: 3.830, Epoch time = 1.475s\n",
      "Epoch: 66, Train loss: 4.458, Val loss: 3.760, Epoch time = 1.321s\n",
      "Epoch: 67, Train loss: 4.103, Val loss: 3.410, Epoch time = 1.490s\n",
      "Epoch: 68, Train loss: 3.964, Val loss: 3.586, Epoch time = 1.628s\n",
      "Epoch: 69, Train loss: 4.314, Val loss: 3.342, Epoch time = 1.517s\n",
      "Epoch: 70, Train loss: 4.165, Val loss: 3.715, Epoch time = 1.517s\n",
      "Epoch: 71, Train loss: 4.848, Val loss: 3.462, Epoch time = 1.548s\n",
      "Epoch: 72, Train loss: 4.645, Val loss: 3.752, Epoch time = 1.283s\n",
      "Epoch: 73, Train loss: 4.604, Val loss: 3.532, Epoch time = 1.592s\n",
      "Epoch: 74, Train loss: 4.569, Val loss: 5.048, Epoch time = 1.442s\n",
      "Epoch: 75, Train loss: 4.437, Val loss: 3.572, Epoch time = 1.363s\n",
      "Epoch: 76, Train loss: 4.485, Val loss: 3.790, Epoch time = 1.401s\n",
      "Epoch: 77, Train loss: 4.756, Val loss: 3.400, Epoch time = 1.451s\n",
      "Epoch: 78, Train loss: 4.816, Val loss: 3.458, Epoch time = 1.551s\n",
      "Epoch: 79, Train loss: 4.567, Val loss: 3.960, Epoch time = 1.283s\n",
      "Epoch: 80, Train loss: 4.190, Val loss: 3.473, Epoch time = 1.502s\n",
      "Epoch: 81, Train loss: 4.131, Val loss: 3.411, Epoch time = 1.406s\n",
      "Epoch: 82, Train loss: 4.641, Val loss: 4.333, Epoch time = 1.631s\n",
      "Epoch: 83, Train loss: 5.033, Val loss: 3.470, Epoch time = 1.376s\n",
      "Epoch: 84, Train loss: 4.980, Val loss: 3.534, Epoch time = 1.494s\n",
      "Epoch: 85, Train loss: 4.411, Val loss: 3.760, Epoch time = 1.364s\n",
      "Epoch: 86, Train loss: 4.614, Val loss: 3.253, Epoch time = 1.505s\n",
      "Epoch: 87, Train loss: 4.008, Val loss: 3.335, Epoch time = 1.409s\n",
      "Epoch: 88, Train loss: 4.704, Val loss: 3.738, Epoch time = 1.483s\n",
      "Epoch: 89, Train loss: 3.948, Val loss: 3.798, Epoch time = 1.352s\n",
      "Epoch: 90, Train loss: 4.414, Val loss: 3.711, Epoch time = 1.460s\n",
      "Epoch: 91, Train loss: 5.200, Val loss: 3.720, Epoch time = 1.332s\n",
      "Epoch: 92, Train loss: 4.499, Val loss: 3.309, Epoch time = 1.558s\n",
      "Epoch: 93, Train loss: 4.475, Val loss: 3.612, Epoch time = 1.224s\n",
      "Epoch: 94, Train loss: 4.614, Val loss: 3.951, Epoch time = 1.427s\n",
      "Epoch: 95, Train loss: 4.759, Val loss: 4.017, Epoch time = 1.468s\n",
      "Epoch: 96, Train loss: 5.572, Val loss: 3.271, Epoch time = 1.313s\n",
      "Epoch: 97, Train loss: 4.120, Val loss: 3.630, Epoch time = 1.463s\n",
      "Epoch: 98, Train loss: 4.348, Val loss: 3.325, Epoch time = 1.444s\n",
      "Epoch: 99, Train loss: 4.025, Val loss: 3.443, Epoch time = 1.499s\n",
      "Epoch: 100, Train loss: 4.187, Val loss: 3.552, Epoch time = 1.466s\n",
      "Epoch: 101, Train loss: 3.929, Val loss: 3.738, Epoch time = 1.577s\n",
      "Epoch: 102, Train loss: 4.899, Val loss: 3.675, Epoch time = 1.405s\n",
      "Epoch: 103, Train loss: 4.214, Val loss: 3.438, Epoch time = 1.480s\n",
      "Epoch: 104, Train loss: 4.684, Val loss: 3.394, Epoch time = 1.795s\n",
      "Epoch: 105, Train loss: 3.918, Val loss: 3.184, Epoch time = 1.457s\n",
      "Epoch: 106, Train loss: 4.388, Val loss: 3.390, Epoch time = 1.493s\n",
      "Epoch: 107, Train loss: 3.938, Val loss: 3.852, Epoch time = 1.515s\n",
      "Epoch: 108, Train loss: 3.648, Val loss: 3.524, Epoch time = 1.401s\n",
      "Epoch: 109, Train loss: 4.708, Val loss: 3.569, Epoch time = 1.426s\n",
      "Epoch: 110, Train loss: 3.833, Val loss: 5.799, Epoch time = 1.529s\n",
      "Epoch: 111, Train loss: 3.948, Val loss: 3.397, Epoch time = 1.447s\n",
      "Epoch: 112, Train loss: 4.607, Val loss: 3.238, Epoch time = 1.516s\n",
      "Epoch: 113, Train loss: 3.923, Val loss: 3.892, Epoch time = 1.515s\n",
      "Epoch: 114, Train loss: 4.169, Val loss: 3.281, Epoch time = 1.452s\n",
      "Epoch: 115, Train loss: 3.772, Val loss: 3.256, Epoch time = 1.366s\n",
      "Epoch: 116, Train loss: 3.778, Val loss: 3.609, Epoch time = 1.386s\n",
      "Epoch: 117, Train loss: 3.951, Val loss: 3.599, Epoch time = 1.574s\n",
      "Epoch: 118, Train loss: 4.020, Val loss: 3.723, Epoch time = 1.469s\n",
      "Epoch: 119, Train loss: 4.262, Val loss: 4.593, Epoch time = 1.453s\n",
      "Epoch: 120, Train loss: 4.081, Val loss: 3.797, Epoch time = 1.260s\n",
      "Epoch: 121, Train loss: 4.370, Val loss: 3.861, Epoch time = 1.408s\n",
      "Epoch: 122, Train loss: 5.157, Val loss: 3.306, Epoch time = 1.493s\n",
      "Epoch: 123, Train loss: 4.436, Val loss: 3.513, Epoch time = 1.579s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train loss: 4.675, Val loss: 3.408, Epoch time = 1.736s\n",
      "Epoch: 125, Train loss: 4.298, Val loss: 3.337, Epoch time = 1.639s\n",
      "Epoch: 126, Train loss: 4.187, Val loss: 3.274, Epoch time = 1.496s\n",
      "Epoch: 127, Train loss: 4.353, Val loss: 3.141, Epoch time = 1.790s\n",
      "Epoch: 128, Train loss: 4.874, Val loss: 3.551, Epoch time = 1.656s\n",
      "Epoch: 129, Train loss: 4.282, Val loss: 3.427, Epoch time = 1.656s\n",
      "Epoch: 130, Train loss: 4.341, Val loss: 3.157, Epoch time = 1.556s\n",
      "Epoch: 131, Train loss: 4.581, Val loss: 3.602, Epoch time = 1.465s\n",
      "Epoch: 132, Train loss: 4.285, Val loss: 3.503, Epoch time = 1.641s\n",
      "Epoch: 133, Train loss: 4.238, Val loss: 3.861, Epoch time = 1.728s\n",
      "Epoch: 134, Train loss: 4.261, Val loss: 4.053, Epoch time = 1.527s\n",
      "Epoch: 135, Train loss: 3.784, Val loss: 3.243, Epoch time = 1.485s\n",
      "Epoch: 136, Train loss: 3.780, Val loss: 3.500, Epoch time = 1.528s\n",
      "Epoch: 137, Train loss: 3.738, Val loss: 3.791, Epoch time = 1.460s\n",
      "Epoch: 138, Train loss: 4.185, Val loss: 3.692, Epoch time = 1.771s\n",
      "Epoch: 139, Train loss: 3.896, Val loss: 3.369, Epoch time = 1.520s\n",
      "Epoch: 140, Train loss: 4.456, Val loss: 3.047, Epoch time = 1.559s\n",
      "Epoch: 141, Train loss: 4.739, Val loss: 3.692, Epoch time = 1.307s\n",
      "Epoch: 142, Train loss: 4.538, Val loss: 3.362, Epoch time = 1.870s\n",
      "Epoch: 143, Train loss: 4.404, Val loss: 3.610, Epoch time = 1.753s\n",
      "Epoch: 144, Train loss: 3.815, Val loss: 3.716, Epoch time = 1.778s\n",
      "Epoch: 145, Train loss: 4.036, Val loss: 3.609, Epoch time = 1.489s\n",
      "Epoch: 146, Train loss: 4.625, Val loss: 3.225, Epoch time = 1.310s\n",
      "Epoch: 147, Train loss: 5.244, Val loss: 3.545, Epoch time = 1.610s\n",
      "Epoch: 148, Train loss: 4.275, Val loss: 3.580, Epoch time = 1.553s\n",
      "Epoch: 149, Train loss: 4.773, Val loss: 3.495, Epoch time = 1.385s\n",
      "Epoch: 150, Train loss: 4.102, Val loss: 3.384, Epoch time = 1.652s\n",
      "Epoch: 151, Train loss: 3.901, Val loss: 3.271, Epoch time = 1.490s\n",
      "Epoch: 152, Train loss: 3.557, Val loss: 3.338, Epoch time = 1.645s\n",
      "Epoch: 153, Train loss: 4.359, Val loss: 3.653, Epoch time = 2.035s\n",
      "Epoch: 154, Train loss: 4.683, Val loss: 3.957, Epoch time = 1.471s\n",
      "Epoch: 155, Train loss: 4.867, Val loss: 3.430, Epoch time = 1.349s\n",
      "Epoch: 156, Train loss: 4.106, Val loss: 3.470, Epoch time = 1.518s\n",
      "Epoch: 157, Train loss: 4.210, Val loss: 3.479, Epoch time = 1.598s\n",
      "Epoch: 158, Train loss: 4.171, Val loss: 5.479, Epoch time = 1.697s\n",
      "Epoch: 159, Train loss: 3.515, Val loss: 3.578, Epoch time = 1.423s\n",
      "Epoch: 160, Train loss: 3.735, Val loss: 3.679, Epoch time = 1.314s\n",
      "Epoch: 161, Train loss: 4.279, Val loss: 3.536, Epoch time = 1.559s\n",
      "Epoch: 162, Train loss: 4.284, Val loss: 3.452, Epoch time = 1.617s\n",
      "Epoch: 163, Train loss: 3.864, Val loss: 3.354, Epoch time = 1.569s\n",
      "Epoch: 164, Train loss: 4.153, Val loss: 3.312, Epoch time = 1.670s\n",
      "Epoch: 165, Train loss: 4.272, Val loss: 3.598, Epoch time = 1.647s\n",
      "Epoch: 166, Train loss: 4.262, Val loss: 3.854, Epoch time = 1.543s\n",
      "Epoch: 167, Train loss: 3.504, Val loss: 3.325, Epoch time = 1.423s\n",
      "Epoch: 168, Train loss: 4.892, Val loss: 3.334, Epoch time = 1.564s\n",
      "Epoch: 169, Train loss: 4.360, Val loss: 3.499, Epoch time = 1.562s\n",
      "Epoch: 170, Train loss: 4.206, Val loss: 3.703, Epoch time = 1.439s\n",
      "Epoch: 171, Train loss: 4.123, Val loss: 3.423, Epoch time = 1.637s\n",
      "Epoch: 172, Train loss: 3.865, Val loss: 3.756, Epoch time = 1.716s\n",
      "Epoch: 173, Train loss: 3.938, Val loss: 3.788, Epoch time = 1.669s\n",
      "Epoch: 174, Train loss: 3.710, Val loss: 3.472, Epoch time = 1.491s\n",
      "Epoch: 175, Train loss: 3.915, Val loss: 3.666, Epoch time = 1.538s\n",
      "Epoch: 176, Train loss: 3.906, Val loss: 3.422, Epoch time = 1.423s\n",
      "Epoch: 177, Train loss: 4.186, Val loss: 3.561, Epoch time = 1.597s\n",
      "Epoch: 178, Train loss: 3.836, Val loss: 3.202, Epoch time = 1.518s\n",
      "Epoch: 179, Train loss: 4.528, Val loss: 3.428, Epoch time = 1.531s\n",
      "Epoch: 180, Train loss: 3.924, Val loss: 3.037, Epoch time = 1.470s\n",
      "Epoch: 181, Train loss: 3.674, Val loss: 3.323, Epoch time = 1.500s\n",
      "Epoch: 182, Train loss: 4.506, Val loss: 3.278, Epoch time = 1.485s\n",
      "Epoch: 183, Train loss: 3.635, Val loss: 3.367, Epoch time = 1.488s\n",
      "Epoch: 184, Train loss: 4.080, Val loss: 3.463, Epoch time = 1.307s\n",
      "Epoch: 185, Train loss: 3.781, Val loss: 3.505, Epoch time = 1.474s\n",
      "Epoch: 186, Train loss: 3.548, Val loss: 3.203, Epoch time = 1.613s\n",
      "Epoch: 187, Train loss: 3.763, Val loss: 3.727, Epoch time = 1.505s\n",
      "Epoch: 188, Train loss: 3.942, Val loss: 3.372, Epoch time = 1.627s\n",
      "Epoch: 189, Train loss: 4.069, Val loss: 3.484, Epoch time = 1.347s\n",
      "Epoch: 190, Train loss: 4.288, Val loss: 3.617, Epoch time = 1.569s\n",
      "Epoch: 191, Train loss: 4.043, Val loss: 3.222, Epoch time = 1.654s\n",
      "Epoch: 192, Train loss: 4.096, Val loss: 3.326, Epoch time = 1.518s\n",
      "Epoch: 193, Train loss: 4.453, Val loss: 3.588, Epoch time = 1.315s\n",
      "Epoch: 194, Train loss: 4.359, Val loss: 3.356, Epoch time = 1.238s\n",
      "Epoch: 195, Train loss: 4.240, Val loss: 3.435, Epoch time = 1.541s\n",
      "Epoch: 196, Train loss: 4.805, Val loss: 3.235, Epoch time = 1.687s\n",
      "Epoch: 197, Train loss: 4.446, Val loss: 3.227, Epoch time = 1.532s\n",
      "Epoch: 198, Train loss: 4.202, Val loss: 3.163, Epoch time = 1.587s\n",
      "Epoch: 199, Train loss: 4.005, Val loss: 2.874, Epoch time = 1.357s\n",
      "Epoch: 200, Train loss: 3.809, Val loss: 3.245, Epoch time = 1.563s\n",
      "Epoch: 201, Train loss: 4.256, Val loss: 3.561, Epoch time = 1.494s\n",
      "Epoch: 202, Train loss: 4.272, Val loss: 3.361, Epoch time = 1.708s\n",
      "Epoch: 203, Train loss: 3.708, Val loss: 3.244, Epoch time = 1.475s\n",
      "Epoch: 204, Train loss: 5.640, Val loss: 3.301, Epoch time = 1.517s\n",
      "Epoch: 205, Train loss: 4.274, Val loss: 3.098, Epoch time = 1.484s\n",
      "Epoch: 206, Train loss: 3.809, Val loss: 3.395, Epoch time = 1.475s\n",
      "Epoch: 207, Train loss: 3.894, Val loss: 3.781, Epoch time = 1.555s\n",
      "Epoch: 208, Train loss: 3.364, Val loss: 2.884, Epoch time = 1.404s\n",
      "Epoch: 209, Train loss: 4.527, Val loss: 3.154, Epoch time = 1.327s\n",
      "Epoch: 210, Train loss: 3.872, Val loss: 3.378, Epoch time = 1.510s\n",
      "Epoch: 211, Train loss: 3.448, Val loss: 3.474, Epoch time = 1.426s\n",
      "Epoch: 212, Train loss: 4.568, Val loss: 3.263, Epoch time = 1.581s\n",
      "Epoch: 213, Train loss: 3.490, Val loss: 3.216, Epoch time = 1.424s\n",
      "Epoch: 214, Train loss: 4.843, Val loss: 3.465, Epoch time = 1.579s\n",
      "Epoch: 215, Train loss: 3.685, Val loss: 3.762, Epoch time = 1.404s\n",
      "Epoch: 216, Train loss: 3.988, Val loss: 3.430, Epoch time = 1.563s\n",
      "Epoch: 217, Train loss: 3.683, Val loss: 3.358, Epoch time = 1.573s\n",
      "Epoch: 218, Train loss: 3.940, Val loss: 3.297, Epoch time = 1.509s\n",
      "Epoch: 219, Train loss: 3.693, Val loss: 3.225, Epoch time = 1.432s\n",
      "Epoch: 220, Train loss: 3.694, Val loss: 3.278, Epoch time = 1.641s\n",
      "Epoch: 221, Train loss: 3.950, Val loss: 3.603, Epoch time = 1.664s\n",
      "Epoch: 222, Train loss: 3.696, Val loss: 3.045, Epoch time = 1.446s\n",
      "Epoch: 223, Train loss: 4.312, Val loss: 3.238, Epoch time = 1.551s\n",
      "Epoch: 224, Train loss: 4.431, Val loss: 3.851, Epoch time = 1.381s\n",
      "Epoch: 225, Train loss: 4.372, Val loss: 3.344, Epoch time = 1.366s\n",
      "Epoch: 226, Train loss: 3.824, Val loss: 3.832, Epoch time = 1.500s\n",
      "Epoch: 227, Train loss: 3.933, Val loss: 3.369, Epoch time = 1.392s\n",
      "Epoch: 228, Train loss: 3.939, Val loss: 5.173, Epoch time = 1.649s\n",
      "Epoch: 229, Train loss: 4.262, Val loss: 4.098, Epoch time = 1.453s\n",
      "Epoch: 230, Train loss: 4.444, Val loss: 3.163, Epoch time = 1.386s\n",
      "Epoch: 231, Train loss: 3.925, Val loss: 3.191, Epoch time = 1.706s\n",
      "Epoch: 232, Train loss: 3.888, Val loss: 3.690, Epoch time = 1.519s\n",
      "Epoch: 233, Train loss: 4.500, Val loss: 3.424, Epoch time = 1.512s\n",
      "Epoch: 234, Train loss: 3.776, Val loss: 3.255, Epoch time = 1.325s\n",
      "Epoch: 235, Train loss: 4.191, Val loss: 3.102, Epoch time = 1.382s\n",
      "Epoch: 236, Train loss: 4.630, Val loss: 3.583, Epoch time = 1.515s\n",
      "Epoch: 237, Train loss: 4.089, Val loss: 3.771, Epoch time = 1.321s\n",
      "Epoch: 238, Train loss: 3.783, Val loss: 3.196, Epoch time = 1.356s\n",
      "Epoch: 239, Train loss: 3.939, Val loss: 3.152, Epoch time = 1.342s\n",
      "Epoch: 240, Train loss: 4.160, Val loss: 3.791, Epoch time = 1.613s\n",
      "Epoch: 241, Train loss: 3.870, Val loss: 3.517, Epoch time = 1.437s\n",
      "Epoch: 242, Train loss: 3.725, Val loss: 3.339, Epoch time = 1.384s\n",
      "Epoch: 243, Train loss: 3.626, Val loss: 3.138, Epoch time = 1.502s\n",
      "Epoch: 244, Train loss: 4.002, Val loss: 3.250, Epoch time = 1.490s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245, Train loss: 3.737, Val loss: 3.530, Epoch time = 1.437s\n",
      "Epoch: 246, Train loss: 3.307, Val loss: 3.704, Epoch time = 1.538s\n",
      "Epoch: 247, Train loss: 4.653, Val loss: 3.163, Epoch time = 1.550s\n",
      "Epoch: 248, Train loss: 3.852, Val loss: 3.031, Epoch time = 1.541s\n",
      "Epoch: 249, Train loss: 4.462, Val loss: 3.461, Epoch time = 1.424s\n",
      "Epoch: 250, Train loss: 4.026, Val loss: 3.456, Epoch time = 1.316s\n",
      "Epoch: 251, Train loss: 3.459, Val loss: 3.513, Epoch time = 1.411s\n",
      "Epoch: 252, Train loss: 4.085, Val loss: 2.999, Epoch time = 1.432s\n",
      "Epoch: 253, Train loss: 3.779, Val loss: 3.337, Epoch time = 1.363s\n",
      "Epoch: 254, Train loss: 4.141, Val loss: 3.158, Epoch time = 1.422s\n",
      "Epoch: 255, Train loss: 3.662, Val loss: 2.909, Epoch time = 1.626s\n",
      "Epoch: 256, Train loss: 3.763, Val loss: 3.129, Epoch time = 1.406s\n",
      "Epoch: 257, Train loss: 4.510, Val loss: 3.702, Epoch time = 1.543s\n",
      "Epoch: 258, Train loss: 4.440, Val loss: 3.698, Epoch time = 1.464s\n",
      "Epoch: 259, Train loss: 4.276, Val loss: 3.613, Epoch time = 1.498s\n",
      "Epoch: 260, Train loss: 3.896, Val loss: 3.244, Epoch time = 1.479s\n",
      "Epoch: 261, Train loss: 3.514, Val loss: 3.359, Epoch time = 1.410s\n",
      "Epoch: 262, Train loss: 3.687, Val loss: 3.259, Epoch time = 1.504s\n",
      "Epoch: 263, Train loss: 4.225, Val loss: 3.840, Epoch time = 1.524s\n",
      "Epoch: 264, Train loss: 4.035, Val loss: 3.219, Epoch time = 1.560s\n",
      "Epoch: 265, Train loss: 3.603, Val loss: 3.471, Epoch time = 1.549s\n",
      "Epoch: 266, Train loss: 4.431, Val loss: 3.129, Epoch time = 1.625s\n",
      "Epoch: 267, Train loss: 3.541, Val loss: 3.533, Epoch time = 1.427s\n",
      "Epoch: 268, Train loss: 3.325, Val loss: 3.169, Epoch time = 1.539s\n",
      "Epoch: 269, Train loss: 3.493, Val loss: 3.502, Epoch time = 1.497s\n",
      "Epoch: 270, Train loss: 4.026, Val loss: 3.215, Epoch time = 1.646s\n",
      "Epoch: 271, Train loss: 3.602, Val loss: 3.876, Epoch time = 1.594s\n",
      "Epoch: 272, Train loss: 4.141, Val loss: 3.280, Epoch time = 1.602s\n",
      "Epoch: 273, Train loss: 4.507, Val loss: 3.393, Epoch time = 1.465s\n",
      "Epoch: 274, Train loss: 3.785, Val loss: 3.706, Epoch time = 1.518s\n",
      "Epoch: 275, Train loss: 3.797, Val loss: 3.611, Epoch time = 1.323s\n",
      "Epoch: 276, Train loss: 3.418, Val loss: 3.536, Epoch time = 1.532s\n",
      "Epoch: 277, Train loss: 3.047, Val loss: 4.544, Epoch time = 1.327s\n",
      "Epoch: 278, Train loss: 3.765, Val loss: 5.010, Epoch time = 1.464s\n",
      "Epoch: 279, Train loss: 4.337, Val loss: 3.766, Epoch time = 1.452s\n",
      "Epoch: 280, Train loss: 3.724, Val loss: 3.381, Epoch time = 1.551s\n",
      "Epoch: 281, Train loss: 3.824, Val loss: 3.025, Epoch time = 1.415s\n",
      "Epoch: 282, Train loss: 3.910, Val loss: 2.942, Epoch time = 1.457s\n",
      "Epoch: 283, Train loss: 4.602, Val loss: 3.485, Epoch time = 1.610s\n",
      "Epoch: 284, Train loss: 4.030, Val loss: 3.069, Epoch time = 1.433s\n",
      "Epoch: 285, Train loss: 3.846, Val loss: 3.567, Epoch time = 1.488s\n",
      "Epoch: 286, Train loss: 4.332, Val loss: 3.404, Epoch time = 1.543s\n",
      "Epoch: 287, Train loss: 3.934, Val loss: 3.363, Epoch time = 1.538s\n",
      "Epoch: 288, Train loss: 4.422, Val loss: 3.732, Epoch time = 1.467s\n",
      "Epoch: 289, Train loss: 4.260, Val loss: 3.263, Epoch time = 1.480s\n",
      "Epoch: 290, Train loss: 3.744, Val loss: 2.908, Epoch time = 1.439s\n",
      "Epoch: 291, Train loss: 4.028, Val loss: 3.340, Epoch time = 1.706s\n",
      "Epoch: 292, Train loss: 3.959, Val loss: 3.037, Epoch time = 1.531s\n",
      "Epoch: 293, Train loss: 4.140, Val loss: 3.489, Epoch time = 1.280s\n",
      "Epoch: 294, Train loss: 3.533, Val loss: 3.257, Epoch time = 1.661s\n",
      "Epoch: 295, Train loss: 3.890, Val loss: 3.417, Epoch time = 1.407s\n",
      "Epoch: 296, Train loss: 3.386, Val loss: 3.598, Epoch time = 1.561s\n",
      "Epoch: 297, Train loss: 3.735, Val loss: 4.938, Epoch time = 1.521s\n",
      "Epoch: 298, Train loss: 4.342, Val loss: 4.884, Epoch time = 1.700s\n",
      "Epoch: 299, Train loss: 3.537, Val loss: 3.292, Epoch time = 1.305s\n",
      "Epoch: 300, Train loss: 3.462, Val loss: 3.486, Epoch time = 1.517s\n",
      "Epoch: 301, Train loss: 4.492, Val loss: 3.622, Epoch time = 1.553s\n",
      "Epoch: 302, Train loss: 3.453, Val loss: 3.291, Epoch time = 1.506s\n",
      "Epoch: 303, Train loss: 3.500, Val loss: 3.253, Epoch time = 1.515s\n",
      "Epoch: 304, Train loss: 3.750, Val loss: 3.554, Epoch time = 1.554s\n",
      "Epoch: 305, Train loss: 3.623, Val loss: 3.659, Epoch time = 1.541s\n",
      "Epoch: 306, Train loss: 3.897, Val loss: 3.241, Epoch time = 1.622s\n",
      "Epoch: 307, Train loss: 3.538, Val loss: 3.570, Epoch time = 1.386s\n",
      "Epoch: 308, Train loss: 4.005, Val loss: 3.428, Epoch time = 1.477s\n",
      "Epoch: 309, Train loss: 3.917, Val loss: 3.606, Epoch time = 1.510s\n",
      "Epoch: 310, Train loss: 4.074, Val loss: 3.331, Epoch time = 1.383s\n",
      "Epoch: 311, Train loss: 4.478, Val loss: 3.466, Epoch time = 1.494s\n",
      "Epoch: 312, Train loss: 3.782, Val loss: 3.451, Epoch time = 1.483s\n",
      "Epoch: 313, Train loss: 3.628, Val loss: 3.372, Epoch time = 1.445s\n",
      "Epoch: 314, Train loss: 3.877, Val loss: 3.138, Epoch time = 1.478s\n",
      "Epoch: 315, Train loss: 3.916, Val loss: 3.405, Epoch time = 1.351s\n",
      "Epoch: 316, Train loss: 4.312, Val loss: 3.706, Epoch time = 1.354s\n",
      "Epoch: 317, Train loss: 4.359, Val loss: 3.552, Epoch time = 1.546s\n",
      "Epoch: 318, Train loss: 3.566, Val loss: 3.096, Epoch time = 1.516s\n",
      "Epoch: 319, Train loss: 3.552, Val loss: 3.390, Epoch time = 1.487s\n",
      "Epoch: 320, Train loss: 3.768, Val loss: 3.438, Epoch time = 1.522s\n",
      "Epoch: 321, Train loss: 4.457, Val loss: 3.776, Epoch time = 1.498s\n",
      "Epoch: 322, Train loss: 4.201, Val loss: 3.606, Epoch time = 1.433s\n",
      "Epoch: 323, Train loss: 4.120, Val loss: 5.344, Epoch time = 1.479s\n",
      "Epoch: 324, Train loss: 3.376, Val loss: 3.061, Epoch time = 1.397s\n",
      "Epoch: 325, Train loss: 3.908, Val loss: 3.686, Epoch time = 1.537s\n",
      "Epoch: 326, Train loss: 4.145, Val loss: 3.292, Epoch time = 1.588s\n",
      "Epoch: 327, Train loss: 3.155, Val loss: 3.338, Epoch time = 1.503s\n",
      "Epoch: 328, Train loss: 3.181, Val loss: 3.222, Epoch time = 1.481s\n",
      "Epoch: 329, Train loss: 3.875, Val loss: 3.161, Epoch time = 1.507s\n",
      "Epoch: 330, Train loss: 3.811, Val loss: 3.346, Epoch time = 1.325s\n",
      "Epoch: 331, Train loss: 3.450, Val loss: 3.229, Epoch time = 1.527s\n",
      "Epoch: 332, Train loss: 3.791, Val loss: 3.050, Epoch time = 1.497s\n",
      "Epoch: 333, Train loss: 3.947, Val loss: 3.429, Epoch time = 1.538s\n",
      "Epoch: 334, Train loss: 3.931, Val loss: 3.785, Epoch time = 1.614s\n",
      "Epoch: 335, Train loss: 3.228, Val loss: 3.542, Epoch time = 1.387s\n",
      "Epoch: 336, Train loss: 3.983, Val loss: 3.358, Epoch time = 1.359s\n",
      "Epoch: 337, Train loss: 3.817, Val loss: 3.351, Epoch time = 1.514s\n",
      "Epoch: 338, Train loss: 3.685, Val loss: 3.358, Epoch time = 1.540s\n",
      "Epoch: 339, Train loss: 4.052, Val loss: 3.238, Epoch time = 1.486s\n",
      "Epoch: 340, Train loss: 3.671, Val loss: 3.114, Epoch time = 1.596s\n",
      "Epoch: 341, Train loss: 3.506, Val loss: 3.203, Epoch time = 1.463s\n",
      "Epoch: 342, Train loss: 3.353, Val loss: 3.127, Epoch time = 1.515s\n",
      "Epoch: 343, Train loss: 3.429, Val loss: 3.398, Epoch time = 1.403s\n",
      "Epoch: 344, Train loss: 3.871, Val loss: 3.811, Epoch time = 1.451s\n",
      "Epoch: 345, Train loss: 4.060, Val loss: 2.996, Epoch time = 1.415s\n",
      "Epoch: 346, Train loss: 3.513, Val loss: 3.217, Epoch time = 1.498s\n",
      "Epoch: 347, Train loss: 3.572, Val loss: 3.453, Epoch time = 1.686s\n",
      "Epoch: 348, Train loss: 3.242, Val loss: 3.001, Epoch time = 1.314s\n",
      "Epoch: 349, Train loss: 3.619, Val loss: 3.275, Epoch time = 1.562s\n",
      "Epoch: 350, Train loss: 3.615, Val loss: 3.561, Epoch time = 1.687s\n",
      "Epoch: 351, Train loss: 3.601, Val loss: 3.727, Epoch time = 1.458s\n",
      "Epoch: 352, Train loss: 3.814, Val loss: 3.361, Epoch time = 1.467s\n",
      "Epoch: 353, Train loss: 4.687, Val loss: 3.119, Epoch time = 1.276s\n",
      "Epoch: 354, Train loss: 3.469, Val loss: 3.614, Epoch time = 1.344s\n",
      "Epoch: 355, Train loss: 3.884, Val loss: 3.436, Epoch time = 1.634s\n",
      "Epoch: 356, Train loss: 4.010, Val loss: 3.517, Epoch time = 1.633s\n",
      "Epoch: 357, Train loss: 3.210, Val loss: 3.219, Epoch time = 1.455s\n",
      "Epoch: 358, Train loss: 4.118, Val loss: 3.872, Epoch time = 1.354s\n",
      "Epoch: 359, Train loss: 4.404, Val loss: 3.067, Epoch time = 1.382s\n",
      "Epoch: 360, Train loss: 3.369, Val loss: 3.466, Epoch time = 1.651s\n",
      "Epoch: 361, Train loss: 3.349, Val loss: 3.010, Epoch time = 1.413s\n",
      "Epoch: 362, Train loss: 4.190, Val loss: 3.159, Epoch time = 1.625s\n",
      "Epoch: 363, Train loss: 4.353, Val loss: 4.587, Epoch time = 1.496s\n",
      "Epoch: 364, Train loss: 3.887, Val loss: 3.372, Epoch time = 1.569s\n",
      "Epoch: 365, Train loss: 3.494, Val loss: 3.277, Epoch time = 1.489s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366, Train loss: 3.665, Val loss: 3.102, Epoch time = 1.564s\n",
      "Epoch: 367, Train loss: 3.834, Val loss: 3.347, Epoch time = 1.545s\n",
      "Epoch: 368, Train loss: 3.502, Val loss: 3.277, Epoch time = 1.318s\n",
      "Epoch: 369, Train loss: 3.475, Val loss: 3.515, Epoch time = 1.540s\n",
      "Epoch: 370, Train loss: 3.208, Val loss: 3.332, Epoch time = 1.422s\n",
      "Epoch: 371, Train loss: 3.394, Val loss: 3.586, Epoch time = 1.286s\n",
      "Epoch: 372, Train loss: 4.078, Val loss: 3.806, Epoch time = 1.491s\n",
      "Epoch: 373, Train loss: 3.563, Val loss: 3.057, Epoch time = 1.539s\n",
      "Epoch: 374, Train loss: 3.818, Val loss: 3.657, Epoch time = 1.633s\n",
      "Epoch: 375, Train loss: 3.324, Val loss: 3.451, Epoch time = 1.712s\n",
      "Epoch: 376, Train loss: 3.608, Val loss: 3.367, Epoch time = 1.427s\n",
      "Epoch: 377, Train loss: 4.016, Val loss: 3.373, Epoch time = 1.663s\n",
      "Epoch: 378, Train loss: 3.522, Val loss: 3.297, Epoch time = 1.539s\n",
      "Epoch: 379, Train loss: 3.677, Val loss: 3.573, Epoch time = 1.553s\n",
      "Epoch: 380, Train loss: 3.593, Val loss: 3.217, Epoch time = 1.493s\n",
      "Epoch: 381, Train loss: 4.052, Val loss: 3.155, Epoch time = 1.629s\n",
      "Epoch: 382, Train loss: 3.446, Val loss: 3.170, Epoch time = 1.620s\n",
      "Epoch: 383, Train loss: 3.803, Val loss: 3.359, Epoch time = 1.686s\n",
      "Epoch: 384, Train loss: 3.148, Val loss: 3.213, Epoch time = 2.066s\n",
      "Epoch: 385, Train loss: 3.418, Val loss: 3.163, Epoch time = 1.869s\n",
      "Epoch: 386, Train loss: 3.460, Val loss: 3.901, Epoch time = 1.423s\n",
      "Epoch: 387, Train loss: 3.448, Val loss: 3.470, Epoch time = 1.345s\n",
      "Epoch: 388, Train loss: 3.695, Val loss: 2.975, Epoch time = 1.542s\n",
      "Epoch: 389, Train loss: 3.633, Val loss: 3.574, Epoch time = 1.347s\n",
      "Epoch: 390, Train loss: 3.765, Val loss: 3.474, Epoch time = 1.658s\n",
      "Epoch: 391, Train loss: 3.987, Val loss: 3.276, Epoch time = 1.433s\n",
      "Epoch: 392, Train loss: 3.588, Val loss: 3.274, Epoch time = 1.582s\n",
      "Epoch: 393, Train loss: 4.080, Val loss: 3.520, Epoch time = 1.515s\n",
      "Epoch: 394, Train loss: 3.572, Val loss: 3.150, Epoch time = 1.585s\n",
      "Epoch: 395, Train loss: 3.609, Val loss: 3.448, Epoch time = 1.511s\n",
      "Epoch: 396, Train loss: 3.606, Val loss: 3.143, Epoch time = 1.610s\n",
      "Epoch: 397, Train loss: 3.614, Val loss: 3.397, Epoch time = 1.681s\n",
      "Epoch: 398, Train loss: 3.152, Val loss: 3.336, Epoch time = 1.737s\n",
      "Epoch: 399, Train loss: 3.257, Val loss: 3.398, Epoch time = 1.643s\n",
      "Epoch: 400, Train loss: 3.544, Val loss: 3.352, Epoch time = 1.549s\n",
      "Epoch: 401, Train loss: 3.671, Val loss: 3.038, Epoch time = 1.438s\n",
      "Epoch: 402, Train loss: 3.471, Val loss: 3.158, Epoch time = 1.585s\n",
      "Epoch: 403, Train loss: 3.679, Val loss: 3.745, Epoch time = 1.603s\n",
      "Epoch: 404, Train loss: 3.219, Val loss: 3.159, Epoch time = 1.576s\n",
      "Epoch: 405, Train loss: 3.549, Val loss: 3.264, Epoch time = 1.460s\n",
      "Epoch: 406, Train loss: 3.606, Val loss: 3.188, Epoch time = 1.798s\n",
      "Epoch: 407, Train loss: 3.576, Val loss: 3.369, Epoch time = 1.586s\n",
      "Epoch: 408, Train loss: 3.787, Val loss: 3.556, Epoch time = 1.504s\n",
      "Epoch: 409, Train loss: 4.193, Val loss: 3.658, Epoch time = 1.555s\n",
      "Epoch: 410, Train loss: 4.340, Val loss: 5.026, Epoch time = 1.413s\n",
      "Epoch: 411, Train loss: 3.545, Val loss: 2.856, Epoch time = 1.589s\n",
      "Epoch: 412, Train loss: 3.348, Val loss: 3.417, Epoch time = 1.669s\n",
      "Epoch: 413, Train loss: 3.607, Val loss: 3.375, Epoch time = 1.703s\n",
      "Epoch: 414, Train loss: 3.432, Val loss: 2.852, Epoch time = 1.633s\n",
      "Epoch: 415, Train loss: 3.458, Val loss: 4.011, Epoch time = 1.479s\n",
      "Epoch: 416, Train loss: 3.952, Val loss: 3.353, Epoch time = 1.676s\n",
      "Epoch: 417, Train loss: 3.725, Val loss: 3.415, Epoch time = 1.888s\n",
      "Epoch: 418, Train loss: 3.344, Val loss: 3.352, Epoch time = 1.404s\n",
      "Epoch: 419, Train loss: 3.503, Val loss: 3.101, Epoch time = 1.650s\n",
      "Epoch: 420, Train loss: 4.147, Val loss: 3.472, Epoch time = 1.749s\n",
      "Epoch: 421, Train loss: 3.492, Val loss: 3.144, Epoch time = 1.518s\n",
      "Epoch: 422, Train loss: 3.311, Val loss: 3.345, Epoch time = 1.544s\n",
      "Epoch: 423, Train loss: 3.518, Val loss: 2.793, Epoch time = 1.752s\n",
      "Epoch: 424, Train loss: 3.740, Val loss: 3.470, Epoch time = 1.745s\n",
      "Epoch: 425, Train loss: 3.741, Val loss: 3.189, Epoch time = 1.473s\n",
      "Epoch: 426, Train loss: 3.664, Val loss: 3.305, Epoch time = 1.404s\n",
      "Epoch: 427, Train loss: 3.784, Val loss: 4.715, Epoch time = 1.607s\n",
      "Epoch: 428, Train loss: 4.211, Val loss: 3.334, Epoch time = 1.468s\n",
      "Epoch: 429, Train loss: 3.797, Val loss: 3.473, Epoch time = 1.561s\n",
      "Epoch: 430, Train loss: 3.778, Val loss: 3.128, Epoch time = 1.509s\n",
      "Epoch: 431, Train loss: 3.290, Val loss: 3.361, Epoch time = 1.429s\n",
      "Epoch: 432, Train loss: 4.451, Val loss: 3.836, Epoch time = 1.526s\n",
      "Epoch: 433, Train loss: 3.409, Val loss: 3.324, Epoch time = 1.385s\n",
      "Epoch: 434, Train loss: 3.999, Val loss: 2.963, Epoch time = 1.667s\n",
      "Epoch: 435, Train loss: 3.812, Val loss: 3.246, Epoch time = 1.527s\n",
      "Epoch: 436, Train loss: 3.283, Val loss: 3.445, Epoch time = 1.447s\n",
      "Epoch: 437, Train loss: 3.851, Val loss: 2.777, Epoch time = 1.442s\n",
      "Epoch: 438, Train loss: 3.758, Val loss: 3.555, Epoch time = 1.477s\n",
      "Epoch: 439, Train loss: 3.635, Val loss: 3.700, Epoch time = 1.374s\n",
      "Epoch: 440, Train loss: 3.900, Val loss: 3.297, Epoch time = 1.257s\n",
      "Epoch: 441, Train loss: 3.669, Val loss: 3.547, Epoch time = 1.471s\n",
      "Epoch: 442, Train loss: 4.076, Val loss: 3.296, Epoch time = 1.388s\n",
      "Epoch: 443, Train loss: 2.972, Val loss: 3.085, Epoch time = 1.504s\n",
      "Epoch: 444, Train loss: 3.659, Val loss: 5.531, Epoch time = 1.774s\n",
      "Epoch: 445, Train loss: 3.937, Val loss: 3.327, Epoch time = 1.697s\n",
      "Epoch: 446, Train loss: 3.636, Val loss: 3.154, Epoch time = 1.510s\n",
      "Epoch: 447, Train loss: 3.439, Val loss: 3.175, Epoch time = 1.714s\n",
      "Epoch: 448, Train loss: 3.369, Val loss: 3.564, Epoch time = 1.675s\n",
      "Epoch: 449, Train loss: 3.776, Val loss: 3.430, Epoch time = 1.491s\n",
      "Epoch: 450, Train loss: 4.026, Val loss: 3.433, Epoch time = 1.757s\n",
      "Epoch: 451, Train loss: 3.504, Val loss: 3.225, Epoch time = 1.726s\n",
      "Epoch: 452, Train loss: 3.505, Val loss: 3.203, Epoch time = 1.984s\n",
      "Epoch: 453, Train loss: 3.350, Val loss: 3.234, Epoch time = 2.033s\n",
      "Epoch: 454, Train loss: 3.457, Val loss: 3.163, Epoch time = 1.595s\n",
      "Epoch: 455, Train loss: 3.831, Val loss: 3.952, Epoch time = 1.558s\n",
      "Epoch: 456, Train loss: 3.247, Val loss: 3.276, Epoch time = 1.479s\n",
      "Epoch: 457, Train loss: 3.761, Val loss: 3.427, Epoch time = 1.749s\n",
      "Epoch: 458, Train loss: 3.237, Val loss: 3.345, Epoch time = 1.431s\n",
      "Epoch: 459, Train loss: 4.133, Val loss: 3.201, Epoch time = 1.557s\n",
      "Epoch: 460, Train loss: 3.248, Val loss: 3.032, Epoch time = 1.702s\n",
      "Epoch: 461, Train loss: 3.686, Val loss: 3.365, Epoch time = 1.837s\n",
      "Epoch: 462, Train loss: 3.943, Val loss: 3.623, Epoch time = 1.830s\n",
      "Epoch: 463, Train loss: 3.926, Val loss: 3.342, Epoch time = 1.758s\n",
      "Epoch: 464, Train loss: 3.509, Val loss: 3.122, Epoch time = 1.830s\n",
      "Epoch: 465, Train loss: 3.535, Val loss: 3.360, Epoch time = 1.674s\n",
      "Epoch: 466, Train loss: 3.626, Val loss: 3.082, Epoch time = 1.740s\n",
      "Epoch: 467, Train loss: 3.441, Val loss: 3.270, Epoch time = 1.713s\n",
      "Epoch: 468, Train loss: 4.193, Val loss: 3.227, Epoch time = 1.763s\n",
      "Epoch: 469, Train loss: 4.072, Val loss: 3.323, Epoch time = 1.565s\n",
      "Epoch: 470, Train loss: 3.546, Val loss: 3.509, Epoch time = 1.774s\n",
      "Epoch: 471, Train loss: 3.577, Val loss: 2.847, Epoch time = 1.612s\n",
      "Epoch: 472, Train loss: 3.428, Val loss: 3.093, Epoch time = 1.503s\n",
      "Epoch: 473, Train loss: 3.798, Val loss: 2.878, Epoch time = 1.537s\n",
      "Epoch: 474, Train loss: 3.514, Val loss: 2.920, Epoch time = 1.541s\n",
      "Epoch: 475, Train loss: 3.686, Val loss: 3.375, Epoch time = 1.702s\n",
      "Epoch: 476, Train loss: 3.241, Val loss: 3.228, Epoch time = 1.763s\n",
      "Epoch: 477, Train loss: 3.883, Val loss: 2.947, Epoch time = 1.667s\n",
      "Epoch: 478, Train loss: 3.471, Val loss: 3.162, Epoch time = 1.417s\n",
      "Epoch: 479, Train loss: 3.310, Val loss: 3.294, Epoch time = 1.491s\n",
      "Epoch: 480, Train loss: 4.114, Val loss: 3.243, Epoch time = 1.644s\n",
      "Epoch: 481, Train loss: 3.635, Val loss: 4.189, Epoch time = 1.529s\n",
      "Epoch: 482, Train loss: 4.230, Val loss: 3.231, Epoch time = 1.652s\n",
      "Epoch: 483, Train loss: 3.665, Val loss: 3.398, Epoch time = 1.764s\n",
      "Epoch: 484, Train loss: 3.415, Val loss: 3.154, Epoch time = 1.625s\n",
      "Epoch: 485, Train loss: 3.489, Val loss: 3.405, Epoch time = 1.566s\n",
      "Epoch: 486, Train loss: 3.895, Val loss: 3.157, Epoch time = 1.660s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 487, Train loss: 3.520, Val loss: 3.081, Epoch time = 1.684s\n",
      "Epoch: 488, Train loss: 3.227, Val loss: 3.205, Epoch time = 1.465s\n",
      "Epoch: 489, Train loss: 4.204, Val loss: 3.372, Epoch time = 1.524s\n",
      "Epoch: 490, Train loss: 3.388, Val loss: 4.450, Epoch time = 1.526s\n",
      "Epoch: 491, Train loss: 3.753, Val loss: 3.673, Epoch time = 1.644s\n",
      "Epoch: 492, Train loss: 3.663, Val loss: 3.139, Epoch time = 1.632s\n",
      "Epoch: 493, Train loss: 3.516, Val loss: 3.408, Epoch time = 1.449s\n",
      "Epoch: 494, Train loss: 3.781, Val loss: 2.927, Epoch time = 1.473s\n",
      "Epoch: 495, Train loss: 3.969, Val loss: 4.764, Epoch time = 1.649s\n",
      "Epoch: 496, Train loss: 3.749, Val loss: 3.405, Epoch time = 1.645s\n",
      "Epoch: 497, Train loss: 3.719, Val loss: 3.100, Epoch time = 1.620s\n",
      "Epoch: 498, Train loss: 3.617, Val loss: 3.411, Epoch time = 1.606s\n",
      "Epoch: 499, Train loss: 3.421, Val loss: 2.971, Epoch time = 1.702s\n",
      "Epoch: 500, Train loss: 3.809, Val loss: 3.384, Epoch time = 1.712s\n",
      "Epoch: 501, Train loss: 3.211, Val loss: 3.708, Epoch time = 1.366s\n",
      "Epoch: 502, Train loss: 3.863, Val loss: 2.844, Epoch time = 1.598s\n",
      "Epoch: 503, Train loss: 3.958, Val loss: 2.840, Epoch time = 1.457s\n",
      "Epoch: 504, Train loss: 3.498, Val loss: 3.304, Epoch time = 1.463s\n",
      "Epoch: 505, Train loss: 4.040, Val loss: 3.311, Epoch time = 1.654s\n",
      "Epoch: 506, Train loss: 3.395, Val loss: 3.358, Epoch time = 1.801s\n",
      "Epoch: 507, Train loss: 3.977, Val loss: 3.196, Epoch time = 1.638s\n",
      "Epoch: 508, Train loss: 3.929, Val loss: 3.251, Epoch time = 1.492s\n",
      "Epoch: 509, Train loss: 3.253, Val loss: 3.607, Epoch time = 1.670s\n",
      "Epoch: 510, Train loss: 3.212, Val loss: 3.079, Epoch time = 1.581s\n",
      "Epoch: 511, Train loss: 3.412, Val loss: 3.230, Epoch time = 1.679s\n",
      "Epoch: 512, Train loss: 3.248, Val loss: 3.445, Epoch time = 1.467s\n",
      "Epoch: 513, Train loss: 3.551, Val loss: 3.078, Epoch time = 1.721s\n",
      "Epoch: 514, Train loss: 3.730, Val loss: 3.206, Epoch time = 1.654s\n",
      "Epoch: 515, Train loss: 3.539, Val loss: 3.135, Epoch time = 1.412s\n",
      "Epoch: 516, Train loss: 3.609, Val loss: 3.160, Epoch time = 1.528s\n",
      "Epoch: 517, Train loss: 3.520, Val loss: 3.140, Epoch time = 1.537s\n",
      "Epoch: 518, Train loss: 3.765, Val loss: 3.509, Epoch time = 1.680s\n",
      "Epoch: 519, Train loss: 3.518, Val loss: 2.815, Epoch time = 1.483s\n",
      "Epoch: 520, Train loss: 3.580, Val loss: 3.280, Epoch time = 1.681s\n",
      "Epoch: 521, Train loss: 3.625, Val loss: 3.152, Epoch time = 1.707s\n",
      "Epoch: 522, Train loss: 3.266, Val loss: 3.255, Epoch time = 1.672s\n",
      "Epoch: 523, Train loss: 3.401, Val loss: 3.047, Epoch time = 1.571s\n",
      "Epoch: 524, Train loss: 3.095, Val loss: 4.079, Epoch time = 1.556s\n",
      "Epoch: 525, Train loss: 3.865, Val loss: 3.086, Epoch time = 1.478s\n",
      "Epoch: 526, Train loss: 3.299, Val loss: 3.019, Epoch time = 1.428s\n",
      "Epoch: 527, Train loss: 3.640, Val loss: 3.356, Epoch time = 1.580s\n",
      "Epoch: 528, Train loss: 3.572, Val loss: 3.469, Epoch time = 1.483s\n",
      "Epoch: 529, Train loss: 3.774, Val loss: 3.615, Epoch time = 1.788s\n",
      "Epoch: 530, Train loss: 4.027, Val loss: 3.276, Epoch time = 1.519s\n",
      "Epoch: 531, Train loss: 3.410, Val loss: 3.299, Epoch time = 1.560s\n",
      "Epoch: 532, Train loss: 3.935, Val loss: 3.459, Epoch time = 1.467s\n",
      "Epoch: 533, Train loss: 4.307, Val loss: 3.831, Epoch time = 1.394s\n",
      "Epoch: 534, Train loss: 3.440, Val loss: 3.201, Epoch time = 1.531s\n",
      "Epoch: 535, Train loss: 4.266, Val loss: 3.148, Epoch time = 1.778s\n",
      "Epoch: 536, Train loss: 3.620, Val loss: 3.516, Epoch time = 1.601s\n",
      "Epoch: 537, Train loss: 4.029, Val loss: 3.100, Epoch time = 1.687s\n",
      "Epoch: 538, Train loss: 3.796, Val loss: 2.863, Epoch time = 1.627s\n",
      "Epoch: 539, Train loss: 3.692, Val loss: 2.886, Epoch time = 1.655s\n",
      "Epoch: 540, Train loss: 3.152, Val loss: 3.840, Epoch time = 1.367s\n",
      "Epoch: 541, Train loss: 3.889, Val loss: 3.198, Epoch time = 1.645s\n",
      "Epoch: 542, Train loss: 3.598, Val loss: 3.126, Epoch time = 1.582s\n",
      "Epoch: 543, Train loss: 3.556, Val loss: 3.110, Epoch time = 1.502s\n",
      "Epoch: 544, Train loss: 3.298, Val loss: 3.208, Epoch time = 1.600s\n",
      "Epoch: 545, Train loss: 3.356, Val loss: 2.883, Epoch time = 1.569s\n",
      "Epoch: 546, Train loss: 3.311, Val loss: 3.220, Epoch time = 1.814s\n",
      "Epoch: 547, Train loss: 3.759, Val loss: 3.046, Epoch time = 1.616s\n",
      "Epoch: 548, Train loss: 3.505, Val loss: 3.502, Epoch time = 1.610s\n",
      "Epoch: 549, Train loss: 4.006, Val loss: 3.173, Epoch time = 1.730s\n",
      "Epoch: 550, Train loss: 3.592, Val loss: 2.953, Epoch time = 1.746s\n",
      "Epoch: 551, Train loss: 3.871, Val loss: 3.103, Epoch time = 1.556s\n",
      "Epoch: 552, Train loss: 3.739, Val loss: 3.118, Epoch time = 1.531s\n",
      "Epoch: 553, Train loss: 3.598, Val loss: 3.117, Epoch time = 1.607s\n",
      "Epoch: 554, Train loss: 3.629, Val loss: 3.401, Epoch time = 1.467s\n",
      "Epoch: 555, Train loss: 3.772, Val loss: 3.021, Epoch time = 1.325s\n",
      "Epoch: 556, Train loss: 3.148, Val loss: 2.841, Epoch time = 1.467s\n",
      "Epoch: 557, Train loss: 3.666, Val loss: 3.185, Epoch time = 1.768s\n",
      "Epoch: 558, Train loss: 4.269, Val loss: 3.367, Epoch time = 1.606s\n",
      "Epoch: 559, Train loss: 3.514, Val loss: 3.314, Epoch time = 1.583s\n",
      "Epoch: 560, Train loss: 3.660, Val loss: 3.196, Epoch time = 1.347s\n",
      "Epoch: 561, Train loss: 3.430, Val loss: 3.025, Epoch time = 1.731s\n",
      "Epoch: 562, Train loss: 3.412, Val loss: 2.676, Epoch time = 1.594s\n",
      "Epoch: 563, Train loss: 3.690, Val loss: 3.151, Epoch time = 1.568s\n",
      "Epoch: 564, Train loss: 3.696, Val loss: 3.052, Epoch time = 1.728s\n",
      "Epoch: 565, Train loss: 3.740, Val loss: 2.970, Epoch time = 1.622s\n",
      "Epoch: 566, Train loss: 3.781, Val loss: 3.095, Epoch time = 1.553s\n",
      "Epoch: 567, Train loss: 3.632, Val loss: 3.085, Epoch time = 1.408s\n",
      "Epoch: 568, Train loss: 3.454, Val loss: 3.146, Epoch time = 1.529s\n",
      "Epoch: 569, Train loss: 3.314, Val loss: 3.156, Epoch time = 1.590s\n",
      "Epoch: 570, Train loss: 3.305, Val loss: 3.219, Epoch time = 1.481s\n",
      "Epoch: 571, Train loss: 3.526, Val loss: 3.290, Epoch time = 1.466s\n",
      "Epoch: 572, Train loss: 3.061, Val loss: 3.003, Epoch time = 1.541s\n",
      "Epoch: 573, Train loss: 4.181, Val loss: 3.325, Epoch time = 1.467s\n",
      "Epoch: 574, Train loss: 3.512, Val loss: 3.297, Epoch time = 1.498s\n",
      "Epoch: 575, Train loss: 3.534, Val loss: 3.227, Epoch time = 1.617s\n",
      "Epoch: 576, Train loss: 3.163, Val loss: 3.178, Epoch time = 1.480s\n",
      "Epoch: 577, Train loss: 3.644, Val loss: 3.212, Epoch time = 1.596s\n",
      "Epoch: 578, Train loss: 3.985, Val loss: 2.917, Epoch time = 1.356s\n",
      "Epoch: 579, Train loss: 2.974, Val loss: 3.418, Epoch time = 1.697s\n",
      "Epoch: 580, Train loss: 3.825, Val loss: 3.844, Epoch time = 1.499s\n",
      "Epoch: 581, Train loss: 3.447, Val loss: 3.328, Epoch time = 1.400s\n",
      "Epoch: 582, Train loss: 3.343, Val loss: 3.355, Epoch time = 1.643s\n",
      "Epoch: 583, Train loss: 3.599, Val loss: 3.618, Epoch time = 1.632s\n",
      "Epoch: 584, Train loss: 3.863, Val loss: 3.600, Epoch time = 1.489s\n",
      "Epoch: 585, Train loss: 3.893, Val loss: 3.512, Epoch time = 1.770s\n",
      "Epoch: 586, Train loss: 3.717, Val loss: 3.386, Epoch time = 1.625s\n",
      "Epoch: 587, Train loss: 3.522, Val loss: 2.989, Epoch time = 1.741s\n",
      "Epoch: 588, Train loss: 4.119, Val loss: 2.929, Epoch time = 1.643s\n",
      "Epoch: 589, Train loss: 2.864, Val loss: 3.303, Epoch time = 1.613s\n",
      "Epoch: 590, Train loss: 4.203, Val loss: 3.213, Epoch time = 1.776s\n",
      "Epoch: 591, Train loss: 3.740, Val loss: 3.191, Epoch time = 1.681s\n",
      "Epoch: 592, Train loss: 4.471, Val loss: 3.010, Epoch time = 1.506s\n",
      "Epoch: 593, Train loss: 3.968, Val loss: 3.483, Epoch time = 1.696s\n",
      "Epoch: 594, Train loss: 3.298, Val loss: 3.131, Epoch time = 1.442s\n",
      "Epoch: 595, Train loss: 3.265, Val loss: 3.459, Epoch time = 1.584s\n",
      "Epoch: 596, Train loss: 3.041, Val loss: 3.305, Epoch time = 1.580s\n",
      "Epoch: 597, Train loss: 3.561, Val loss: 4.350, Epoch time = 1.572s\n",
      "Epoch: 598, Train loss: 3.235, Val loss: 3.049, Epoch time = 1.502s\n",
      "Epoch: 599, Train loss: 3.584, Val loss: 3.281, Epoch time = 1.650s\n",
      "Epoch: 600, Train loss: 3.793, Val loss: 4.751, Epoch time = 1.725s\n",
      "Epoch: 601, Train loss: 3.743, Val loss: 4.862, Epoch time = 1.637s\n",
      "Epoch: 602, Train loss: 3.352, Val loss: 3.142, Epoch time = 1.598s\n",
      "Epoch: 603, Train loss: 3.963, Val loss: 3.245, Epoch time = 1.668s\n",
      "Epoch: 604, Train loss: 4.248, Val loss: 3.636, Epoch time = 1.532s\n",
      "Epoch: 605, Train loss: 4.010, Val loss: 3.223, Epoch time = 1.844s\n",
      "Epoch: 606, Train loss: 3.727, Val loss: 3.616, Epoch time = 1.550s\n",
      "Epoch: 607, Train loss: 3.516, Val loss: 3.404, Epoch time = 1.509s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 608, Train loss: 3.505, Val loss: 2.911, Epoch time = 1.732s\n",
      "Epoch: 609, Train loss: 3.383, Val loss: 3.141, Epoch time = 1.648s\n",
      "Epoch: 610, Train loss: 3.576, Val loss: 3.538, Epoch time = 1.509s\n",
      "Epoch: 611, Train loss: 3.532, Val loss: 3.216, Epoch time = 1.718s\n",
      "Epoch: 612, Train loss: 3.357, Val loss: 3.068, Epoch time = 1.494s\n",
      "Epoch: 613, Train loss: 3.653, Val loss: 2.949, Epoch time = 1.653s\n",
      "Epoch: 614, Train loss: 3.442, Val loss: 3.129, Epoch time = 1.608s\n",
      "Epoch: 615, Train loss: 3.452, Val loss: 3.320, Epoch time = 1.566s\n",
      "Epoch: 616, Train loss: 3.451, Val loss: 2.883, Epoch time = 1.651s\n",
      "Epoch: 617, Train loss: 3.339, Val loss: 2.928, Epoch time = 1.650s\n",
      "Epoch: 618, Train loss: 2.728, Val loss: 3.250, Epoch time = 1.309s\n",
      "Epoch: 619, Train loss: 3.556, Val loss: 3.093, Epoch time = 1.695s\n",
      "Epoch: 620, Train loss: 3.181, Val loss: 3.460, Epoch time = 1.523s\n",
      "Epoch: 621, Train loss: 3.073, Val loss: 3.263, Epoch time = 1.539s\n",
      "Epoch: 622, Train loss: 3.585, Val loss: 2.828, Epoch time = 1.715s\n",
      "Epoch: 623, Train loss: 3.560, Val loss: 3.221, Epoch time = 1.583s\n",
      "Epoch: 624, Train loss: 2.984, Val loss: 3.126, Epoch time = 1.737s\n",
      "Epoch: 625, Train loss: 3.697, Val loss: 2.732, Epoch time = 1.476s\n",
      "Epoch: 626, Train loss: 3.251, Val loss: 3.531, Epoch time = 1.566s\n",
      "Epoch: 627, Train loss: 3.501, Val loss: 3.122, Epoch time = 1.615s\n",
      "Epoch: 628, Train loss: 3.845, Val loss: 2.807, Epoch time = 1.553s\n",
      "Epoch: 629, Train loss: 3.655, Val loss: 3.343, Epoch time = 1.528s\n",
      "Epoch: 630, Train loss: 3.442, Val loss: 2.872, Epoch time = 1.620s\n",
      "Epoch: 631, Train loss: 3.512, Val loss: 3.547, Epoch time = 1.532s\n",
      "Epoch: 632, Train loss: 3.331, Val loss: 3.214, Epoch time = 1.642s\n",
      "Epoch: 633, Train loss: 3.640, Val loss: 3.200, Epoch time = 1.693s\n",
      "Epoch: 634, Train loss: 3.183, Val loss: 3.210, Epoch time = 1.649s\n",
      "Epoch: 635, Train loss: 3.230, Val loss: 3.210, Epoch time = 1.541s\n",
      "Epoch: 636, Train loss: 3.373, Val loss: 3.084, Epoch time = 1.500s\n",
      "Epoch: 637, Train loss: 3.544, Val loss: 3.020, Epoch time = 1.674s\n",
      "Epoch: 638, Train loss: 3.926, Val loss: 3.247, Epoch time = 1.681s\n",
      "Epoch: 639, Train loss: 3.730, Val loss: 3.264, Epoch time = 1.438s\n",
      "Epoch: 640, Train loss: 3.394, Val loss: 3.123, Epoch time = 1.700s\n",
      "Epoch: 641, Train loss: 3.440, Val loss: 3.017, Epoch time = 1.624s\n",
      "Epoch: 642, Train loss: 3.409, Val loss: 3.010, Epoch time = 1.765s\n",
      "Epoch: 643, Train loss: 3.365, Val loss: 2.765, Epoch time = 1.601s\n",
      "Epoch: 644, Train loss: 3.585, Val loss: 3.519, Epoch time = 1.587s\n",
      "Epoch: 645, Train loss: 3.272, Val loss: 5.188, Epoch time = 1.590s\n",
      "Epoch: 646, Train loss: 3.818, Val loss: 2.964, Epoch time = 1.477s\n",
      "Epoch: 647, Train loss: 3.532, Val loss: 3.550, Epoch time = 1.819s\n",
      "Epoch: 648, Train loss: 3.661, Val loss: 3.552, Epoch time = 1.518s\n",
      "Epoch: 649, Train loss: 3.697, Val loss: 2.971, Epoch time = 1.599s\n",
      "Epoch: 650, Train loss: 3.757, Val loss: 3.056, Epoch time = 1.662s\n",
      "Epoch: 651, Train loss: 3.270, Val loss: 3.502, Epoch time = 1.521s\n",
      "Epoch: 652, Train loss: 3.428, Val loss: 3.003, Epoch time = 1.536s\n",
      "Epoch: 653, Train loss: 3.999, Val loss: 2.979, Epoch time = 1.588s\n",
      "Epoch: 654, Train loss: 3.547, Val loss: 3.814, Epoch time = 1.376s\n",
      "Epoch: 655, Train loss: 3.514, Val loss: 3.740, Epoch time = 1.618s\n",
      "Epoch: 656, Train loss: 3.874, Val loss: 3.204, Epoch time = 1.727s\n",
      "Epoch: 657, Train loss: 3.723, Val loss: 3.316, Epoch time = 1.644s\n",
      "Epoch: 658, Train loss: 3.510, Val loss: 3.274, Epoch time = 1.483s\n",
      "Epoch: 659, Train loss: 3.761, Val loss: 3.609, Epoch time = 1.450s\n",
      "Epoch: 660, Train loss: 3.645, Val loss: 3.303, Epoch time = 1.684s\n",
      "Epoch: 661, Train loss: 3.212, Val loss: 3.613, Epoch time = 1.542s\n",
      "Epoch: 662, Train loss: 3.679, Val loss: 2.930, Epoch time = 1.518s\n",
      "Epoch: 663, Train loss: 3.387, Val loss: 3.048, Epoch time = 1.705s\n",
      "Epoch: 664, Train loss: 3.531, Val loss: 4.084, Epoch time = 1.691s\n",
      "Epoch: 665, Train loss: 3.841, Val loss: 3.402, Epoch time = 1.664s\n",
      "Epoch: 666, Train loss: 4.358, Val loss: 3.809, Epoch time = 1.589s\n",
      "Epoch: 667, Train loss: 3.322, Val loss: 3.179, Epoch time = 1.655s\n",
      "Epoch: 668, Train loss: 3.143, Val loss: 5.507, Epoch time = 1.594s\n",
      "Epoch: 669, Train loss: 3.716, Val loss: 3.558, Epoch time = 1.825s\n",
      "Epoch: 670, Train loss: 3.692, Val loss: 3.078, Epoch time = 1.547s\n",
      "Epoch: 671, Train loss: 3.262, Val loss: 3.253, Epoch time = 1.637s\n",
      "Epoch: 672, Train loss: 3.368, Val loss: 3.263, Epoch time = 1.674s\n",
      "Epoch: 673, Train loss: 3.327, Val loss: 3.309, Epoch time = 1.582s\n",
      "Epoch: 674, Train loss: 3.064, Val loss: 3.034, Epoch time = 1.528s\n",
      "Epoch: 675, Train loss: 3.581, Val loss: 2.976, Epoch time = 1.733s\n",
      "Epoch: 676, Train loss: 3.304, Val loss: 3.508, Epoch time = 1.634s\n",
      "Epoch: 677, Train loss: 3.395, Val loss: 3.482, Epoch time = 1.580s\n",
      "Epoch: 678, Train loss: 3.394, Val loss: 3.419, Epoch time = 1.681s\n",
      "Epoch: 679, Train loss: 4.095, Val loss: 3.554, Epoch time = 1.554s\n",
      "Epoch: 680, Train loss: 3.701, Val loss: 3.088, Epoch time = 1.553s\n",
      "Epoch: 681, Train loss: 3.010, Val loss: 3.415, Epoch time = 1.538s\n",
      "Epoch: 682, Train loss: 3.385, Val loss: 3.035, Epoch time = 1.651s\n",
      "Epoch: 683, Train loss: 3.710, Val loss: 3.283, Epoch time = 1.507s\n",
      "Epoch: 684, Train loss: 3.304, Val loss: 3.691, Epoch time = 1.564s\n",
      "Epoch: 685, Train loss: 3.515, Val loss: 3.159, Epoch time = 1.574s\n",
      "Epoch: 686, Train loss: 3.291, Val loss: 3.327, Epoch time = 1.419s\n",
      "Epoch: 687, Train loss: 4.286, Val loss: 3.204, Epoch time = 1.659s\n",
      "Epoch: 688, Train loss: 3.153, Val loss: 3.158, Epoch time = 1.646s\n",
      "Epoch: 689, Train loss: 3.548, Val loss: 2.967, Epoch time = 1.706s\n",
      "Epoch: 690, Train loss: 3.439, Val loss: 3.403, Epoch time = 1.435s\n",
      "Epoch: 691, Train loss: 3.502, Val loss: 3.034, Epoch time = 1.609s\n",
      "Epoch: 692, Train loss: 3.404, Val loss: 2.960, Epoch time = 1.651s\n",
      "Epoch: 693, Train loss: 3.563, Val loss: 5.306, Epoch time = 1.554s\n",
      "Epoch: 694, Train loss: 3.393, Val loss: 3.504, Epoch time = 1.857s\n",
      "Epoch: 695, Train loss: 3.528, Val loss: 2.914, Epoch time = 1.773s\n",
      "Epoch: 696, Train loss: 3.701, Val loss: 3.317, Epoch time = 1.578s\n",
      "Epoch: 697, Train loss: 3.278, Val loss: 2.796, Epoch time = 1.620s\n",
      "Epoch: 698, Train loss: 3.324, Val loss: 3.261, Epoch time = 1.666s\n",
      "Epoch: 699, Train loss: 3.094, Val loss: 3.200, Epoch time = 1.640s\n",
      "Epoch: 700, Train loss: 3.463, Val loss: 3.312, Epoch time = 1.600s\n",
      "Epoch: 701, Train loss: 3.281, Val loss: 3.216, Epoch time = 1.499s\n",
      "Epoch: 702, Train loss: 3.152, Val loss: 2.813, Epoch time = 1.394s\n",
      "Epoch: 703, Train loss: 3.330, Val loss: 3.922, Epoch time = 1.500s\n",
      "Epoch: 704, Train loss: 3.663, Val loss: 2.948, Epoch time = 1.579s\n",
      "Epoch: 705, Train loss: 3.534, Val loss: 2.798, Epoch time = 1.631s\n",
      "Epoch: 706, Train loss: 3.989, Val loss: 3.253, Epoch time = 1.419s\n",
      "Epoch: 707, Train loss: 3.914, Val loss: 3.137, Epoch time = 1.643s\n",
      "Epoch: 708, Train loss: 3.640, Val loss: 3.470, Epoch time = 1.595s\n",
      "Epoch: 709, Train loss: 3.489, Val loss: 3.073, Epoch time = 1.733s\n",
      "Epoch: 710, Train loss: 3.565, Val loss: 2.880, Epoch time = 1.587s\n",
      "Epoch: 711, Train loss: 3.342, Val loss: 3.133, Epoch time = 1.584s\n",
      "Epoch: 712, Train loss: 3.767, Val loss: 3.497, Epoch time = 1.541s\n",
      "Epoch: 713, Train loss: 3.760, Val loss: 3.283, Epoch time = 1.768s\n",
      "Epoch: 714, Train loss: 3.769, Val loss: 3.057, Epoch time = 1.576s\n",
      "Epoch: 715, Train loss: 3.576, Val loss: 3.267, Epoch time = 1.660s\n",
      "Epoch: 716, Train loss: 4.180, Val loss: 3.014, Epoch time = 1.609s\n",
      "Epoch: 717, Train loss: 3.514, Val loss: 3.291, Epoch time = 1.680s\n",
      "Epoch: 718, Train loss: 4.021, Val loss: 3.198, Epoch time = 1.734s\n",
      "Epoch: 719, Train loss: 2.943, Val loss: 3.117, Epoch time = 1.542s\n",
      "Epoch: 720, Train loss: 3.789, Val loss: 3.278, Epoch time = 1.422s\n",
      "Epoch: 721, Train loss: 3.743, Val loss: 3.181, Epoch time = 1.553s\n",
      "Epoch: 722, Train loss: 3.509, Val loss: 3.302, Epoch time = 1.355s\n",
      "Epoch: 723, Train loss: 3.258, Val loss: 3.250, Epoch time = 1.560s\n",
      "Epoch: 724, Train loss: 2.865, Val loss: 3.421, Epoch time = 1.633s\n",
      "Epoch: 725, Train loss: 3.496, Val loss: 3.042, Epoch time = 1.732s\n",
      "Epoch: 726, Train loss: 3.347, Val loss: 2.843, Epoch time = 1.528s\n",
      "Epoch: 727, Train loss: 2.859, Val loss: 3.273, Epoch time = 1.486s\n",
      "Epoch: 728, Train loss: 3.402, Val loss: 3.291, Epoch time = 1.553s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 729, Train loss: 3.880, Val loss: 2.987, Epoch time = 1.606s\n",
      "Epoch: 730, Train loss: 3.483, Val loss: 3.414, Epoch time = 1.516s\n",
      "Epoch: 731, Train loss: 3.877, Val loss: 3.185, Epoch time = 1.508s\n",
      "Epoch: 732, Train loss: 3.753, Val loss: 3.335, Epoch time = 1.615s\n",
      "Epoch: 733, Train loss: 3.541, Val loss: 3.477, Epoch time = 1.593s\n",
      "Epoch: 734, Train loss: 2.886, Val loss: 3.294, Epoch time = 1.519s\n",
      "Epoch: 735, Train loss: 3.680, Val loss: 3.863, Epoch time = 1.351s\n",
      "Epoch: 736, Train loss: 3.595, Val loss: 3.232, Epoch time = 1.610s\n",
      "Epoch: 737, Train loss: 3.666, Val loss: 3.389, Epoch time = 1.549s\n",
      "Epoch: 738, Train loss: 3.863, Val loss: 3.098, Epoch time = 1.616s\n",
      "Epoch: 739, Train loss: 3.381, Val loss: 3.234, Epoch time = 1.581s\n",
      "Epoch: 740, Train loss: 3.422, Val loss: 2.998, Epoch time = 1.409s\n",
      "Epoch: 741, Train loss: 3.672, Val loss: 2.883, Epoch time = 1.431s\n",
      "Epoch: 742, Train loss: 3.630, Val loss: 3.164, Epoch time = 1.560s\n",
      "Epoch: 743, Train loss: 3.672, Val loss: 3.394, Epoch time = 1.855s\n",
      "Epoch: 744, Train loss: 3.394, Val loss: 3.200, Epoch time = 1.404s\n",
      "Epoch: 745, Train loss: 3.661, Val loss: 3.509, Epoch time = 1.596s\n",
      "Epoch: 746, Train loss: 3.942, Val loss: 3.264, Epoch time = 1.644s\n",
      "Epoch: 747, Train loss: 3.332, Val loss: 4.831, Epoch time = 1.685s\n",
      "Epoch: 748, Train loss: 3.691, Val loss: 3.486, Epoch time = 1.608s\n",
      "Epoch: 749, Train loss: 3.720, Val loss: 3.357, Epoch time = 1.757s\n",
      "Epoch: 750, Train loss: 2.987, Val loss: 3.117, Epoch time = 1.696s\n",
      "Epoch: 751, Train loss: 3.926, Val loss: 3.022, Epoch time = 1.616s\n",
      "Epoch: 752, Train loss: 3.440, Val loss: 3.016, Epoch time = 1.425s\n",
      "Epoch: 753, Train loss: 3.463, Val loss: 3.238, Epoch time = 1.480s\n",
      "Epoch: 754, Train loss: 4.805, Val loss: 3.745, Epoch time = 1.479s\n",
      "Epoch: 755, Train loss: 3.306, Val loss: 3.078, Epoch time = 1.637s\n",
      "Epoch: 756, Train loss: 3.210, Val loss: 3.235, Epoch time = 1.397s\n",
      "Epoch: 757, Train loss: 3.404, Val loss: 4.583, Epoch time = 1.624s\n",
      "Epoch: 758, Train loss: 3.499, Val loss: 4.750, Epoch time = 1.801s\n",
      "Epoch: 759, Train loss: 3.508, Val loss: 3.239, Epoch time = 1.631s\n",
      "Epoch: 760, Train loss: 3.258, Val loss: 2.909, Epoch time = 1.399s\n",
      "Epoch: 761, Train loss: 3.475, Val loss: 2.987, Epoch time = 1.721s\n",
      "Epoch: 762, Train loss: 3.462, Val loss: 2.841, Epoch time = 1.722s\n",
      "Epoch: 763, Train loss: 3.120, Val loss: 3.124, Epoch time = 1.421s\n",
      "Epoch: 764, Train loss: 3.628, Val loss: 3.294, Epoch time = 1.703s\n",
      "Epoch: 765, Train loss: 3.331, Val loss: 2.930, Epoch time = 1.555s\n",
      "Epoch: 766, Train loss: 3.176, Val loss: 3.192, Epoch time = 1.949s\n",
      "Epoch: 767, Train loss: 3.921, Val loss: 3.145, Epoch time = 1.578s\n",
      "Epoch: 768, Train loss: 3.234, Val loss: 3.298, Epoch time = 1.688s\n",
      "Epoch: 769, Train loss: 3.734, Val loss: 3.261, Epoch time = 1.698s\n",
      "Epoch: 770, Train loss: 3.648, Val loss: 3.148, Epoch time = 1.657s\n",
      "Epoch: 771, Train loss: 3.616, Val loss: 3.023, Epoch time = 1.634s\n",
      "Epoch: 772, Train loss: 3.550, Val loss: 3.168, Epoch time = 1.581s\n",
      "Epoch: 773, Train loss: 3.425, Val loss: 3.033, Epoch time = 1.651s\n",
      "Epoch: 774, Train loss: 3.141, Val loss: 3.684, Epoch time = 1.524s\n",
      "Epoch: 775, Train loss: 3.524, Val loss: 3.166, Epoch time = 1.587s\n",
      "Epoch: 776, Train loss: 3.476, Val loss: 3.063, Epoch time = 1.759s\n",
      "Epoch: 777, Train loss: 3.902, Val loss: 2.799, Epoch time = 1.663s\n",
      "Epoch: 778, Train loss: 3.603, Val loss: 2.999, Epoch time = 1.911s\n",
      "Epoch: 779, Train loss: 3.892, Val loss: 3.274, Epoch time = 1.460s\n",
      "Epoch: 780, Train loss: 3.062, Val loss: 3.238, Epoch time = 1.679s\n",
      "Epoch: 781, Train loss: 3.251, Val loss: 3.349, Epoch time = 1.601s\n",
      "Epoch: 782, Train loss: 3.168, Val loss: 3.289, Epoch time = 1.692s\n",
      "Epoch: 783, Train loss: 3.531, Val loss: 3.099, Epoch time = 1.626s\n",
      "Epoch: 784, Train loss: 3.225, Val loss: 3.068, Epoch time = 1.477s\n",
      "Epoch: 785, Train loss: 3.608, Val loss: 3.022, Epoch time = 1.447s\n",
      "Epoch: 786, Train loss: 3.534, Val loss: 2.774, Epoch time = 1.528s\n",
      "Epoch: 787, Train loss: 3.030, Val loss: 3.062, Epoch time = 1.764s\n",
      "Epoch: 788, Train loss: 3.604, Val loss: 4.949, Epoch time = 1.575s\n",
      "Epoch: 789, Train loss: 3.255, Val loss: 2.806, Epoch time = 1.589s\n",
      "Epoch: 790, Train loss: 3.086, Val loss: 3.182, Epoch time = 1.478s\n",
      "Epoch: 791, Train loss: 3.695, Val loss: 3.131, Epoch time = 1.692s\n",
      "Epoch: 792, Train loss: 3.648, Val loss: 3.045, Epoch time = 1.620s\n",
      "Epoch: 793, Train loss: 3.307, Val loss: 3.229, Epoch time = 1.474s\n",
      "Epoch: 794, Train loss: 3.186, Val loss: 3.498, Epoch time = 1.562s\n",
      "Epoch: 795, Train loss: 3.655, Val loss: 3.408, Epoch time = 1.785s\n",
      "Epoch: 796, Train loss: 3.655, Val loss: 3.545, Epoch time = 1.599s\n",
      "Epoch: 797, Train loss: 3.062, Val loss: 3.345, Epoch time = 1.593s\n",
      "Epoch: 798, Train loss: 3.781, Val loss: 3.139, Epoch time = 1.741s\n",
      "Epoch: 799, Train loss: 3.163, Val loss: 3.004, Epoch time = 1.716s\n",
      "Epoch: 800, Train loss: 3.284, Val loss: 2.922, Epoch time = 1.569s\n",
      "Epoch: 801, Train loss: 3.727, Val loss: 3.099, Epoch time = 1.521s\n",
      "Epoch: 802, Train loss: 3.574, Val loss: 3.144, Epoch time = 1.500s\n",
      "Epoch: 803, Train loss: 3.428, Val loss: 2.818, Epoch time = 1.506s\n",
      "Epoch: 804, Train loss: 3.913, Val loss: 3.097, Epoch time = 1.672s\n",
      "Epoch: 805, Train loss: 3.534, Val loss: 3.058, Epoch time = 1.618s\n",
      "Epoch: 806, Train loss: 3.398, Val loss: 3.625, Epoch time = 1.367s\n",
      "Epoch: 807, Train loss: 3.638, Val loss: 5.333, Epoch time = 1.455s\n",
      "Epoch: 808, Train loss: 3.159, Val loss: 3.031, Epoch time = 1.596s\n",
      "Epoch: 809, Train loss: 3.249, Val loss: 3.213, Epoch time = 1.326s\n",
      "Epoch: 810, Train loss: 3.027, Val loss: 3.178, Epoch time = 1.528s\n",
      "Epoch: 811, Train loss: 3.289, Val loss: 3.644, Epoch time = 1.405s\n",
      "Epoch: 812, Train loss: 3.917, Val loss: 3.131, Epoch time = 1.621s\n",
      "Epoch: 813, Train loss: 3.765, Val loss: 2.800, Epoch time = 1.550s\n",
      "Epoch: 814, Train loss: 3.712, Val loss: 3.257, Epoch time = 1.561s\n",
      "Epoch: 815, Train loss: 3.643, Val loss: 2.954, Epoch time = 1.732s\n",
      "Epoch: 816, Train loss: 3.772, Val loss: 3.106, Epoch time = 1.622s\n",
      "Epoch: 817, Train loss: 3.738, Val loss: 3.218, Epoch time = 1.747s\n",
      "Epoch: 818, Train loss: 3.266, Val loss: 3.097, Epoch time = 1.373s\n",
      "Epoch: 819, Train loss: 2.912, Val loss: 3.321, Epoch time = 1.783s\n",
      "Epoch: 820, Train loss: 3.558, Val loss: 3.150, Epoch time = 1.546s\n",
      "Epoch: 821, Train loss: 3.273, Val loss: 3.446, Epoch time = 1.483s\n",
      "Epoch: 822, Train loss: 3.533, Val loss: 2.966, Epoch time = 1.286s\n",
      "Epoch: 823, Train loss: 3.104, Val loss: 2.918, Epoch time = 1.566s\n",
      "Epoch: 824, Train loss: 2.961, Val loss: 2.848, Epoch time = 1.418s\n",
      "Epoch: 825, Train loss: 3.964, Val loss: 3.178, Epoch time = 1.237s\n",
      "Epoch: 826, Train loss: 3.229, Val loss: 3.716, Epoch time = 1.505s\n",
      "Epoch: 827, Train loss: 3.544, Val loss: 2.972, Epoch time = 1.847s\n",
      "Epoch: 828, Train loss: 3.377, Val loss: 3.159, Epoch time = 1.634s\n",
      "Epoch: 829, Train loss: 3.673, Val loss: 3.648, Epoch time = 1.677s\n",
      "Epoch: 830, Train loss: 3.554, Val loss: 3.147, Epoch time = 1.577s\n",
      "Epoch: 831, Train loss: 3.720, Val loss: 3.105, Epoch time = 1.567s\n",
      "Epoch: 832, Train loss: 3.272, Val loss: 3.171, Epoch time = 1.716s\n",
      "Epoch: 833, Train loss: 3.145, Val loss: 3.101, Epoch time = 1.506s\n",
      "Epoch: 834, Train loss: 3.477, Val loss: 3.383, Epoch time = 1.584s\n",
      "Epoch: 835, Train loss: 3.561, Val loss: 2.906, Epoch time = 1.596s\n",
      "Epoch: 836, Train loss: 3.171, Val loss: 3.486, Epoch time = 1.565s\n",
      "Epoch: 837, Train loss: 3.502, Val loss: 3.183, Epoch time = 1.589s\n",
      "Epoch: 838, Train loss: 3.112, Val loss: 2.948, Epoch time = 1.678s\n",
      "Epoch: 839, Train loss: 3.084, Val loss: 3.265, Epoch time = 1.489s\n",
      "Epoch: 840, Train loss: 3.908, Val loss: 3.099, Epoch time = 1.469s\n",
      "Epoch: 841, Train loss: 3.031, Val loss: 2.934, Epoch time = 1.683s\n",
      "Epoch: 842, Train loss: 3.122, Val loss: 3.441, Epoch time = 1.645s\n",
      "Epoch: 843, Train loss: 3.429, Val loss: 3.414, Epoch time = 1.725s\n",
      "Epoch: 844, Train loss: 3.381, Val loss: 3.501, Epoch time = 1.551s\n",
      "Epoch: 845, Train loss: 3.148, Val loss: 3.054, Epoch time = 1.618s\n",
      "Epoch: 846, Train loss: 4.077, Val loss: 3.510, Epoch time = 1.399s\n",
      "Epoch: 847, Train loss: 3.202, Val loss: 3.223, Epoch time = 1.379s\n",
      "Epoch: 848, Train loss: 3.413, Val loss: 2.884, Epoch time = 1.535s\n",
      "Epoch: 849, Train loss: 3.505, Val loss: 3.055, Epoch time = 1.563s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 850, Train loss: 3.780, Val loss: 3.364, Epoch time = 1.580s\n",
      "Epoch: 851, Train loss: 3.129, Val loss: 2.896, Epoch time = 1.729s\n",
      "Epoch: 852, Train loss: 3.237, Val loss: 2.787, Epoch time = 1.522s\n",
      "Epoch: 853, Train loss: 3.944, Val loss: 3.098, Epoch time = 1.802s\n",
      "Epoch: 854, Train loss: 4.370, Val loss: 4.191, Epoch time = 1.310s\n",
      "Epoch: 855, Train loss: 3.868, Val loss: 3.163, Epoch time = 1.537s\n",
      "Epoch: 856, Train loss: 3.268, Val loss: 3.238, Epoch time = 1.769s\n",
      "Epoch: 857, Train loss: 3.586, Val loss: 3.084, Epoch time = 1.498s\n",
      "Epoch: 858, Train loss: 3.362, Val loss: 3.021, Epoch time = 1.737s\n",
      "Epoch: 859, Train loss: 3.970, Val loss: 3.116, Epoch time = 1.622s\n",
      "Epoch: 860, Train loss: 3.487, Val loss: 3.242, Epoch time = 1.511s\n",
      "Epoch: 861, Train loss: 3.256, Val loss: 3.089, Epoch time = 1.542s\n",
      "Epoch: 862, Train loss: 3.848, Val loss: 2.850, Epoch time = 1.553s\n",
      "Epoch: 863, Train loss: 3.830, Val loss: 3.054, Epoch time = 1.625s\n",
      "Epoch: 864, Train loss: 3.807, Val loss: 3.873, Epoch time = 1.715s\n",
      "Epoch: 865, Train loss: 3.388, Val loss: 3.311, Epoch time = 1.432s\n",
      "Epoch: 866, Train loss: 3.370, Val loss: 2.886, Epoch time = 1.593s\n",
      "Epoch: 867, Train loss: 3.670, Val loss: 3.037, Epoch time = 1.619s\n",
      "Epoch: 868, Train loss: 2.973, Val loss: 3.049, Epoch time = 1.486s\n",
      "Epoch: 869, Train loss: 3.124, Val loss: 2.976, Epoch time = 1.587s\n",
      "Epoch: 870, Train loss: 3.156, Val loss: 3.031, Epoch time = 1.470s\n",
      "Epoch: 871, Train loss: 3.593, Val loss: 2.701, Epoch time = 1.480s\n",
      "Epoch: 872, Train loss: 3.241, Val loss: 3.004, Epoch time = 1.430s\n",
      "Epoch: 873, Train loss: 3.806, Val loss: 3.147, Epoch time = 1.425s\n",
      "Epoch: 874, Train loss: 3.406, Val loss: 3.098, Epoch time = 1.593s\n",
      "Epoch: 875, Train loss: 3.352, Val loss: 3.605, Epoch time = 1.488s\n",
      "Epoch: 876, Train loss: 2.778, Val loss: 2.831, Epoch time = 1.375s\n",
      "Epoch: 877, Train loss: 3.155, Val loss: 3.290, Epoch time = 1.518s\n",
      "Epoch: 878, Train loss: 3.737, Val loss: 3.340, Epoch time = 1.513s\n",
      "Epoch: 879, Train loss: 3.196, Val loss: 3.330, Epoch time = 1.516s\n",
      "Epoch: 880, Train loss: 3.874, Val loss: 3.027, Epoch time = 1.676s\n",
      "Epoch: 881, Train loss: 3.634, Val loss: 3.155, Epoch time = 1.630s\n",
      "Epoch: 882, Train loss: 3.417, Val loss: 4.103, Epoch time = 1.568s\n",
      "Epoch: 883, Train loss: 3.322, Val loss: 3.293, Epoch time = 1.543s\n",
      "Epoch: 884, Train loss: 4.074, Val loss: 3.046, Epoch time = 1.771s\n",
      "Epoch: 885, Train loss: 3.157, Val loss: 3.238, Epoch time = 1.601s\n",
      "Epoch: 886, Train loss: 2.889, Val loss: 3.111, Epoch time = 1.691s\n",
      "Epoch: 887, Train loss: 3.735, Val loss: 3.943, Epoch time = 1.616s\n",
      "Epoch: 888, Train loss: 3.503, Val loss: 3.027, Epoch time = 1.628s\n",
      "Epoch: 889, Train loss: 3.254, Val loss: 2.770, Epoch time = 1.564s\n",
      "Epoch: 890, Train loss: 3.260, Val loss: 3.318, Epoch time = 1.633s\n",
      "Epoch: 891, Train loss: 3.725, Val loss: 2.779, Epoch time = 1.602s\n",
      "Epoch: 892, Train loss: 3.743, Val loss: 3.068, Epoch time = 1.662s\n",
      "Epoch: 893, Train loss: 3.351, Val loss: 3.381, Epoch time = 1.481s\n",
      "Epoch: 894, Train loss: 3.007, Val loss: 3.176, Epoch time = 1.480s\n",
      "Epoch: 895, Train loss: 3.599, Val loss: 3.490, Epoch time = 1.685s\n",
      "Epoch: 896, Train loss: 3.642, Val loss: 2.803, Epoch time = 1.569s\n",
      "Epoch: 897, Train loss: 3.458, Val loss: 2.991, Epoch time = 1.533s\n",
      "Epoch: 898, Train loss: 3.561, Val loss: 2.958, Epoch time = 1.580s\n",
      "Epoch: 899, Train loss: 3.111, Val loss: 2.830, Epoch time = 1.649s\n",
      "Epoch: 900, Train loss: 3.160, Val loss: 3.133, Epoch time = 1.466s\n",
      "Epoch: 901, Train loss: 3.216, Val loss: 3.954, Epoch time = 1.619s\n",
      "Epoch: 902, Train loss: 4.010, Val loss: 3.059, Epoch time = 1.426s\n",
      "Epoch: 903, Train loss: 3.665, Val loss: 2.930, Epoch time = 1.510s\n",
      "Epoch: 904, Train loss: 2.788, Val loss: 2.629, Epoch time = 1.515s\n",
      "Epoch: 905, Train loss: 3.343, Val loss: 2.950, Epoch time = 1.436s\n",
      "Epoch: 906, Train loss: 3.162, Val loss: 3.111, Epoch time = 1.485s\n",
      "Epoch: 907, Train loss: 3.608, Val loss: 3.156, Epoch time = 1.673s\n",
      "Epoch: 908, Train loss: 3.494, Val loss: 3.620, Epoch time = 1.473s\n",
      "Epoch: 909, Train loss: 3.162, Val loss: 3.100, Epoch time = 1.540s\n",
      "Epoch: 910, Train loss: 3.666, Val loss: 2.817, Epoch time = 1.608s\n",
      "Epoch: 911, Train loss: 3.722, Val loss: 3.108, Epoch time = 1.509s\n",
      "Epoch: 912, Train loss: 3.442, Val loss: 2.861, Epoch time = 1.606s\n",
      "Epoch: 913, Train loss: 4.762, Val loss: 2.888, Epoch time = 1.492s\n",
      "Epoch: 914, Train loss: 3.045, Val loss: 4.535, Epoch time = 1.541s\n",
      "Epoch: 915, Train loss: 3.005, Val loss: 2.859, Epoch time = 1.478s\n",
      "Epoch: 916, Train loss: 3.512, Val loss: 3.092, Epoch time = 1.579s\n",
      "Epoch: 917, Train loss: 3.284, Val loss: 3.089, Epoch time = 1.681s\n",
      "Epoch: 918, Train loss: 3.992, Val loss: 3.191, Epoch time = 1.690s\n",
      "Epoch: 919, Train loss: 3.428, Val loss: 3.358, Epoch time = 1.432s\n",
      "Epoch: 920, Train loss: 3.089, Val loss: 3.604, Epoch time = 1.379s\n",
      "Epoch: 921, Train loss: 3.574, Val loss: 3.197, Epoch time = 1.537s\n",
      "Epoch: 922, Train loss: 3.594, Val loss: 2.978, Epoch time = 1.648s\n",
      "Epoch: 923, Train loss: 3.412, Val loss: 3.640, Epoch time = 1.387s\n",
      "Epoch: 924, Train loss: 3.485, Val loss: 4.683, Epoch time = 1.435s\n",
      "Epoch: 925, Train loss: 3.545, Val loss: 2.982, Epoch time = 1.594s\n",
      "Epoch: 926, Train loss: 3.293, Val loss: 3.238, Epoch time = 1.648s\n",
      "Epoch: 927, Train loss: 3.568, Val loss: 2.956, Epoch time = 1.528s\n",
      "Epoch: 928, Train loss: 3.441, Val loss: 2.880, Epoch time = 1.438s\n",
      "Epoch: 929, Train loss: 2.978, Val loss: 3.200, Epoch time = 1.719s\n",
      "Epoch: 930, Train loss: 2.999, Val loss: 3.200, Epoch time = 1.540s\n",
      "Epoch: 931, Train loss: 3.465, Val loss: 2.824, Epoch time = 1.569s\n",
      "Epoch: 932, Train loss: 3.330, Val loss: 3.117, Epoch time = 1.557s\n",
      "Epoch: 933, Train loss: 3.402, Val loss: 3.094, Epoch time = 1.681s\n",
      "Epoch: 934, Train loss: 3.696, Val loss: 3.281, Epoch time = 1.728s\n",
      "Epoch: 935, Train loss: 3.390, Val loss: 3.522, Epoch time = 1.316s\n",
      "Epoch: 936, Train loss: 3.323, Val loss: 3.302, Epoch time = 1.723s\n",
      "Epoch: 937, Train loss: 3.755, Val loss: 3.155, Epoch time = 1.723s\n",
      "Epoch: 938, Train loss: 3.085, Val loss: 2.971, Epoch time = 1.632s\n",
      "Epoch: 939, Train loss: 3.299, Val loss: 3.820, Epoch time = 1.664s\n",
      "Epoch: 940, Train loss: 3.264, Val loss: 3.034, Epoch time = 1.588s\n",
      "Epoch: 941, Train loss: 3.581, Val loss: 2.998, Epoch time = 1.462s\n",
      "Epoch: 942, Train loss: 4.054, Val loss: 2.948, Epoch time = 1.515s\n",
      "Epoch: 943, Train loss: 3.525, Val loss: 3.071, Epoch time = 1.635s\n",
      "Epoch: 944, Train loss: 3.825, Val loss: 3.122, Epoch time = 1.563s\n",
      "Epoch: 945, Train loss: 3.668, Val loss: 2.791, Epoch time = 1.582s\n",
      "Epoch: 946, Train loss: 3.189, Val loss: 3.373, Epoch time = 1.390s\n",
      "Epoch: 947, Train loss: 3.558, Val loss: 3.086, Epoch time = 1.658s\n",
      "Epoch: 948, Train loss: 3.501, Val loss: 2.909, Epoch time = 1.646s\n",
      "Epoch: 949, Train loss: 3.469, Val loss: 2.876, Epoch time = 1.448s\n",
      "Epoch: 950, Train loss: 4.238, Val loss: 3.312, Epoch time = 1.618s\n",
      "Epoch: 951, Train loss: 2.952, Val loss: 3.065, Epoch time = 1.516s\n",
      "Epoch: 952, Train loss: 3.476, Val loss: 3.143, Epoch time = 1.604s\n",
      "Epoch: 953, Train loss: 2.919, Val loss: 3.621, Epoch time = 1.487s\n",
      "Epoch: 954, Train loss: 3.435, Val loss: 2.882, Epoch time = 1.475s\n",
      "Epoch: 955, Train loss: 3.020, Val loss: 2.887, Epoch time = 1.576s\n",
      "Epoch: 956, Train loss: 3.276, Val loss: 3.621, Epoch time = 1.622s\n",
      "Epoch: 957, Train loss: 3.952, Val loss: 2.867, Epoch time = 1.656s\n",
      "Epoch: 958, Train loss: 3.370, Val loss: 3.243, Epoch time = 1.569s\n",
      "Epoch: 959, Train loss: 3.382, Val loss: 3.452, Epoch time = 1.499s\n",
      "Epoch: 960, Train loss: 3.541, Val loss: 3.374, Epoch time = 1.659s\n",
      "Epoch: 961, Train loss: 3.145, Val loss: 2.933, Epoch time = 1.496s\n",
      "Epoch: 962, Train loss: 3.129, Val loss: 3.291, Epoch time = 1.483s\n",
      "Epoch: 963, Train loss: 3.200, Val loss: 3.006, Epoch time = 1.499s\n",
      "Epoch: 964, Train loss: 3.042, Val loss: 3.028, Epoch time = 1.595s\n",
      "Epoch: 965, Train loss: 4.397, Val loss: 3.213, Epoch time = 1.582s\n",
      "Epoch: 966, Train loss: 3.471, Val loss: 3.240, Epoch time = 1.505s\n",
      "Epoch: 967, Train loss: 3.105, Val loss: 2.725, Epoch time = 1.546s\n",
      "Epoch: 968, Train loss: 4.045, Val loss: 3.313, Epoch time = 1.597s\n",
      "Epoch: 969, Train loss: 3.012, Val loss: 3.156, Epoch time = 1.390s\n",
      "Epoch: 970, Train loss: 3.410, Val loss: 3.260, Epoch time = 1.640s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 971, Train loss: 3.715, Val loss: 3.588, Epoch time = 1.647s\n",
      "Epoch: 972, Train loss: 3.129, Val loss: 3.029, Epoch time = 1.694s\n",
      "Epoch: 973, Train loss: 2.931, Val loss: 2.825, Epoch time = 1.431s\n",
      "Epoch: 974, Train loss: 3.518, Val loss: 3.266, Epoch time = 1.606s\n",
      "Epoch: 975, Train loss: 3.435, Val loss: 2.975, Epoch time = 1.676s\n",
      "Epoch: 976, Train loss: 4.422, Val loss: 3.118, Epoch time = 1.721s\n",
      "Epoch: 977, Train loss: 3.333, Val loss: 2.588, Epoch time = 1.593s\n",
      "Epoch: 978, Train loss: 3.573, Val loss: 3.096, Epoch time = 1.573s\n",
      "Epoch: 979, Train loss: 3.170, Val loss: 3.138, Epoch time = 1.559s\n",
      "Epoch: 980, Train loss: 3.805, Val loss: 3.081, Epoch time = 1.560s\n",
      "Epoch: 981, Train loss: 3.807, Val loss: 2.953, Epoch time = 1.702s\n",
      "Epoch: 982, Train loss: 3.678, Val loss: 2.865, Epoch time = 1.423s\n",
      "Epoch: 983, Train loss: 3.137, Val loss: 4.145, Epoch time = 1.472s\n",
      "Epoch: 984, Train loss: 3.867, Val loss: 3.043, Epoch time = 1.564s\n",
      "Epoch: 985, Train loss: 3.918, Val loss: 3.285, Epoch time = 1.423s\n",
      "Epoch: 986, Train loss: 3.861, Val loss: 3.106, Epoch time = 1.712s\n",
      "Epoch: 987, Train loss: 3.493, Val loss: 2.861, Epoch time = 1.683s\n",
      "Epoch: 988, Train loss: 3.695, Val loss: 2.981, Epoch time = 1.590s\n",
      "Epoch: 989, Train loss: 3.059, Val loss: 3.258, Epoch time = 1.433s\n",
      "Epoch: 990, Train loss: 3.403, Val loss: 3.123, Epoch time = 1.606s\n",
      "Epoch: 991, Train loss: 3.458, Val loss: 3.099, Epoch time = 1.499s\n",
      "Epoch: 992, Train loss: 3.509, Val loss: 2.887, Epoch time = 1.646s\n",
      "Epoch: 993, Train loss: 3.522, Val loss: 2.852, Epoch time = 1.523s\n",
      "Epoch: 994, Train loss: 3.413, Val loss: 2.621, Epoch time = 1.610s\n",
      "Epoch: 995, Train loss: 2.973, Val loss: 2.880, Epoch time = 1.627s\n",
      "Epoch: 996, Train loss: 3.116, Val loss: 2.818, Epoch time = 1.343s\n",
      "Epoch: 997, Train loss: 3.617, Val loss: 2.986, Epoch time = 1.517s\n",
      "Epoch: 998, Train loss: 3.631, Val loss: 3.970, Epoch time = 1.395s\n",
      "Epoch: 999, Train loss: 3.687, Val loss: 3.254, Epoch time = 1.491s\n",
      "Epoch: 1000, Train loss: 3.350, Val loss: 3.070, Epoch time = 1.552s\n",
      "Epoch: 1001, Train loss: 3.358, Val loss: 3.061, Epoch time = 1.768s\n",
      "Epoch: 1002, Train loss: 2.884, Val loss: 3.148, Epoch time = 1.682s\n",
      "Epoch: 1003, Train loss: 3.249, Val loss: 2.955, Epoch time = 1.562s\n",
      "Epoch: 1004, Train loss: 3.574, Val loss: 2.999, Epoch time = 1.456s\n",
      "Epoch: 1005, Train loss: 3.122, Val loss: 3.092, Epoch time = 1.755s\n",
      "Epoch: 1006, Train loss: 3.475, Val loss: 3.164, Epoch time = 1.701s\n",
      "Epoch: 1007, Train loss: 3.166, Val loss: 2.785, Epoch time = 1.700s\n",
      "Epoch: 1008, Train loss: 3.558, Val loss: 3.207, Epoch time = 1.598s\n",
      "Epoch: 1009, Train loss: 3.252, Val loss: 2.899, Epoch time = 1.621s\n",
      "Epoch: 1010, Train loss: 3.418, Val loss: 3.027, Epoch time = 1.492s\n",
      "Epoch: 1011, Train loss: 3.701, Val loss: 4.184, Epoch time = 1.563s\n",
      "Epoch: 1012, Train loss: 3.524, Val loss: 2.947, Epoch time = 1.329s\n",
      "Epoch: 1013, Train loss: 3.616, Val loss: 4.402, Epoch time = 1.450s\n",
      "Epoch: 1014, Train loss: 3.656, Val loss: 2.836, Epoch time = 1.525s\n",
      "Epoch: 1015, Train loss: 3.838, Val loss: 3.047, Epoch time = 1.575s\n",
      "Epoch: 1016, Train loss: 3.333, Val loss: 3.182, Epoch time = 1.574s\n",
      "Epoch: 1017, Train loss: 2.795, Val loss: 2.529, Epoch time = 1.309s\n",
      "Epoch: 1018, Train loss: 3.406, Val loss: 3.290, Epoch time = 1.784s\n",
      "Epoch: 1019, Train loss: 3.461, Val loss: 3.222, Epoch time = 1.719s\n",
      "Epoch: 1020, Train loss: 3.118, Val loss: 3.014, Epoch time = 1.556s\n",
      "Epoch: 1021, Train loss: 3.809, Val loss: 3.169, Epoch time = 1.622s\n",
      "Epoch: 1022, Train loss: 3.260, Val loss: 2.873, Epoch time = 1.397s\n",
      "Epoch: 1023, Train loss: 2.645, Val loss: 2.848, Epoch time = 1.465s\n",
      "Epoch: 1024, Train loss: 3.899, Val loss: 3.026, Epoch time = 1.587s\n",
      "Epoch: 1025, Train loss: 3.305, Val loss: 3.310, Epoch time = 1.609s\n",
      "Epoch: 1026, Train loss: 2.913, Val loss: 3.265, Epoch time = 1.292s\n",
      "Epoch: 1027, Train loss: 3.579, Val loss: 3.293, Epoch time = 1.594s\n",
      "Epoch: 1028, Train loss: 3.786, Val loss: 3.196, Epoch time = 1.497s\n",
      "Epoch: 1029, Train loss: 3.953, Val loss: 2.803, Epoch time = 1.705s\n",
      "Epoch: 1030, Train loss: 3.251, Val loss: 2.859, Epoch time = 1.533s\n",
      "Epoch: 1031, Train loss: 3.773, Val loss: 2.616, Epoch time = 1.592s\n",
      "Epoch: 1032, Train loss: 3.949, Val loss: 3.228, Epoch time = 1.393s\n",
      "Epoch: 1033, Train loss: 3.068, Val loss: 3.351, Epoch time = 1.629s\n",
      "Epoch: 1034, Train loss: 3.876, Val loss: 2.696, Epoch time = 1.754s\n",
      "Epoch: 1035, Train loss: 3.172, Val loss: 2.778, Epoch time = 1.757s\n",
      "Epoch: 1036, Train loss: 3.683, Val loss: 3.039, Epoch time = 1.589s\n",
      "Epoch: 1037, Train loss: 3.434, Val loss: 2.777, Epoch time = 1.552s\n",
      "Epoch: 1038, Train loss: 3.554, Val loss: 2.902, Epoch time = 1.422s\n",
      "Epoch: 1039, Train loss: 3.340, Val loss: 2.876, Epoch time = 1.567s\n",
      "Epoch: 1040, Train loss: 3.660, Val loss: 2.838, Epoch time = 1.713s\n",
      "Epoch: 1041, Train loss: 3.189, Val loss: 2.783, Epoch time = 1.600s\n",
      "Epoch: 1042, Train loss: 3.193, Val loss: 2.891, Epoch time = 1.732s\n",
      "Epoch: 1043, Train loss: 3.467, Val loss: 3.061, Epoch time = 1.696s\n",
      "Epoch: 1044, Train loss: 3.224, Val loss: 2.765, Epoch time = 1.650s\n",
      "Epoch: 1045, Train loss: 3.486, Val loss: 2.984, Epoch time = 1.747s\n",
      "Epoch: 1046, Train loss: 3.548, Val loss: 3.146, Epoch time = 1.616s\n",
      "Epoch: 1047, Train loss: 3.276, Val loss: 2.556, Epoch time = 1.618s\n",
      "Epoch: 1048, Train loss: 3.257, Val loss: 2.781, Epoch time = 1.613s\n",
      "Epoch: 1049, Train loss: 3.336, Val loss: 2.926, Epoch time = 1.475s\n",
      "Epoch: 1050, Train loss: 3.692, Val loss: 2.998, Epoch time = 1.622s\n",
      "Epoch: 1051, Train loss: 3.405, Val loss: 2.983, Epoch time = 1.722s\n",
      "Epoch: 1052, Train loss: 3.495, Val loss: 2.682, Epoch time = 1.531s\n",
      "Epoch: 1053, Train loss: 3.714, Val loss: 3.622, Epoch time = 1.627s\n",
      "Epoch: 1054, Train loss: 3.764, Val loss: 2.423, Epoch time = 1.409s\n",
      "Epoch: 1055, Train loss: 3.327, Val loss: 2.846, Epoch time = 1.647s\n",
      "Epoch: 1056, Train loss: 3.501, Val loss: 2.794, Epoch time = 1.463s\n",
      "Epoch: 1057, Train loss: 3.046, Val loss: 3.065, Epoch time = 1.643s\n",
      "Epoch: 1058, Train loss: 3.804, Val loss: 2.942, Epoch time = 1.437s\n",
      "Epoch: 1059, Train loss: 3.302, Val loss: 3.109, Epoch time = 1.470s\n",
      "Epoch: 1060, Train loss: 3.722, Val loss: 2.838, Epoch time = 1.597s\n",
      "Epoch: 1061, Train loss: 3.404, Val loss: 2.852, Epoch time = 1.431s\n",
      "Epoch: 1062, Train loss: 3.489, Val loss: 3.090, Epoch time = 1.601s\n",
      "Epoch: 1063, Train loss: 3.890, Val loss: 3.400, Epoch time = 1.547s\n",
      "Epoch: 1064, Train loss: 3.595, Val loss: 2.737, Epoch time = 1.925s\n",
      "Epoch: 1065, Train loss: 3.062, Val loss: 3.375, Epoch time = 1.669s\n",
      "Epoch: 1066, Train loss: 3.443, Val loss: 3.099, Epoch time = 1.655s\n",
      "Epoch: 1067, Train loss: 3.631, Val loss: 3.255, Epoch time = 1.605s\n",
      "Epoch: 1068, Train loss: 3.254, Val loss: 2.937, Epoch time = 1.568s\n",
      "Epoch: 1069, Train loss: 3.619, Val loss: 4.062, Epoch time = 1.670s\n",
      "Epoch: 1070, Train loss: 2.974, Val loss: 2.973, Epoch time = 1.715s\n",
      "Epoch: 1071, Train loss: 3.541, Val loss: 3.524, Epoch time = 1.408s\n",
      "Epoch: 1072, Train loss: 3.673, Val loss: 2.863, Epoch time = 1.686s\n",
      "Epoch: 1073, Train loss: 3.981, Val loss: 2.825, Epoch time = 1.726s\n",
      "Epoch: 1074, Train loss: 3.159, Val loss: 2.794, Epoch time = 1.521s\n",
      "Epoch: 1075, Train loss: 3.092, Val loss: 3.091, Epoch time = 1.467s\n",
      "Epoch: 1076, Train loss: 3.088, Val loss: 3.408, Epoch time = 1.534s\n",
      "Epoch: 1077, Train loss: 3.365, Val loss: 3.116, Epoch time = 1.482s\n",
      "Epoch: 1078, Train loss: 3.415, Val loss: 4.573, Epoch time = 1.449s\n",
      "Epoch: 1079, Train loss: 3.125, Val loss: 4.154, Epoch time = 1.684s\n",
      "Epoch: 1080, Train loss: 3.358, Val loss: 2.487, Epoch time = 1.688s\n",
      "Epoch: 1081, Train loss: 3.549, Val loss: 3.653, Epoch time = 1.716s\n",
      "Epoch: 1082, Train loss: 3.028, Val loss: 3.283, Epoch time = 1.550s\n",
      "Epoch: 1083, Train loss: 3.031, Val loss: 3.062, Epoch time = 1.437s\n",
      "Epoch: 1084, Train loss: 2.848, Val loss: 2.716, Epoch time = 1.448s\n",
      "Epoch: 1085, Train loss: 3.583, Val loss: 3.589, Epoch time = 1.717s\n",
      "Epoch: 1086, Train loss: 3.111, Val loss: 3.294, Epoch time = 1.381s\n",
      "Epoch: 1087, Train loss: 3.254, Val loss: 2.922, Epoch time = 1.657s\n",
      "Epoch: 1088, Train loss: 3.413, Val loss: 3.589, Epoch time = 1.423s\n",
      "Epoch: 1089, Train loss: 3.529, Val loss: 3.015, Epoch time = 1.645s\n",
      "Epoch: 1090, Train loss: 3.369, Val loss: 3.166, Epoch time = 1.334s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1091, Train loss: 3.673, Val loss: 2.940, Epoch time = 1.622s\n",
      "Epoch: 1092, Train loss: 3.033, Val loss: 3.091, Epoch time = 1.417s\n",
      "Epoch: 1093, Train loss: 3.223, Val loss: 2.781, Epoch time = 1.401s\n",
      "Epoch: 1094, Train loss: 3.444, Val loss: 3.103, Epoch time = 1.515s\n",
      "Epoch: 1095, Train loss: 3.067, Val loss: 4.379, Epoch time = 1.449s\n",
      "Epoch: 1096, Train loss: 3.522, Val loss: 3.116, Epoch time = 1.599s\n",
      "Epoch: 1097, Train loss: 3.398, Val loss: 2.922, Epoch time = 1.468s\n",
      "Epoch: 1098, Train loss: 3.527, Val loss: 2.769, Epoch time = 1.498s\n",
      "Epoch: 1099, Train loss: 3.163, Val loss: 2.797, Epoch time = 1.896s\n",
      "Epoch: 1100, Train loss: 3.080, Val loss: 2.848, Epoch time = 1.447s\n",
      "Epoch: 1101, Train loss: 3.401, Val loss: 3.165, Epoch time = 1.449s\n",
      "Epoch: 1102, Train loss: 3.740, Val loss: 3.363, Epoch time = 1.653s\n",
      "Epoch: 1103, Train loss: 3.189, Val loss: 2.992, Epoch time = 1.375s\n",
      "Epoch: 1104, Train loss: 3.327, Val loss: 3.184, Epoch time = 1.547s\n",
      "Epoch: 1105, Train loss: 3.198, Val loss: 3.390, Epoch time = 1.522s\n",
      "Epoch: 1106, Train loss: 3.862, Val loss: 3.625, Epoch time = 1.675s\n",
      "Epoch: 1107, Train loss: 3.089, Val loss: 2.968, Epoch time = 1.579s\n",
      "Epoch: 1108, Train loss: 2.942, Val loss: 2.960, Epoch time = 1.356s\n",
      "Epoch: 1109, Train loss: 3.381, Val loss: 3.032, Epoch time = 1.529s\n",
      "Epoch: 1110, Train loss: 3.619, Val loss: 3.142, Epoch time = 1.436s\n",
      "Epoch: 1111, Train loss: 3.246, Val loss: 2.501, Epoch time = 1.679s\n",
      "Epoch: 1112, Train loss: 2.920, Val loss: 2.947, Epoch time = 1.526s\n",
      "Epoch: 1113, Train loss: 3.509, Val loss: 2.972, Epoch time = 1.445s\n",
      "Epoch: 1114, Train loss: 3.646, Val loss: 3.123, Epoch time = 1.670s\n",
      "Epoch: 1115, Train loss: 3.338, Val loss: 2.884, Epoch time = 1.714s\n",
      "Epoch: 1116, Train loss: 3.317, Val loss: 3.063, Epoch time = 1.661s\n",
      "Epoch: 1117, Train loss: 3.670, Val loss: 2.966, Epoch time = 1.712s\n",
      "Epoch: 1118, Train loss: 3.382, Val loss: 3.247, Epoch time = 1.635s\n",
      "Epoch: 1119, Train loss: 3.588, Val loss: 2.889, Epoch time = 1.503s\n",
      "Epoch: 1120, Train loss: 3.623, Val loss: 3.031, Epoch time = 1.721s\n",
      "Epoch: 1121, Train loss: 3.427, Val loss: 3.043, Epoch time = 1.602s\n",
      "Epoch: 1122, Train loss: 3.470, Val loss: 2.926, Epoch time = 1.498s\n",
      "Epoch: 1123, Train loss: 3.480, Val loss: 3.183, Epoch time = 1.592s\n",
      "Epoch: 1124, Train loss: 3.162, Val loss: 3.036, Epoch time = 1.544s\n",
      "Epoch: 1125, Train loss: 3.041, Val loss: 2.640, Epoch time = 1.486s\n",
      "Epoch: 1126, Train loss: 3.215, Val loss: 2.749, Epoch time = 1.494s\n",
      "Epoch: 1127, Train loss: 3.515, Val loss: 2.902, Epoch time = 1.500s\n",
      "Epoch: 1128, Train loss: 3.502, Val loss: 3.285, Epoch time = 1.584s\n",
      "Epoch: 1129, Train loss: 3.222, Val loss: 3.132, Epoch time = 1.533s\n",
      "Epoch: 1130, Train loss: 3.375, Val loss: 2.878, Epoch time = 1.582s\n",
      "Epoch: 1131, Train loss: 3.926, Val loss: 3.141, Epoch time = 1.633s\n",
      "Epoch: 1132, Train loss: 2.990, Val loss: 2.798, Epoch time = 1.478s\n",
      "Epoch: 1133, Train loss: 3.459, Val loss: 2.978, Epoch time = 1.877s\n",
      "Epoch: 1134, Train loss: 3.597, Val loss: 3.216, Epoch time = 1.509s\n",
      "Epoch: 1135, Train loss: 3.328, Val loss: 3.071, Epoch time = 1.590s\n",
      "Epoch: 1136, Train loss: 3.535, Val loss: 3.136, Epoch time = 1.775s\n",
      "Epoch: 1137, Train loss: 3.166, Val loss: 3.855, Epoch time = 1.588s\n",
      "Epoch: 1138, Train loss: 3.927, Val loss: 2.843, Epoch time = 1.506s\n",
      "Epoch: 1139, Train loss: 3.542, Val loss: 3.020, Epoch time = 1.625s\n",
      "Epoch: 1140, Train loss: 3.210, Val loss: 3.184, Epoch time = 1.517s\n",
      "Epoch: 1141, Train loss: 3.638, Val loss: 2.576, Epoch time = 1.490s\n",
      "Epoch: 1142, Train loss: 3.068, Val loss: 2.947, Epoch time = 1.377s\n",
      "Epoch: 1143, Train loss: 3.609, Val loss: 2.898, Epoch time = 1.765s\n",
      "Epoch: 1144, Train loss: 3.215, Val loss: 2.829, Epoch time = 1.624s\n",
      "Epoch: 1145, Train loss: 3.040, Val loss: 3.143, Epoch time = 1.306s\n",
      "Epoch: 1146, Train loss: 3.468, Val loss: 3.154, Epoch time = 1.521s\n",
      "Epoch: 1147, Train loss: 3.353, Val loss: 3.034, Epoch time = 1.466s\n",
      "Epoch: 1148, Train loss: 3.385, Val loss: 2.864, Epoch time = 1.543s\n",
      "Epoch: 1149, Train loss: 3.670, Val loss: 2.969, Epoch time = 1.609s\n",
      "Epoch: 1150, Train loss: 3.775, Val loss: 3.504, Epoch time = 1.493s\n",
      "Epoch: 1151, Train loss: 3.199, Val loss: 2.665, Epoch time = 1.543s\n",
      "Epoch: 1152, Train loss: 3.689, Val loss: 3.053, Epoch time = 1.421s\n",
      "Epoch: 1153, Train loss: 3.892, Val loss: 3.137, Epoch time = 1.507s\n",
      "Epoch: 1154, Train loss: 3.479, Val loss: 3.056, Epoch time = 1.395s\n",
      "Epoch: 1155, Train loss: 3.870, Val loss: 4.196, Epoch time = 1.684s\n",
      "Epoch: 1156, Train loss: 2.909, Val loss: 2.447, Epoch time = 1.507s\n",
      "Epoch: 1157, Train loss: 3.755, Val loss: 2.614, Epoch time = 1.661s\n",
      "Epoch: 1158, Train loss: 2.834, Val loss: 3.056, Epoch time = 1.763s\n",
      "Epoch: 1159, Train loss: 3.502, Val loss: 2.700, Epoch time = 1.568s\n",
      "Epoch: 1160, Train loss: 3.033, Val loss: 2.435, Epoch time = 1.634s\n",
      "Epoch: 1161, Train loss: 3.330, Val loss: 3.146, Epoch time = 1.869s\n",
      "Epoch: 1162, Train loss: 3.939, Val loss: 2.719, Epoch time = 1.639s\n",
      "Epoch: 1163, Train loss: 3.164, Val loss: 3.051, Epoch time = 1.680s\n",
      "Epoch: 1164, Train loss: 3.114, Val loss: 2.752, Epoch time = 1.615s\n",
      "Epoch: 1165, Train loss: 3.304, Val loss: 2.941, Epoch time = 1.631s\n",
      "Epoch: 1166, Train loss: 3.881, Val loss: 2.937, Epoch time = 1.557s\n",
      "Epoch: 1167, Train loss: 3.309, Val loss: 3.059, Epoch time = 1.445s\n",
      "Epoch: 1168, Train loss: 3.997, Val loss: 3.338, Epoch time = 1.694s\n",
      "Epoch: 1169, Train loss: 3.585, Val loss: 2.918, Epoch time = 1.624s\n",
      "Epoch: 1170, Train loss: 3.386, Val loss: 3.197, Epoch time = 1.599s\n",
      "Epoch: 1171, Train loss: 3.163, Val loss: 2.406, Epoch time = 1.553s\n",
      "Epoch: 1172, Train loss: 3.424, Val loss: 2.777, Epoch time = 1.534s\n",
      "Epoch: 1173, Train loss: 4.213, Val loss: 3.043, Epoch time = 1.514s\n",
      "Epoch: 1174, Train loss: 3.362, Val loss: 2.645, Epoch time = 1.664s\n",
      "Epoch: 1175, Train loss: 3.055, Val loss: 3.175, Epoch time = 1.695s\n",
      "Epoch: 1176, Train loss: 3.970, Val loss: 2.459, Epoch time = 1.705s\n",
      "Epoch: 1177, Train loss: 3.021, Val loss: 2.829, Epoch time = 1.522s\n",
      "Epoch: 1178, Train loss: 3.635, Val loss: 3.091, Epoch time = 1.625s\n",
      "Epoch: 1179, Train loss: 3.536, Val loss: 3.038, Epoch time = 1.519s\n",
      "Epoch: 1180, Train loss: 2.936, Val loss: 2.756, Epoch time = 1.717s\n",
      "Epoch: 1181, Train loss: 3.008, Val loss: 3.524, Epoch time = 1.406s\n",
      "Epoch: 1182, Train loss: 3.179, Val loss: 3.069, Epoch time = 1.576s\n",
      "Epoch: 1183, Train loss: 3.616, Val loss: 4.052, Epoch time = 1.659s\n",
      "Epoch: 1184, Train loss: 3.317, Val loss: 2.818, Epoch time = 1.749s\n",
      "Epoch: 1185, Train loss: 3.894, Val loss: 2.904, Epoch time = 1.790s\n",
      "Epoch: 1186, Train loss: 3.079, Val loss: 3.597, Epoch time = 1.615s\n",
      "Epoch: 1187, Train loss: 3.609, Val loss: 2.792, Epoch time = 1.693s\n",
      "Epoch: 1188, Train loss: 3.412, Val loss: 2.587, Epoch time = 1.445s\n",
      "Epoch: 1189, Train loss: 3.695, Val loss: 3.114, Epoch time = 1.639s\n",
      "Epoch: 1190, Train loss: 3.578, Val loss: 2.817, Epoch time = 1.502s\n",
      "Epoch: 1191, Train loss: 3.661, Val loss: 2.968, Epoch time = 1.602s\n",
      "Epoch: 1192, Train loss: 3.953, Val loss: 2.822, Epoch time = 1.737s\n",
      "Epoch: 1193, Train loss: 3.083, Val loss: 3.103, Epoch time = 1.666s\n",
      "Epoch: 1194, Train loss: 3.134, Val loss: 3.250, Epoch time = 1.672s\n",
      "Epoch: 1195, Train loss: 3.281, Val loss: 3.367, Epoch time = 1.730s\n",
      "Epoch: 1196, Train loss: 4.003, Val loss: 3.166, Epoch time = 1.503s\n",
      "Epoch: 1197, Train loss: 3.268, Val loss: 3.692, Epoch time = 1.454s\n",
      "Epoch: 1198, Train loss: 3.533, Val loss: 2.848, Epoch time = 1.957s\n",
      "Epoch: 1199, Train loss: 3.257, Val loss: 3.075, Epoch time = 1.446s\n",
      "Epoch: 1200, Train loss: 3.564, Val loss: 2.898, Epoch time = 1.453s\n",
      "Epoch: 1201, Train loss: 3.589, Val loss: 3.274, Epoch time = 1.638s\n",
      "Epoch: 1202, Train loss: 3.063, Val loss: 3.058, Epoch time = 1.538s\n",
      "Epoch: 1203, Train loss: 3.383, Val loss: 2.828, Epoch time = 1.430s\n",
      "Epoch: 1204, Train loss: 3.116, Val loss: 3.012, Epoch time = 1.510s\n",
      "Epoch: 1205, Train loss: 3.886, Val loss: 2.329, Epoch time = 1.540s\n",
      "Epoch: 1206, Train loss: 3.514, Val loss: 3.034, Epoch time = 1.646s\n",
      "Epoch: 1207, Train loss: 3.402, Val loss: 2.929, Epoch time = 1.646s\n",
      "Epoch: 1208, Train loss: 3.793, Val loss: 2.968, Epoch time = 1.593s\n",
      "Epoch: 1209, Train loss: 3.570, Val loss: 2.975, Epoch time = 1.629s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1210, Train loss: 3.286, Val loss: 2.912, Epoch time = 1.681s\n",
      "Epoch: 1211, Train loss: 3.314, Val loss: 2.860, Epoch time = 1.655s\n",
      "Epoch: 1212, Train loss: 4.038, Val loss: 2.983, Epoch time = 1.613s\n",
      "Epoch: 1213, Train loss: 3.100, Val loss: 2.525, Epoch time = 1.546s\n",
      "Epoch: 1214, Train loss: 3.664, Val loss: 3.069, Epoch time = 1.500s\n",
      "Epoch: 1215, Train loss: 3.341, Val loss: 2.917, Epoch time = 1.593s\n",
      "Epoch: 1216, Train loss: 3.792, Val loss: 2.814, Epoch time = 1.512s\n",
      "Epoch: 1217, Train loss: 3.187, Val loss: 3.137, Epoch time = 1.513s\n",
      "Epoch: 1218, Train loss: 3.483, Val loss: 2.879, Epoch time = 1.488s\n",
      "Epoch: 1219, Train loss: 3.024, Val loss: 2.779, Epoch time = 1.610s\n",
      "Epoch: 1220, Train loss: 3.401, Val loss: 2.912, Epoch time = 1.717s\n",
      "Epoch: 1221, Train loss: 3.253, Val loss: 2.670, Epoch time = 1.445s\n",
      "Epoch: 1222, Train loss: 3.074, Val loss: 3.140, Epoch time = 1.693s\n",
      "Epoch: 1223, Train loss: 3.362, Val loss: 3.244, Epoch time = 1.768s\n",
      "Epoch: 1224, Train loss: 3.199, Val loss: 2.928, Epoch time = 1.718s\n",
      "Epoch: 1225, Train loss: 3.063, Val loss: 2.738, Epoch time = 1.650s\n",
      "Epoch: 1226, Train loss: 3.302, Val loss: 2.688, Epoch time = 1.546s\n",
      "Epoch: 1227, Train loss: 4.228, Val loss: 3.092, Epoch time = 1.438s\n",
      "Epoch: 1228, Train loss: 3.665, Val loss: 3.721, Epoch time = 1.352s\n",
      "Epoch: 1229, Train loss: 3.344, Val loss: 3.304, Epoch time = 1.724s\n",
      "Epoch: 1230, Train loss: 3.439, Val loss: 3.205, Epoch time = 1.458s\n",
      "Epoch: 1231, Train loss: 3.210, Val loss: 3.249, Epoch time = 1.456s\n",
      "Epoch: 1232, Train loss: 3.173, Val loss: 2.475, Epoch time = 1.387s\n",
      "Epoch: 1233, Train loss: 3.654, Val loss: 3.330, Epoch time = 1.429s\n",
      "Epoch: 1234, Train loss: 2.557, Val loss: 3.008, Epoch time = 1.360s\n",
      "Epoch: 1235, Train loss: 3.083, Val loss: 2.967, Epoch time = 1.589s\n",
      "Epoch: 1236, Train loss: 3.017, Val loss: 4.365, Epoch time = 1.506s\n",
      "Epoch: 1237, Train loss: 3.235, Val loss: 3.126, Epoch time = 1.614s\n",
      "Epoch: 1238, Train loss: 3.672, Val loss: 3.344, Epoch time = 1.543s\n",
      "Epoch: 1239, Train loss: 3.667, Val loss: 2.586, Epoch time = 1.649s\n",
      "Epoch: 1240, Train loss: 3.201, Val loss: 3.102, Epoch time = 1.584s\n",
      "Epoch: 1241, Train loss: 2.864, Val loss: 3.100, Epoch time = 1.488s\n",
      "Epoch: 1242, Train loss: 3.280, Val loss: 3.073, Epoch time = 1.606s\n",
      "Epoch: 1243, Train loss: 3.287, Val loss: 2.929, Epoch time = 1.398s\n",
      "Epoch: 1244, Train loss: 3.696, Val loss: 3.448, Epoch time = 1.594s\n",
      "Epoch: 1245, Train loss: 3.577, Val loss: 2.678, Epoch time = 1.682s\n",
      "Epoch: 1246, Train loss: 3.145, Val loss: 2.988, Epoch time = 1.729s\n",
      "Epoch: 1247, Train loss: 3.644, Val loss: 3.045, Epoch time = 1.680s\n",
      "Epoch: 1248, Train loss: 3.645, Val loss: 3.429, Epoch time = 1.383s\n",
      "Epoch: 1249, Train loss: 3.311, Val loss: 2.745, Epoch time = 1.647s\n",
      "Epoch: 1250, Train loss: 3.228, Val loss: 3.082, Epoch time = 1.593s\n",
      "Epoch: 1251, Train loss: 3.048, Val loss: 3.067, Epoch time = 1.702s\n",
      "Epoch: 1252, Train loss: 3.833, Val loss: 2.785, Epoch time = 1.565s\n",
      "Epoch: 1253, Train loss: 2.983, Val loss: 3.207, Epoch time = 1.554s\n",
      "Epoch: 1254, Train loss: 3.717, Val loss: 2.839, Epoch time = 1.732s\n",
      "Epoch: 1255, Train loss: 3.264, Val loss: 3.178, Epoch time = 1.554s\n",
      "Epoch: 1256, Train loss: 3.549, Val loss: 4.287, Epoch time = 1.787s\n",
      "Epoch: 1257, Train loss: 3.340, Val loss: 3.166, Epoch time = 1.684s\n",
      "Epoch: 1258, Train loss: 3.223, Val loss: 2.912, Epoch time = 1.442s\n",
      "Epoch: 1259, Train loss: 4.459, Val loss: 3.330, Epoch time = 1.420s\n",
      "Epoch: 1260, Train loss: 3.602, Val loss: 3.317, Epoch time = 1.446s\n",
      "Epoch: 1261, Train loss: 3.185, Val loss: 2.595, Epoch time = 1.619s\n",
      "Epoch: 1262, Train loss: 3.023, Val loss: 2.723, Epoch time = 1.415s\n",
      "Epoch: 1263, Train loss: 3.362, Val loss: 3.108, Epoch time = 1.722s\n",
      "Epoch: 1264, Train loss: 3.708, Val loss: 2.898, Epoch time = 1.547s\n",
      "Epoch: 1265, Train loss: 3.163, Val loss: 3.245, Epoch time = 1.523s\n",
      "Epoch: 1266, Train loss: 3.480, Val loss: 3.108, Epoch time = 1.351s\n",
      "Epoch: 1267, Train loss: 3.867, Val loss: 3.376, Epoch time = 1.587s\n",
      "Epoch: 1268, Train loss: 4.012, Val loss: 2.773, Epoch time = 1.658s\n",
      "Epoch: 1269, Train loss: 4.236, Val loss: 2.695, Epoch time = 1.684s\n",
      "Epoch: 1270, Train loss: 3.386, Val loss: 3.101, Epoch time = 1.599s\n",
      "Epoch: 1271, Train loss: 3.476, Val loss: 2.716, Epoch time = 1.592s\n",
      "Epoch: 1272, Train loss: 3.655, Val loss: 2.859, Epoch time = 1.426s\n",
      "Epoch: 1273, Train loss: 3.600, Val loss: 3.291, Epoch time = 1.489s\n",
      "Epoch: 1274, Train loss: 3.266, Val loss: 3.057, Epoch time = 1.610s\n",
      "Epoch: 1275, Train loss: 2.930, Val loss: 3.322, Epoch time = 1.321s\n",
      "Epoch: 1276, Train loss: 3.051, Val loss: 2.822, Epoch time = 1.547s\n",
      "Epoch: 1277, Train loss: 3.329, Val loss: 3.274, Epoch time = 1.555s\n",
      "Epoch: 1278, Train loss: 3.839, Val loss: 2.640, Epoch time = 1.673s\n",
      "Epoch: 1279, Train loss: 3.389, Val loss: 3.014, Epoch time = 1.544s\n",
      "Epoch: 1280, Train loss: 3.859, Val loss: 2.969, Epoch time = 1.636s\n",
      "Epoch: 1281, Train loss: 3.360, Val loss: 3.370, Epoch time = 1.402s\n",
      "Epoch: 1282, Train loss: 3.419, Val loss: 3.042, Epoch time = 1.659s\n",
      "Epoch: 1283, Train loss: 3.529, Val loss: 2.656, Epoch time = 1.561s\n",
      "Epoch: 1284, Train loss: 2.934, Val loss: 2.729, Epoch time = 1.570s\n",
      "Epoch: 1285, Train loss: 3.949, Val loss: 3.125, Epoch time = 1.662s\n",
      "Epoch: 1286, Train loss: 3.101, Val loss: 2.949, Epoch time = 1.594s\n",
      "Epoch: 1287, Train loss: 3.014, Val loss: 3.343, Epoch time = 1.370s\n",
      "Epoch: 1288, Train loss: 3.572, Val loss: 2.763, Epoch time = 1.846s\n",
      "Epoch: 1289, Train loss: 3.286, Val loss: 2.658, Epoch time = 1.486s\n",
      "Epoch: 1290, Train loss: 3.041, Val loss: 3.285, Epoch time = 1.477s\n",
      "Epoch: 1291, Train loss: 3.490, Val loss: 2.994, Epoch time = 1.552s\n",
      "Epoch: 1292, Train loss: 3.380, Val loss: 2.643, Epoch time = 1.580s\n",
      "Epoch: 1293, Train loss: 3.647, Val loss: 3.005, Epoch time = 1.687s\n",
      "Epoch: 1294, Train loss: 3.602, Val loss: 3.804, Epoch time = 1.730s\n",
      "Epoch: 1295, Train loss: 3.200, Val loss: 3.050, Epoch time = 1.513s\n",
      "Epoch: 1296, Train loss: 2.939, Val loss: 2.848, Epoch time = 1.544s\n",
      "Epoch: 1297, Train loss: 3.691, Val loss: 4.368, Epoch time = 1.528s\n",
      "Epoch: 1298, Train loss: 3.035, Val loss: 2.961, Epoch time = 1.350s\n",
      "Epoch: 1299, Train loss: 3.276, Val loss: 2.881, Epoch time = 1.546s\n",
      "Epoch: 1300, Train loss: 3.653, Val loss: 2.824, Epoch time = 1.570s\n",
      "Epoch: 1301, Train loss: 3.883, Val loss: 3.248, Epoch time = 1.472s\n",
      "Epoch: 1302, Train loss: 3.884, Val loss: 2.899, Epoch time = 1.706s\n",
      "Epoch: 1303, Train loss: 3.726, Val loss: 2.589, Epoch time = 1.748s\n",
      "Epoch: 1304, Train loss: 3.498, Val loss: 2.917, Epoch time = 1.660s\n",
      "Epoch: 1305, Train loss: 3.920, Val loss: 2.518, Epoch time = 1.492s\n",
      "Epoch: 1306, Train loss: 2.998, Val loss: 3.197, Epoch time = 1.396s\n",
      "Epoch: 1307, Train loss: 3.766, Val loss: 3.133, Epoch time = 1.608s\n",
      "Epoch: 1308, Train loss: 2.817, Val loss: 2.879, Epoch time = 1.397s\n",
      "Epoch: 1309, Train loss: 3.425, Val loss: 3.178, Epoch time = 1.439s\n",
      "Epoch: 1310, Train loss: 3.460, Val loss: 2.959, Epoch time = 1.704s\n",
      "Epoch: 1311, Train loss: 3.509, Val loss: 3.429, Epoch time = 1.469s\n",
      "Epoch: 1312, Train loss: 3.307, Val loss: 3.062, Epoch time = 1.436s\n",
      "Epoch: 1313, Train loss: 3.751, Val loss: 2.971, Epoch time = 1.678s\n",
      "Epoch: 1314, Train loss: 3.861, Val loss: 2.896, Epoch time = 1.759s\n",
      "Epoch: 1315, Train loss: 3.073, Val loss: 3.018, Epoch time = 1.791s\n",
      "Epoch: 1316, Train loss: 3.008, Val loss: 3.162, Epoch time = 1.462s\n",
      "Epoch: 1317, Train loss: 3.416, Val loss: 2.730, Epoch time = 1.488s\n",
      "Epoch: 1318, Train loss: 4.416, Val loss: 2.862, Epoch time = 1.670s\n",
      "Epoch: 1319, Train loss: 3.359, Val loss: 3.119, Epoch time = 1.590s\n",
      "Epoch: 1320, Train loss: 4.307, Val loss: 3.267, Epoch time = 1.317s\n",
      "Epoch: 1321, Train loss: 3.143, Val loss: 3.449, Epoch time = 1.529s\n",
      "Epoch: 1322, Train loss: 3.294, Val loss: 3.091, Epoch time = 1.520s\n",
      "Epoch: 1323, Train loss: 3.255, Val loss: 3.298, Epoch time = 1.847s\n",
      "Epoch: 1324, Train loss: 3.394, Val loss: 2.839, Epoch time = 1.618s\n",
      "Epoch: 1325, Train loss: 3.401, Val loss: 2.663, Epoch time = 1.437s\n",
      "Epoch: 1326, Train loss: 2.676, Val loss: 2.879, Epoch time = 1.435s\n",
      "Epoch: 1327, Train loss: 2.967, Val loss: 3.145, Epoch time = 1.878s\n",
      "Epoch: 1328, Train loss: 3.802, Val loss: 2.965, Epoch time = 1.702s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1329, Train loss: 3.553, Val loss: 3.024, Epoch time = 1.631s\n",
      "Epoch: 1330, Train loss: 3.758, Val loss: 3.325, Epoch time = 1.469s\n",
      "Epoch: 1331, Train loss: 3.786, Val loss: 3.283, Epoch time = 1.371s\n",
      "Epoch: 1332, Train loss: 3.284, Val loss: 2.972, Epoch time = 1.559s\n",
      "Epoch: 1333, Train loss: 3.047, Val loss: 2.733, Epoch time = 1.386s\n",
      "Epoch: 1334, Train loss: 2.901, Val loss: 3.065, Epoch time = 1.694s\n",
      "Epoch: 1335, Train loss: 3.155, Val loss: 3.309, Epoch time = 1.711s\n",
      "Epoch: 1336, Train loss: 3.326, Val loss: 2.942, Epoch time = 1.518s\n",
      "Epoch: 1337, Train loss: 3.816, Val loss: 2.997, Epoch time = 1.539s\n",
      "Epoch: 1338, Train loss: 3.507, Val loss: 3.064, Epoch time = 1.699s\n",
      "Epoch: 1339, Train loss: 3.322, Val loss: 2.741, Epoch time = 1.641s\n",
      "Epoch: 1340, Train loss: 3.366, Val loss: 4.375, Epoch time = 1.878s\n",
      "Epoch: 1341, Train loss: 3.845, Val loss: 2.780, Epoch time = 1.577s\n",
      "Epoch: 1342, Train loss: 3.925, Val loss: 4.369, Epoch time = 1.677s\n",
      "Epoch: 1343, Train loss: 3.064, Val loss: 2.902, Epoch time = 1.726s\n",
      "Epoch: 1344, Train loss: 3.436, Val loss: 3.299, Epoch time = 1.804s\n",
      "Epoch: 1345, Train loss: 4.073, Val loss: 3.099, Epoch time = 1.537s\n",
      "Epoch: 1346, Train loss: 3.036, Val loss: 3.229, Epoch time = 1.570s\n",
      "Epoch: 1347, Train loss: 3.404, Val loss: 2.940, Epoch time = 1.437s\n",
      "Epoch: 1348, Train loss: 3.370, Val loss: 3.165, Epoch time = 1.541s\n",
      "Epoch: 1349, Train loss: 3.222, Val loss: 3.056, Epoch time = 1.648s\n",
      "Epoch: 1350, Train loss: 3.285, Val loss: 2.850, Epoch time = 1.629s\n",
      "Epoch: 1351, Train loss: 3.348, Val loss: 2.923, Epoch time = 1.580s\n",
      "Epoch: 1352, Train loss: 2.660, Val loss: 3.031, Epoch time = 1.577s\n",
      "Epoch: 1353, Train loss: 3.147, Val loss: 3.155, Epoch time = 1.369s\n",
      "Epoch: 1354, Train loss: 3.239, Val loss: 2.559, Epoch time = 1.654s\n",
      "Epoch: 1355, Train loss: 3.382, Val loss: 2.502, Epoch time = 1.599s\n",
      "Epoch: 1356, Train loss: 3.735, Val loss: 4.305, Epoch time = 1.633s\n",
      "Epoch: 1357, Train loss: 3.368, Val loss: 3.098, Epoch time = 1.681s\n",
      "Epoch: 1358, Train loss: 3.752, Val loss: 3.193, Epoch time = 1.450s\n",
      "Epoch: 1359, Train loss: 3.276, Val loss: 2.877, Epoch time = 1.741s\n",
      "Epoch: 1360, Train loss: 3.397, Val loss: 2.847, Epoch time = 1.765s\n",
      "Epoch: 1361, Train loss: 3.049, Val loss: 3.704, Epoch time = 1.794s\n",
      "Epoch: 1362, Train loss: 2.987, Val loss: 3.096, Epoch time = 1.760s\n",
      "Epoch: 1363, Train loss: 3.270, Val loss: 3.018, Epoch time = 1.784s\n",
      "Epoch: 1364, Train loss: 3.439, Val loss: 2.604, Epoch time = 1.933s\n",
      "Epoch: 1365, Train loss: 3.195, Val loss: 3.048, Epoch time = 1.659s\n",
      "Epoch: 1366, Train loss: 3.196, Val loss: 3.090, Epoch time = 1.671s\n",
      "Epoch: 1367, Train loss: 3.496, Val loss: 3.186, Epoch time = 1.798s\n",
      "Epoch: 1368, Train loss: 5.050, Val loss: 2.955, Epoch time = 1.315s\n",
      "Epoch: 1369, Train loss: 3.627, Val loss: 2.575, Epoch time = 1.784s\n",
      "Epoch: 1370, Train loss: 3.813, Val loss: 2.915, Epoch time = 1.719s\n",
      "Epoch: 1371, Train loss: 3.113, Val loss: 3.153, Epoch time = 1.738s\n",
      "Epoch: 1372, Train loss: 3.102, Val loss: 3.282, Epoch time = 1.533s\n",
      "Epoch: 1373, Train loss: 3.445, Val loss: 2.713, Epoch time = 1.754s\n",
      "Epoch: 1374, Train loss: 3.467, Val loss: 2.701, Epoch time = 1.804s\n",
      "Epoch: 1375, Train loss: 3.473, Val loss: 2.793, Epoch time = 1.756s\n",
      "Epoch: 1376, Train loss: 3.221, Val loss: 3.190, Epoch time = 1.673s\n",
      "Epoch: 1377, Train loss: 2.744, Val loss: 3.278, Epoch time = 1.503s\n",
      "Epoch: 1378, Train loss: 2.842, Val loss: 2.966, Epoch time = 1.681s\n",
      "Epoch: 1379, Train loss: 3.319, Val loss: 2.795, Epoch time = 1.756s\n",
      "Epoch: 1380, Train loss: 3.084, Val loss: 2.792, Epoch time = 1.765s\n",
      "Epoch: 1381, Train loss: 3.376, Val loss: 3.299, Epoch time = 1.738s\n",
      "Epoch: 1382, Train loss: 3.450, Val loss: 2.775, Epoch time = 1.695s\n",
      "Epoch: 1383, Train loss: 3.877, Val loss: 2.823, Epoch time = 1.705s\n",
      "Epoch: 1384, Train loss: 3.256, Val loss: 3.131, Epoch time = 1.547s\n",
      "Epoch: 1385, Train loss: 3.166, Val loss: 3.103, Epoch time = 1.559s\n",
      "Epoch: 1386, Train loss: 3.516, Val loss: 2.823, Epoch time = 1.970s\n",
      "Epoch: 1387, Train loss: 3.075, Val loss: 3.138, Epoch time = 1.905s\n",
      "Epoch: 1388, Train loss: 3.280, Val loss: 2.937, Epoch time = 1.581s\n",
      "Epoch: 1389, Train loss: 3.142, Val loss: 2.943, Epoch time = 1.568s\n",
      "Epoch: 1390, Train loss: 3.175, Val loss: 2.746, Epoch time = 1.457s\n",
      "Epoch: 1391, Train loss: 3.447, Val loss: 2.871, Epoch time = 1.755s\n",
      "Epoch: 1392, Train loss: 2.917, Val loss: 2.683, Epoch time = 1.593s\n",
      "Epoch: 1393, Train loss: 3.190, Val loss: 2.982, Epoch time = 1.710s\n",
      "Epoch: 1394, Train loss: 3.355, Val loss: 3.044, Epoch time = 1.712s\n",
      "Epoch: 1395, Train loss: 2.984, Val loss: 2.660, Epoch time = 1.597s\n",
      "Epoch: 1396, Train loss: 3.225, Val loss: 3.373, Epoch time = 1.644s\n",
      "Epoch: 1397, Train loss: 3.649, Val loss: 2.831, Epoch time = 1.546s\n",
      "Epoch: 1398, Train loss: 3.242, Val loss: 3.093, Epoch time = 1.642s\n",
      "Epoch: 1399, Train loss: 3.572, Val loss: 3.177, Epoch time = 1.660s\n",
      "Epoch: 1400, Train loss: 3.605, Val loss: 3.013, Epoch time = 1.692s\n",
      "Epoch: 1401, Train loss: 2.738, Val loss: 3.160, Epoch time = 1.747s\n",
      "Epoch: 1402, Train loss: 2.966, Val loss: 3.062, Epoch time = 1.597s\n",
      "Epoch: 1403, Train loss: 3.245, Val loss: 3.100, Epoch time = 1.709s\n",
      "Epoch: 1404, Train loss: 2.683, Val loss: 2.814, Epoch time = 1.477s\n",
      "Epoch: 1405, Train loss: 3.166, Val loss: 3.000, Epoch time = 1.647s\n",
      "Epoch: 1406, Train loss: 3.705, Val loss: 3.170, Epoch time = 1.811s\n",
      "Epoch: 1407, Train loss: 3.215, Val loss: 2.746, Epoch time = 1.409s\n",
      "Epoch: 1408, Train loss: 3.637, Val loss: 2.927, Epoch time = 1.634s\n",
      "Epoch: 1409, Train loss: 3.579, Val loss: 3.207, Epoch time = 1.665s\n",
      "Epoch: 1410, Train loss: 3.490, Val loss: 3.108, Epoch time = 1.422s\n",
      "Epoch: 1411, Train loss: 3.285, Val loss: 2.717, Epoch time = 1.613s\n",
      "Epoch: 1412, Train loss: 3.116, Val loss: 2.960, Epoch time = 1.647s\n",
      "Epoch: 1413, Train loss: 2.926, Val loss: 2.739, Epoch time = 1.719s\n",
      "Epoch: 1414, Train loss: 3.409, Val loss: 2.975, Epoch time = 1.689s\n",
      "Epoch: 1415, Train loss: 2.803, Val loss: 3.071, Epoch time = 1.619s\n",
      "Epoch: 1416, Train loss: 3.365, Val loss: 2.596, Epoch time = 1.659s\n",
      "Epoch: 1417, Train loss: 3.173, Val loss: 3.225, Epoch time = 1.729s\n",
      "Epoch: 1418, Train loss: 3.501, Val loss: 2.816, Epoch time = 1.768s\n",
      "Epoch: 1419, Train loss: 3.471, Val loss: 3.035, Epoch time = 1.772s\n",
      "Epoch: 1420, Train loss: 3.098, Val loss: 2.689, Epoch time = 1.628s\n",
      "Epoch: 1421, Train loss: 3.193, Val loss: 3.136, Epoch time = 1.651s\n",
      "Epoch: 1422, Train loss: 3.248, Val loss: 2.733, Epoch time = 1.860s\n",
      "Epoch: 1423, Train loss: 3.426, Val loss: 2.747, Epoch time = 1.857s\n",
      "Epoch: 1424, Train loss: 3.211, Val loss: 3.649, Epoch time = 1.634s\n",
      "Epoch: 1425, Train loss: 3.489, Val loss: 3.366, Epoch time = 1.850s\n",
      "Epoch: 1426, Train loss: 3.216, Val loss: 2.960, Epoch time = 1.810s\n",
      "Epoch: 1427, Train loss: 2.971, Val loss: 2.781, Epoch time = 1.524s\n",
      "Epoch: 1428, Train loss: 2.851, Val loss: 2.652, Epoch time = 1.657s\n",
      "Epoch: 1429, Train loss: 3.367, Val loss: 2.466, Epoch time = 1.553s\n",
      "Epoch: 1430, Train loss: 4.202, Val loss: 2.697, Epoch time = 1.744s\n",
      "Epoch: 1431, Train loss: 3.238, Val loss: 3.637, Epoch time = 1.810s\n",
      "Epoch: 1432, Train loss: 3.409, Val loss: 2.607, Epoch time = 1.548s\n",
      "Epoch: 1433, Train loss: 3.318, Val loss: 3.187, Epoch time = 1.516s\n",
      "Epoch: 1434, Train loss: 3.502, Val loss: 2.649, Epoch time = 1.742s\n",
      "Epoch: 1435, Train loss: 3.012, Val loss: 2.990, Epoch time = 1.499s\n",
      "Epoch: 1436, Train loss: 3.306, Val loss: 4.442, Epoch time = 1.559s\n",
      "Epoch: 1437, Train loss: 3.274, Val loss: 2.768, Epoch time = 1.672s\n",
      "Epoch: 1438, Train loss: 2.909, Val loss: 2.821, Epoch time = 1.675s\n",
      "Epoch: 1439, Train loss: 2.675, Val loss: 3.053, Epoch time = 1.713s\n",
      "Epoch: 1440, Train loss: 2.897, Val loss: 2.956, Epoch time = 1.684s\n",
      "Epoch: 1441, Train loss: 3.352, Val loss: 2.960, Epoch time = 1.739s\n",
      "Epoch: 1442, Train loss: 3.480, Val loss: 2.739, Epoch time = 1.588s\n",
      "Epoch: 1443, Train loss: 3.352, Val loss: 2.893, Epoch time = 1.629s\n",
      "Epoch: 1444, Train loss: 3.137, Val loss: 2.350, Epoch time = 1.847s\n",
      "Epoch: 1445, Train loss: 2.855, Val loss: 3.180, Epoch time = 1.749s\n",
      "Epoch: 1446, Train loss: 3.296, Val loss: 4.069, Epoch time = 1.766s\n",
      "Epoch: 1447, Train loss: 3.357, Val loss: 2.849, Epoch time = 1.749s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1448, Train loss: 3.016, Val loss: 2.976, Epoch time = 1.667s\n",
      "Epoch: 1449, Train loss: 2.857, Val loss: 2.761, Epoch time = 1.581s\n",
      "Epoch: 1450, Train loss: 3.031, Val loss: 3.175, Epoch time = 1.733s\n",
      "Epoch: 1451, Train loss: 2.857, Val loss: 2.760, Epoch time = 1.643s\n",
      "Epoch: 1452, Train loss: 3.513, Val loss: 2.985, Epoch time = 1.561s\n",
      "Epoch: 1453, Train loss: 2.851, Val loss: 3.003, Epoch time = 1.533s\n",
      "Epoch: 1454, Train loss: 3.330, Val loss: 2.978, Epoch time = 1.488s\n",
      "Epoch: 1455, Train loss: 3.136, Val loss: 2.811, Epoch time = 1.666s\n",
      "Epoch: 1456, Train loss: 3.199, Val loss: 2.796, Epoch time = 1.877s\n",
      "Epoch: 1457, Train loss: 3.334, Val loss: 3.045, Epoch time = 1.632s\n",
      "Epoch: 1458, Train loss: 3.353, Val loss: 3.862, Epoch time = 1.750s\n",
      "Epoch: 1459, Train loss: 3.721, Val loss: 2.782, Epoch time = 1.788s\n",
      "Epoch: 1460, Train loss: 2.965, Val loss: 2.714, Epoch time = 1.394s\n",
      "Epoch: 1461, Train loss: 3.549, Val loss: 3.254, Epoch time = 1.623s\n",
      "Epoch: 1462, Train loss: 3.784, Val loss: 2.854, Epoch time = 1.496s\n",
      "Epoch: 1463, Train loss: 3.226, Val loss: 2.801, Epoch time = 1.818s\n",
      "Epoch: 1464, Train loss: 3.405, Val loss: 2.474, Epoch time = 1.720s\n",
      "Epoch: 1465, Train loss: 3.476, Val loss: 3.043, Epoch time = 1.515s\n",
      "Epoch: 1466, Train loss: 3.211, Val loss: 2.923, Epoch time = 1.605s\n",
      "Epoch: 1467, Train loss: 3.741, Val loss: 3.103, Epoch time = 1.889s\n",
      "Epoch: 1468, Train loss: 3.144, Val loss: 3.091, Epoch time = 1.551s\n",
      "Epoch: 1469, Train loss: 3.375, Val loss: 2.911, Epoch time = 1.722s\n",
      "Epoch: 1470, Train loss: 3.414, Val loss: 2.677, Epoch time = 1.454s\n",
      "Epoch: 1471, Train loss: 3.339, Val loss: 3.057, Epoch time = 1.716s\n",
      "Epoch: 1472, Train loss: 3.272, Val loss: 2.865, Epoch time = 1.755s\n",
      "Epoch: 1473, Train loss: 3.285, Val loss: 2.852, Epoch time = 1.718s\n",
      "Epoch: 1474, Train loss: 3.606, Val loss: 2.920, Epoch time = 1.602s\n",
      "Epoch: 1475, Train loss: 2.961, Val loss: 2.976, Epoch time = 1.633s\n",
      "Epoch: 1476, Train loss: 3.454, Val loss: 3.046, Epoch time = 1.681s\n",
      "Epoch: 1477, Train loss: 3.500, Val loss: 2.647, Epoch time = 1.737s\n",
      "Epoch: 1478, Train loss: 3.504, Val loss: 2.900, Epoch time = 1.585s\n",
      "Epoch: 1479, Train loss: 3.240, Val loss: 2.919, Epoch time = 1.804s\n",
      "Epoch: 1480, Train loss: 3.521, Val loss: 3.451, Epoch time = 1.514s\n",
      "Epoch: 1481, Train loss: 3.738, Val loss: 3.262, Epoch time = 1.600s\n",
      "Epoch: 1482, Train loss: 3.900, Val loss: 2.910, Epoch time = 1.436s\n",
      "Epoch: 1483, Train loss: 3.596, Val loss: 2.765, Epoch time = 1.834s\n",
      "Epoch: 1484, Train loss: 3.003, Val loss: 2.648, Epoch time = 1.648s\n",
      "Epoch: 1485, Train loss: 3.119, Val loss: 3.009, Epoch time = 1.713s\n",
      "Epoch: 1486, Train loss: 3.172, Val loss: 3.093, Epoch time = 1.735s\n",
      "Epoch: 1487, Train loss: 3.754, Val loss: 2.740, Epoch time = 1.668s\n",
      "Epoch: 1488, Train loss: 3.385, Val loss: 4.519, Epoch time = 1.641s\n",
      "Epoch: 1489, Train loss: 3.097, Val loss: 2.864, Epoch time = 1.648s\n",
      "Epoch: 1490, Train loss: 2.847, Val loss: 2.650, Epoch time = 1.445s\n",
      "Epoch: 1491, Train loss: 3.302, Val loss: 2.966, Epoch time = 1.790s\n",
      "Epoch: 1492, Train loss: 3.749, Val loss: 2.795, Epoch time = 1.925s\n",
      "Epoch: 1493, Train loss: 3.505, Val loss: 4.599, Epoch time = 1.689s\n",
      "Epoch: 1494, Train loss: 3.266, Val loss: 2.870, Epoch time = 1.550s\n",
      "Epoch: 1495, Train loss: 3.156, Val loss: 2.677, Epoch time = 1.642s\n",
      "Epoch: 1496, Train loss: 2.837, Val loss: 2.839, Epoch time = 1.632s\n",
      "Epoch: 1497, Train loss: 3.538, Val loss: 2.848, Epoch time = 1.642s\n",
      "Epoch: 1498, Train loss: 3.280, Val loss: 2.547, Epoch time = 1.616s\n",
      "Epoch: 1499, Train loss: 4.061, Val loss: 3.046, Epoch time = 1.756s\n",
      "Epoch: 1500, Train loss: 2.984, Val loss: 2.652, Epoch time = 1.603s\n",
      "Epoch: 1501, Train loss: 3.118, Val loss: 2.802, Epoch time = 1.777s\n",
      "Epoch: 1502, Train loss: 3.301, Val loss: 3.049, Epoch time = 1.648s\n",
      "Epoch: 1503, Train loss: 3.777, Val loss: 2.858, Epoch time = 1.670s\n",
      "Epoch: 1504, Train loss: 3.463, Val loss: 3.646, Epoch time = 1.586s\n",
      "Epoch: 1505, Train loss: 3.051, Val loss: 2.564, Epoch time = 1.635s\n",
      "Epoch: 1506, Train loss: 3.641, Val loss: 3.120, Epoch time = 1.694s\n",
      "Epoch: 1507, Train loss: 3.026, Val loss: 2.904, Epoch time = 1.857s\n",
      "Epoch: 1508, Train loss: 3.277, Val loss: 2.940, Epoch time = 1.706s\n",
      "Epoch: 1509, Train loss: 3.361, Val loss: 2.766, Epoch time = 1.590s\n",
      "Epoch: 1510, Train loss: 3.379, Val loss: 2.867, Epoch time = 1.883s\n",
      "Epoch: 1511, Train loss: 2.887, Val loss: 2.887, Epoch time = 1.689s\n",
      "Epoch: 1512, Train loss: 3.905, Val loss: 2.722, Epoch time = 1.625s\n",
      "Epoch: 1513, Train loss: 3.182, Val loss: 2.914, Epoch time = 1.562s\n",
      "Epoch: 1514, Train loss: 3.654, Val loss: 2.811, Epoch time = 1.683s\n",
      "Epoch: 1515, Train loss: 2.929, Val loss: 3.087, Epoch time = 1.779s\n",
      "Epoch: 1516, Train loss: 3.308, Val loss: 2.858, Epoch time = 1.562s\n",
      "Epoch: 1517, Train loss: 3.450, Val loss: 2.770, Epoch time = 1.560s\n",
      "Epoch: 1518, Train loss: 2.976, Val loss: 2.851, Epoch time = 1.480s\n",
      "Epoch: 1519, Train loss: 3.449, Val loss: 5.402, Epoch time = 1.753s\n",
      "Epoch: 1520, Train loss: 2.985, Val loss: 3.268, Epoch time = 1.700s\n",
      "Epoch: 1521, Train loss: 3.618, Val loss: 2.802, Epoch time = 1.672s\n",
      "Epoch: 1522, Train loss: 3.088, Val loss: 2.972, Epoch time = 1.625s\n",
      "Epoch: 1523, Train loss: 4.015, Val loss: 3.209, Epoch time = 1.507s\n",
      "Epoch: 1524, Train loss: 3.030, Val loss: 2.740, Epoch time = 1.687s\n",
      "Epoch: 1525, Train loss: 3.580, Val loss: 2.708, Epoch time = 1.696s\n",
      "Epoch: 1526, Train loss: 3.934, Val loss: 2.731, Epoch time = 1.741s\n",
      "Epoch: 1527, Train loss: 3.624, Val loss: 2.808, Epoch time = 1.722s\n",
      "Epoch: 1528, Train loss: 2.860, Val loss: 3.147, Epoch time = 1.581s\n",
      "Epoch: 1529, Train loss: 3.778, Val loss: 3.294, Epoch time = 1.843s\n",
      "Epoch: 1530, Train loss: 3.326, Val loss: 2.805, Epoch time = 1.674s\n",
      "Epoch: 1531, Train loss: 3.040, Val loss: 2.556, Epoch time = 1.820s\n",
      "Epoch: 1532, Train loss: 3.129, Val loss: 2.566, Epoch time = 1.623s\n",
      "Epoch: 1533, Train loss: 3.506, Val loss: 3.270, Epoch time = 1.585s\n",
      "Epoch: 1534, Train loss: 3.264, Val loss: 2.914, Epoch time = 1.674s\n",
      "Epoch: 1535, Train loss: 2.920, Val loss: 3.078, Epoch time = 1.840s\n",
      "Epoch: 1536, Train loss: 3.022, Val loss: 2.849, Epoch time = 1.660s\n",
      "Epoch: 1537, Train loss: 3.016, Val loss: 3.484, Epoch time = 1.635s\n",
      "Epoch: 1538, Train loss: 3.148, Val loss: 2.942, Epoch time = 1.460s\n",
      "Epoch: 1539, Train loss: 2.861, Val loss: 3.785, Epoch time = 1.392s\n",
      "Epoch: 1540, Train loss: 3.198, Val loss: 2.906, Epoch time = 1.684s\n",
      "Epoch: 1541, Train loss: 2.972, Val loss: 2.797, Epoch time = 1.510s\n",
      "Epoch: 1542, Train loss: 2.948, Val loss: 2.854, Epoch time = 1.687s\n",
      "Epoch: 1543, Train loss: 3.145, Val loss: 2.526, Epoch time = 1.839s\n",
      "Epoch: 1544, Train loss: 3.383, Val loss: 2.915, Epoch time = 1.810s\n",
      "Epoch: 1545, Train loss: 2.676, Val loss: 3.198, Epoch time = 1.592s\n",
      "Epoch: 1546, Train loss: 3.189, Val loss: 3.024, Epoch time = 1.843s\n",
      "Epoch: 1547, Train loss: 3.637, Val loss: 2.787, Epoch time = 1.452s\n",
      "Epoch: 1548, Train loss: 3.604, Val loss: 3.030, Epoch time = 1.695s\n",
      "Epoch: 1549, Train loss: 3.423, Val loss: 3.263, Epoch time = 1.685s\n",
      "Epoch: 1550, Train loss: 2.760, Val loss: 2.973, Epoch time = 1.692s\n",
      "Epoch: 1551, Train loss: 3.257, Val loss: 2.557, Epoch time = 1.695s\n",
      "Epoch: 1552, Train loss: 2.924, Val loss: 2.822, Epoch time = 1.520s\n",
      "Epoch: 1553, Train loss: 3.454, Val loss: 3.575, Epoch time = 1.811s\n",
      "Epoch: 1554, Train loss: 2.805, Val loss: 2.895, Epoch time = 1.558s\n",
      "Epoch: 1555, Train loss: 4.367, Val loss: 2.917, Epoch time = 1.645s\n",
      "Epoch: 1556, Train loss: 3.358, Val loss: 3.583, Epoch time = 1.553s\n",
      "Epoch: 1557, Train loss: 3.125, Val loss: 3.035, Epoch time = 1.636s\n",
      "Epoch: 1558, Train loss: 2.712, Val loss: 2.548, Epoch time = 1.562s\n",
      "Epoch: 1559, Train loss: 3.011, Val loss: 2.498, Epoch time = 1.702s\n",
      "Epoch: 1560, Train loss: 3.578, Val loss: 2.550, Epoch time = 1.763s\n",
      "Epoch: 1561, Train loss: 3.639, Val loss: 2.731, Epoch time = 1.739s\n",
      "Epoch: 1562, Train loss: 3.284, Val loss: 2.897, Epoch time = 1.599s\n",
      "Epoch: 1563, Train loss: 3.358, Val loss: 2.954, Epoch time = 1.575s\n",
      "Epoch: 1564, Train loss: 3.187, Val loss: 2.915, Epoch time = 1.649s\n",
      "Epoch: 1565, Train loss: 3.402, Val loss: 2.855, Epoch time = 1.820s\n",
      "Epoch: 1566, Train loss: 2.680, Val loss: 3.364, Epoch time = 1.461s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1567, Train loss: 3.670, Val loss: 2.854, Epoch time = 1.553s\n",
      "Epoch: 1568, Train loss: 3.327, Val loss: 2.998, Epoch time = 1.638s\n",
      "Epoch: 1569, Train loss: 2.978, Val loss: 2.750, Epoch time = 1.483s\n",
      "Epoch: 1570, Train loss: 3.036, Val loss: 3.236, Epoch time = 1.724s\n",
      "Epoch: 1571, Train loss: 3.470, Val loss: 2.828, Epoch time = 1.860s\n",
      "Epoch: 1572, Train loss: 2.963, Val loss: 2.929, Epoch time = 1.589s\n",
      "Epoch: 1573, Train loss: 3.908, Val loss: 2.973, Epoch time = 1.642s\n",
      "Epoch: 1574, Train loss: 3.105, Val loss: 2.878, Epoch time = 1.642s\n",
      "Epoch: 1575, Train loss: 3.172, Val loss: 2.862, Epoch time = 1.579s\n",
      "Epoch: 1576, Train loss: 3.356, Val loss: 2.961, Epoch time = 1.782s\n",
      "Epoch: 1577, Train loss: 3.747, Val loss: 3.037, Epoch time = 1.691s\n",
      "Epoch: 1578, Train loss: 3.505, Val loss: 2.789, Epoch time = 1.675s\n",
      "Epoch: 1579, Train loss: 4.175, Val loss: 3.340, Epoch time = 1.796s\n",
      "Epoch: 1580, Train loss: 3.717, Val loss: 2.768, Epoch time = 1.735s\n",
      "Epoch: 1581, Train loss: 3.392, Val loss: 4.100, Epoch time = 1.636s\n",
      "Epoch: 1582, Train loss: 3.185, Val loss: 3.107, Epoch time = 1.931s\n",
      "Epoch: 1583, Train loss: 3.313, Val loss: 2.661, Epoch time = 1.757s\n",
      "Epoch: 1584, Train loss: 4.000, Val loss: 2.697, Epoch time = 1.743s\n",
      "Epoch: 1585, Train loss: 3.293, Val loss: 2.887, Epoch time = 1.691s\n",
      "Epoch: 1586, Train loss: 3.928, Val loss: 2.440, Epoch time = 1.709s\n",
      "Epoch: 1587, Train loss: 2.869, Val loss: 2.936, Epoch time = 1.551s\n",
      "Epoch: 1588, Train loss: 2.994, Val loss: 3.243, Epoch time = 1.622s\n",
      "Epoch: 1589, Train loss: 3.472, Val loss: 3.239, Epoch time = 1.701s\n",
      "Epoch: 1590, Train loss: 3.120, Val loss: 3.406, Epoch time = 1.822s\n",
      "Epoch: 1591, Train loss: 3.543, Val loss: 2.927, Epoch time = 1.673s\n",
      "Epoch: 1592, Train loss: 3.371, Val loss: 2.838, Epoch time = 1.776s\n",
      "Epoch: 1593, Train loss: 3.253, Val loss: 2.898, Epoch time = 1.659s\n",
      "Epoch: 1594, Train loss: 3.127, Val loss: 2.822, Epoch time = 1.795s\n",
      "Epoch: 1595, Train loss: 3.581, Val loss: 3.182, Epoch time = 1.600s\n",
      "Epoch: 1596, Train loss: 3.861, Val loss: 3.024, Epoch time = 1.873s\n",
      "Epoch: 1597, Train loss: 2.917, Val loss: 2.856, Epoch time = 1.393s\n",
      "Epoch: 1598, Train loss: 3.244, Val loss: 2.749, Epoch time = 1.706s\n",
      "Epoch: 1599, Train loss: 3.017, Val loss: 3.185, Epoch time = 1.609s\n",
      "Epoch: 1600, Train loss: 3.221, Val loss: 2.857, Epoch time = 1.611s\n",
      "Epoch: 1601, Train loss: 3.207, Val loss: 2.841, Epoch time = 1.527s\n",
      "Epoch: 1602, Train loss: 3.184, Val loss: 2.920, Epoch time = 1.982s\n",
      "Epoch: 1603, Train loss: 3.855, Val loss: 2.887, Epoch time = 1.632s\n",
      "Epoch: 1604, Train loss: 4.011, Val loss: 2.759, Epoch time = 1.801s\n",
      "Epoch: 1605, Train loss: 3.804, Val loss: 2.901, Epoch time = 1.591s\n",
      "Epoch: 1606, Train loss: 3.415, Val loss: 2.902, Epoch time = 1.583s\n",
      "Epoch: 1607, Train loss: 3.063, Val loss: 2.948, Epoch time = 1.673s\n",
      "Epoch: 1608, Train loss: 3.521, Val loss: 3.125, Epoch time = 1.845s\n",
      "Epoch: 1609, Train loss: 2.754, Val loss: 2.806, Epoch time = 1.473s\n",
      "Epoch: 1610, Train loss: 3.711, Val loss: 3.060, Epoch time = 1.732s\n",
      "Epoch: 1611, Train loss: 3.518, Val loss: 2.971, Epoch time = 1.702s\n",
      "Epoch: 1612, Train loss: 3.497, Val loss: 2.712, Epoch time = 1.608s\n",
      "Epoch: 1613, Train loss: 3.322, Val loss: 3.022, Epoch time = 1.741s\n",
      "Epoch: 1614, Train loss: 3.166, Val loss: 2.962, Epoch time = 1.678s\n",
      "Epoch: 1615, Train loss: 3.671, Val loss: 2.741, Epoch time = 1.594s\n",
      "Epoch: 1616, Train loss: 3.038, Val loss: 3.256, Epoch time = 1.598s\n",
      "Epoch: 1617, Train loss: 3.153, Val loss: 3.417, Epoch time = 1.722s\n",
      "Epoch: 1618, Train loss: 2.929, Val loss: 2.696, Epoch time = 1.743s\n",
      "Epoch: 1619, Train loss: 2.856, Val loss: 3.097, Epoch time = 1.742s\n",
      "Epoch: 1620, Train loss: 3.612, Val loss: 2.807, Epoch time = 1.684s\n",
      "Epoch: 1621, Train loss: 3.093, Val loss: 3.176, Epoch time = 1.794s\n",
      "Epoch: 1622, Train loss: 3.448, Val loss: 2.649, Epoch time = 1.553s\n",
      "Epoch: 1623, Train loss: 3.681, Val loss: 3.162, Epoch time = 1.690s\n",
      "Epoch: 1624, Train loss: 3.282, Val loss: 4.020, Epoch time = 1.581s\n",
      "Epoch: 1625, Train loss: 3.391, Val loss: 3.541, Epoch time = 1.698s\n",
      "Epoch: 1626, Train loss: 3.010, Val loss: 3.076, Epoch time = 1.596s\n",
      "Epoch: 1627, Train loss: 3.138, Val loss: 2.940, Epoch time = 1.550s\n",
      "Epoch: 1628, Train loss: 2.808, Val loss: 2.807, Epoch time = 1.402s\n",
      "Epoch: 1629, Train loss: 2.883, Val loss: 2.436, Epoch time = 1.618s\n",
      "Epoch: 1630, Train loss: 3.047, Val loss: 3.066, Epoch time = 1.837s\n",
      "Epoch: 1631, Train loss: 3.437, Val loss: 2.797, Epoch time = 1.724s\n",
      "Epoch: 1632, Train loss: 4.089, Val loss: 3.373, Epoch time = 1.988s\n",
      "Epoch: 1633, Train loss: 3.789, Val loss: 2.861, Epoch time = 1.783s\n",
      "Epoch: 1634, Train loss: 3.281, Val loss: 2.805, Epoch time = 1.645s\n",
      "Epoch: 1635, Train loss: 3.522, Val loss: 3.119, Epoch time = 1.727s\n",
      "Epoch: 1636, Train loss: 3.611, Val loss: 3.218, Epoch time = 1.718s\n",
      "Epoch: 1637, Train loss: 2.997, Val loss: 3.083, Epoch time = 1.784s\n",
      "Epoch: 1638, Train loss: 3.286, Val loss: 4.386, Epoch time = 1.754s\n",
      "Epoch: 1639, Train loss: 3.282, Val loss: 2.769, Epoch time = 1.530s\n",
      "Epoch: 1640, Train loss: 2.948, Val loss: 2.904, Epoch time = 1.552s\n",
      "Epoch: 1641, Train loss: 3.642, Val loss: 3.058, Epoch time = 1.580s\n",
      "Epoch: 1642, Train loss: 3.229, Val loss: 3.263, Epoch time = 1.825s\n",
      "Epoch: 1643, Train loss: 3.513, Val loss: 2.939, Epoch time = 1.671s\n",
      "Epoch: 1644, Train loss: 3.116, Val loss: 2.865, Epoch time = 1.498s\n",
      "Epoch: 1645, Train loss: 3.115, Val loss: 3.261, Epoch time = 1.663s\n",
      "Epoch: 1646, Train loss: 3.833, Val loss: 2.764, Epoch time = 1.793s\n",
      "Epoch: 1647, Train loss: 3.012, Val loss: 3.067, Epoch time = 1.466s\n",
      "Epoch: 1648, Train loss: 3.109, Val loss: 2.868, Epoch time = 1.689s\n",
      "Epoch: 1649, Train loss: 2.954, Val loss: 3.015, Epoch time = 1.671s\n",
      "Epoch: 1650, Train loss: 3.935, Val loss: 2.885, Epoch time = 1.538s\n",
      "Epoch: 1651, Train loss: 3.463, Val loss: 2.732, Epoch time = 1.624s\n",
      "Epoch: 1652, Train loss: 3.037, Val loss: 2.892, Epoch time = 1.487s\n",
      "Epoch: 1653, Train loss: 3.826, Val loss: 3.105, Epoch time = 1.674s\n",
      "Epoch: 1654, Train loss: 2.741, Val loss: 3.065, Epoch time = 1.606s\n",
      "Epoch: 1655, Train loss: 3.099, Val loss: 2.912, Epoch time = 1.670s\n",
      "Epoch: 1656, Train loss: 2.980, Val loss: 2.617, Epoch time = 1.540s\n",
      "Epoch: 1657, Train loss: 3.042, Val loss: 2.689, Epoch time = 1.873s\n",
      "Epoch: 1658, Train loss: 3.033, Val loss: 3.178, Epoch time = 1.746s\n",
      "Epoch: 1659, Train loss: 3.297, Val loss: 3.031, Epoch time = 1.813s\n",
      "Epoch: 1660, Train loss: 3.399, Val loss: 3.174, Epoch time = 1.896s\n",
      "Epoch: 1661, Train loss: 3.146, Val loss: 2.761, Epoch time = 1.644s\n",
      "Epoch: 1662, Train loss: 3.531, Val loss: 2.836, Epoch time = 1.637s\n",
      "Epoch: 1663, Train loss: 3.240, Val loss: 3.051, Epoch time = 1.600s\n",
      "Epoch: 1664, Train loss: 3.129, Val loss: 2.702, Epoch time = 1.544s\n",
      "Epoch: 1665, Train loss: 3.446, Val loss: 2.693, Epoch time = 1.829s\n",
      "Epoch: 1666, Train loss: 3.693, Val loss: 2.900, Epoch time = 1.752s\n",
      "Epoch: 1667, Train loss: 3.252, Val loss: 3.205, Epoch time = 1.803s\n",
      "Epoch: 1668, Train loss: 3.114, Val loss: 2.878, Epoch time = 1.695s\n",
      "Epoch: 1669, Train loss: 3.074, Val loss: 2.909, Epoch time = 1.478s\n",
      "Epoch: 1670, Train loss: 3.174, Val loss: 3.039, Epoch time = 1.629s\n",
      "Epoch: 1671, Train loss: 3.472, Val loss: 3.040, Epoch time = 1.697s\n",
      "Epoch: 1672, Train loss: 3.261, Val loss: 3.006, Epoch time = 1.655s\n",
      "Epoch: 1673, Train loss: 3.110, Val loss: 2.936, Epoch time = 1.719s\n",
      "Epoch: 1674, Train loss: 3.306, Val loss: 2.655, Epoch time = 1.688s\n",
      "Epoch: 1675, Train loss: 2.907, Val loss: 3.717, Epoch time = 1.855s\n",
      "Epoch: 1676, Train loss: 3.335, Val loss: 2.963, Epoch time = 1.694s\n",
      "Epoch: 1677, Train loss: 3.656, Val loss: 2.802, Epoch time = 1.654s\n",
      "Epoch: 1678, Train loss: 3.011, Val loss: 2.827, Epoch time = 1.662s\n",
      "Epoch: 1679, Train loss: 3.639, Val loss: 2.906, Epoch time = 1.870s\n",
      "Epoch: 1680, Train loss: 3.298, Val loss: 2.797, Epoch time = 1.612s\n",
      "Epoch: 1681, Train loss: 2.752, Val loss: 2.601, Epoch time = 1.610s\n",
      "Epoch: 1682, Train loss: 3.280, Val loss: 3.045, Epoch time = 1.653s\n",
      "Epoch: 1683, Train loss: 3.010, Val loss: 2.621, Epoch time = 1.626s\n",
      "Epoch: 1684, Train loss: 3.178, Val loss: 2.868, Epoch time = 1.596s\n",
      "Epoch: 1685, Train loss: 2.968, Val loss: 2.759, Epoch time = 1.608s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1686, Train loss: 2.810, Val loss: 2.811, Epoch time = 1.414s\n",
      "Epoch: 1687, Train loss: 2.966, Val loss: 3.061, Epoch time = 1.539s\n",
      "Epoch: 1688, Train loss: 3.624, Val loss: 3.038, Epoch time = 1.625s\n",
      "Epoch: 1689, Train loss: 3.345, Val loss: 3.050, Epoch time = 1.533s\n",
      "Epoch: 1690, Train loss: 3.386, Val loss: 3.117, Epoch time = 1.641s\n",
      "Epoch: 1691, Train loss: 3.361, Val loss: 2.644, Epoch time = 1.868s\n",
      "Epoch: 1692, Train loss: 3.351, Val loss: 2.610, Epoch time = 1.941s\n",
      "Epoch: 1693, Train loss: 3.663, Val loss: 2.641, Epoch time = 1.495s\n",
      "Epoch: 1694, Train loss: 3.012, Val loss: 2.845, Epoch time = 1.803s\n",
      "Epoch: 1695, Train loss: 3.355, Val loss: 2.788, Epoch time = 1.562s\n",
      "Epoch: 1696, Train loss: 2.825, Val loss: 2.678, Epoch time = 1.558s\n",
      "Epoch: 1697, Train loss: 3.758, Val loss: 2.946, Epoch time = 1.858s\n",
      "Epoch: 1698, Train loss: 2.985, Val loss: 2.895, Epoch time = 1.874s\n",
      "Epoch: 1699, Train loss: 3.235, Val loss: 2.705, Epoch time = 1.782s\n",
      "Epoch: 1700, Train loss: 2.895, Val loss: 2.833, Epoch time = 1.778s\n",
      "Epoch: 1701, Train loss: 3.655, Val loss: 3.242, Epoch time = 1.816s\n",
      "Epoch: 1702, Train loss: 3.330, Val loss: 2.314, Epoch time = 1.562s\n",
      "Epoch: 1703, Train loss: 3.850, Val loss: 2.942, Epoch time = 1.943s\n",
      "Epoch: 1704, Train loss: 3.207, Val loss: 2.951, Epoch time = 1.772s\n",
      "Epoch: 1705, Train loss: 3.181, Val loss: 2.772, Epoch time = 1.754s\n",
      "Epoch: 1706, Train loss: 3.092, Val loss: 2.891, Epoch time = 1.917s\n",
      "Epoch: 1707, Train loss: 3.478, Val loss: 2.756, Epoch time = 1.976s\n",
      "Epoch: 1708, Train loss: 2.961, Val loss: 2.719, Epoch time = 1.746s\n",
      "Epoch: 1709, Train loss: 3.624, Val loss: 3.061, Epoch time = 1.917s\n",
      "Epoch: 1710, Train loss: 3.481, Val loss: 4.340, Epoch time = 1.517s\n",
      "Epoch: 1711, Train loss: 3.083, Val loss: 2.655, Epoch time = 1.630s\n",
      "Epoch: 1712, Train loss: 3.649, Val loss: 2.907, Epoch time = 1.806s\n",
      "Epoch: 1713, Train loss: 3.238, Val loss: 2.923, Epoch time = 1.705s\n",
      "Epoch: 1714, Train loss: 3.384, Val loss: 2.828, Epoch time = 1.544s\n",
      "Epoch: 1715, Train loss: 3.090, Val loss: 2.537, Epoch time = 1.631s\n",
      "Epoch: 1716, Train loss: 3.241, Val loss: 3.845, Epoch time = 1.594s\n",
      "Epoch: 1717, Train loss: 3.062, Val loss: 2.989, Epoch time = 1.604s\n",
      "Epoch: 1718, Train loss: 3.034, Val loss: 2.872, Epoch time = 1.547s\n",
      "Epoch: 1719, Train loss: 2.867, Val loss: 2.829, Epoch time = 1.633s\n",
      "Epoch: 1720, Train loss: 3.176, Val loss: 2.630, Epoch time = 1.644s\n",
      "Epoch: 1721, Train loss: 3.684, Val loss: 2.778, Epoch time = 1.578s\n",
      "Epoch: 1722, Train loss: 3.634, Val loss: 2.599, Epoch time = 1.542s\n",
      "Epoch: 1723, Train loss: 2.930, Val loss: 2.723, Epoch time = 1.517s\n",
      "Epoch: 1724, Train loss: 3.347, Val loss: 3.058, Epoch time = 1.587s\n",
      "Epoch: 1725, Train loss: 3.521, Val loss: 2.782, Epoch time = 1.516s\n",
      "Epoch: 1726, Train loss: 3.103, Val loss: 2.875, Epoch time = 1.787s\n",
      "Epoch: 1727, Train loss: 3.354, Val loss: 3.169, Epoch time = 1.587s\n",
      "Epoch: 1728, Train loss: 3.323, Val loss: 2.857, Epoch time = 1.572s\n",
      "Epoch: 1729, Train loss: 3.311, Val loss: 3.056, Epoch time = 1.605s\n",
      "Epoch: 1730, Train loss: 3.397, Val loss: 3.203, Epoch time = 1.831s\n",
      "Epoch: 1731, Train loss: 3.439, Val loss: 2.791, Epoch time = 1.679s\n",
      "Epoch: 1732, Train loss: 3.483, Val loss: 2.982, Epoch time = 1.807s\n",
      "Epoch: 1733, Train loss: 2.903, Val loss: 2.910, Epoch time = 1.599s\n",
      "Epoch: 1734, Train loss: 3.539, Val loss: 3.346, Epoch time = 1.506s\n",
      "Epoch: 1735, Train loss: 3.309, Val loss: 2.898, Epoch time = 1.607s\n",
      "Epoch: 1736, Train loss: 2.950, Val loss: 3.181, Epoch time = 1.553s\n",
      "Epoch: 1737, Train loss: 3.084, Val loss: 3.103, Epoch time = 1.538s\n",
      "Epoch: 1738, Train loss: 3.877, Val loss: 2.660, Epoch time = 1.567s\n",
      "Epoch: 1739, Train loss: 3.746, Val loss: 2.907, Epoch time = 1.584s\n",
      "Epoch: 1740, Train loss: 3.593, Val loss: 3.011, Epoch time = 1.663s\n",
      "Epoch: 1741, Train loss: 4.244, Val loss: 2.475, Epoch time = 1.784s\n",
      "Epoch: 1742, Train loss: 3.670, Val loss: 2.688, Epoch time = 1.606s\n",
      "Epoch: 1743, Train loss: 2.752, Val loss: 2.914, Epoch time = 1.603s\n",
      "Epoch: 1744, Train loss: 3.035, Val loss: 2.645, Epoch time = 1.717s\n",
      "Epoch: 1745, Train loss: 3.131, Val loss: 3.067, Epoch time = 1.559s\n",
      "Epoch: 1746, Train loss: 3.901, Val loss: 2.810, Epoch time = 1.633s\n",
      "Epoch: 1747, Train loss: 3.561, Val loss: 3.059, Epoch time = 1.688s\n",
      "Epoch: 1748, Train loss: 2.875, Val loss: 2.905, Epoch time = 1.538s\n",
      "Epoch: 1749, Train loss: 3.096, Val loss: 2.871, Epoch time = 1.540s\n",
      "Epoch: 1750, Train loss: 3.407, Val loss: 3.019, Epoch time = 1.563s\n",
      "Epoch: 1751, Train loss: 3.230, Val loss: 3.006, Epoch time = 1.678s\n",
      "Epoch: 1752, Train loss: 3.743, Val loss: 2.690, Epoch time = 1.582s\n",
      "Epoch: 1753, Train loss: 3.128, Val loss: 2.728, Epoch time = 1.529s\n",
      "Epoch: 1754, Train loss: 3.792, Val loss: 2.683, Epoch time = 1.764s\n",
      "Epoch: 1755, Train loss: 2.767, Val loss: 3.111, Epoch time = 1.427s\n",
      "Epoch: 1756, Train loss: 3.126, Val loss: 2.484, Epoch time = 1.672s\n",
      "Epoch: 1757, Train loss: 3.195, Val loss: 2.925, Epoch time = 1.769s\n",
      "Epoch: 1758, Train loss: 3.138, Val loss: 2.950, Epoch time = 1.533s\n",
      "Epoch: 1759, Train loss: 2.900, Val loss: 2.754, Epoch time = 1.679s\n",
      "Epoch: 1760, Train loss: 3.464, Val loss: 3.606, Epoch time = 1.633s\n",
      "Epoch: 1761, Train loss: 3.092, Val loss: 2.779, Epoch time = 1.587s\n",
      "Epoch: 1762, Train loss: 3.216, Val loss: 2.717, Epoch time = 1.477s\n",
      "Epoch: 1763, Train loss: 3.236, Val loss: 3.011, Epoch time = 1.826s\n",
      "Epoch: 1764, Train loss: 3.370, Val loss: 2.946, Epoch time = 1.855s\n",
      "Epoch: 1765, Train loss: 3.216, Val loss: 2.889, Epoch time = 2.027s\n",
      "Epoch: 1766, Train loss: 3.159, Val loss: 2.988, Epoch time = 1.645s\n",
      "Epoch: 1767, Train loss: 3.099, Val loss: 3.032, Epoch time = 1.631s\n",
      "Epoch: 1768, Train loss: 3.727, Val loss: 3.127, Epoch time = 1.887s\n",
      "Epoch: 1769, Train loss: 3.113, Val loss: 2.654, Epoch time = 1.485s\n",
      "Epoch: 1770, Train loss: 3.015, Val loss: 2.678, Epoch time = 2.307s\n",
      "Epoch: 1771, Train loss: 3.226, Val loss: 3.032, Epoch time = 1.562s\n",
      "Epoch: 1772, Train loss: 3.460, Val loss: 2.881, Epoch time = 1.707s\n",
      "Epoch: 1773, Train loss: 3.185, Val loss: 3.645, Epoch time = 1.692s\n",
      "Epoch: 1774, Train loss: 3.502, Val loss: 3.010, Epoch time = 1.700s\n",
      "Epoch: 1775, Train loss: 3.074, Val loss: 2.934, Epoch time = 2.201s\n",
      "Epoch: 1776, Train loss: 3.444, Val loss: 2.869, Epoch time = 1.967s\n",
      "Epoch: 1777, Train loss: 3.063, Val loss: 2.803, Epoch time = 1.741s\n",
      "Epoch: 1778, Train loss: 3.478, Val loss: 3.380, Epoch time = 1.538s\n",
      "Epoch: 1779, Train loss: 3.227, Val loss: 2.811, Epoch time = 1.772s\n",
      "Epoch: 1780, Train loss: 3.411, Val loss: 2.896, Epoch time = 1.894s\n",
      "Epoch: 1781, Train loss: 3.314, Val loss: 3.031, Epoch time = 1.800s\n",
      "Epoch: 1782, Train loss: 2.882, Val loss: 2.912, Epoch time = 2.090s\n",
      "Epoch: 1783, Train loss: 3.208, Val loss: 3.031, Epoch time = 1.943s\n",
      "Epoch: 1784, Train loss: 3.187, Val loss: 2.752, Epoch time = 1.810s\n",
      "Epoch: 1785, Train loss: 3.314, Val loss: 2.691, Epoch time = 1.758s\n",
      "Epoch: 1786, Train loss: 3.420, Val loss: 2.724, Epoch time = 1.738s\n",
      "Epoch: 1787, Train loss: 3.215, Val loss: 2.547, Epoch time = 1.635s\n",
      "Epoch: 1788, Train loss: 3.792, Val loss: 3.043, Epoch time = 1.619s\n",
      "Epoch: 1789, Train loss: 2.675, Val loss: 2.809, Epoch time = 1.599s\n",
      "Epoch: 1790, Train loss: 3.395, Val loss: 2.983, Epoch time = 1.537s\n",
      "Epoch: 1791, Train loss: 3.316, Val loss: 2.706, Epoch time = 1.737s\n",
      "Epoch: 1792, Train loss: 3.322, Val loss: 2.953, Epoch time = 1.736s\n",
      "Epoch: 1793, Train loss: 3.231, Val loss: 2.840, Epoch time = 1.605s\n",
      "Epoch: 1794, Train loss: 2.996, Val loss: 2.624, Epoch time = 2.042s\n",
      "Epoch: 1795, Train loss: 2.970, Val loss: 3.081, Epoch time = 2.009s\n",
      "Epoch: 1796, Train loss: 3.892, Val loss: 2.717, Epoch time = 2.312s\n",
      "Epoch: 1797, Train loss: 3.497, Val loss: 2.702, Epoch time = 1.841s\n",
      "Epoch: 1798, Train loss: 3.428, Val loss: 3.044, Epoch time = 2.153s\n",
      "Epoch: 1799, Train loss: 3.612, Val loss: 2.620, Epoch time = 1.685s\n",
      "Epoch: 1800, Train loss: 3.397, Val loss: 2.815, Epoch time = 1.720s\n",
      "Epoch: 1801, Train loss: 3.249, Val loss: 3.042, Epoch time = 1.634s\n",
      "Epoch: 1802, Train loss: 3.091, Val loss: 2.883, Epoch time = 1.662s\n",
      "Epoch: 1803, Train loss: 3.602, Val loss: 2.763, Epoch time = 1.565s\n",
      "Epoch: 1804, Train loss: 3.265, Val loss: 2.978, Epoch time = 1.574s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1805, Train loss: 3.350, Val loss: 2.822, Epoch time = 1.479s\n",
      "Epoch: 1806, Train loss: 3.495, Val loss: 2.876, Epoch time = 1.757s\n",
      "Epoch: 1807, Train loss: 3.034, Val loss: 3.259, Epoch time = 1.649s\n",
      "Epoch: 1808, Train loss: 3.422, Val loss: 2.800, Epoch time = 1.537s\n",
      "Epoch: 1809, Train loss: 3.439, Val loss: 3.100, Epoch time = 1.774s\n",
      "Epoch: 1810, Train loss: 3.459, Val loss: 2.882, Epoch time = 1.685s\n",
      "Epoch: 1811, Train loss: 3.907, Val loss: 3.059, Epoch time = 1.759s\n",
      "Epoch: 1812, Train loss: 2.981, Val loss: 2.935, Epoch time = 1.523s\n",
      "Epoch: 1813, Train loss: 3.084, Val loss: 3.100, Epoch time = 1.673s\n",
      "Epoch: 1814, Train loss: 3.249, Val loss: 2.957, Epoch time = 1.678s\n",
      "Epoch: 1815, Train loss: 3.323, Val loss: 2.790, Epoch time = 1.465s\n",
      "Epoch: 1816, Train loss: 3.311, Val loss: 2.706, Epoch time = 1.495s\n",
      "Epoch: 1817, Train loss: 3.151, Val loss: 2.913, Epoch time = 2.070s\n",
      "Epoch: 1818, Train loss: 3.209, Val loss: 2.865, Epoch time = 1.472s\n",
      "Epoch: 1819, Train loss: 3.311, Val loss: 2.831, Epoch time = 2.255s\n",
      "Epoch: 1820, Train loss: 3.521, Val loss: 4.007, Epoch time = 1.868s\n",
      "Epoch: 1821, Train loss: 3.262, Val loss: 2.940, Epoch time = 1.769s\n",
      "Epoch: 1822, Train loss: 3.105, Val loss: 3.066, Epoch time = 1.758s\n",
      "Epoch: 1823, Train loss: 3.030, Val loss: 2.498, Epoch time = 1.609s\n",
      "Epoch: 1824, Train loss: 2.882, Val loss: 3.022, Epoch time = 1.699s\n",
      "Epoch: 1825, Train loss: 3.459, Val loss: 2.678, Epoch time = 1.508s\n",
      "Epoch: 1826, Train loss: 4.153, Val loss: 2.508, Epoch time = 1.509s\n",
      "Epoch: 1827, Train loss: 3.432, Val loss: 2.814, Epoch time = 1.684s\n",
      "Epoch: 1828, Train loss: 3.648, Val loss: 2.922, Epoch time = 1.554s\n",
      "Epoch: 1829, Train loss: 3.121, Val loss: 2.926, Epoch time = 1.687s\n",
      "Epoch: 1830, Train loss: 3.137, Val loss: 2.707, Epoch time = 1.518s\n",
      "Epoch: 1831, Train loss: 2.952, Val loss: 2.661, Epoch time = 1.703s\n",
      "Epoch: 1832, Train loss: 2.832, Val loss: 2.647, Epoch time = 1.584s\n",
      "Epoch: 1833, Train loss: 2.940, Val loss: 3.025, Epoch time = 1.465s\n",
      "Epoch: 1834, Train loss: 2.968, Val loss: 2.912, Epoch time = 1.505s\n",
      "Epoch: 1835, Train loss: 2.930, Val loss: 2.868, Epoch time = 1.415s\n",
      "Epoch: 1836, Train loss: 2.992, Val loss: 2.780, Epoch time = 1.997s\n",
      "Epoch: 1837, Train loss: 3.007, Val loss: 3.014, Epoch time = 1.522s\n",
      "Epoch: 1838, Train loss: 3.666, Val loss: 2.819, Epoch time = 1.448s\n",
      "Epoch: 1839, Train loss: 3.322, Val loss: 2.891, Epoch time = 1.430s\n",
      "Epoch: 1840, Train loss: 3.117, Val loss: 2.784, Epoch time = 1.819s\n",
      "Epoch: 1841, Train loss: 2.737, Val loss: 2.960, Epoch time = 1.331s\n",
      "Epoch: 1842, Train loss: 3.111, Val loss: 3.035, Epoch time = 1.588s\n",
      "Epoch: 1843, Train loss: 3.314, Val loss: 3.589, Epoch time = 1.659s\n",
      "Epoch: 1844, Train loss: 3.731, Val loss: 2.929, Epoch time = 1.542s\n",
      "Epoch: 1845, Train loss: 3.761, Val loss: 3.148, Epoch time = 1.609s\n",
      "Epoch: 1846, Train loss: 2.977, Val loss: 2.927, Epoch time = 1.605s\n",
      "Epoch: 1847, Train loss: 3.582, Val loss: 3.190, Epoch time = 1.748s\n",
      "Epoch: 1848, Train loss: 2.813, Val loss: 2.934, Epoch time = 1.432s\n",
      "Epoch: 1849, Train loss: 3.268, Val loss: 2.871, Epoch time = 1.717s\n",
      "Epoch: 1850, Train loss: 2.910, Val loss: 3.162, Epoch time = 1.613s\n",
      "Epoch: 1851, Train loss: 2.944, Val loss: 3.311, Epoch time = 1.717s\n",
      "Epoch: 1852, Train loss: 3.520, Val loss: 2.894, Epoch time = 1.611s\n",
      "Epoch: 1853, Train loss: 3.176, Val loss: 2.774, Epoch time = 1.723s\n",
      "Epoch: 1854, Train loss: 3.282, Val loss: 2.912, Epoch time = 1.774s\n",
      "Epoch: 1855, Train loss: 2.992, Val loss: 3.005, Epoch time = 1.407s\n",
      "Epoch: 1856, Train loss: 3.964, Val loss: 2.709, Epoch time = 1.775s\n",
      "Epoch: 1857, Train loss: 3.101, Val loss: 3.112, Epoch time = 1.645s\n",
      "Epoch: 1858, Train loss: 3.671, Val loss: 2.663, Epoch time = 1.716s\n",
      "Epoch: 1859, Train loss: 3.147, Val loss: 3.093, Epoch time = 1.737s\n",
      "Epoch: 1860, Train loss: 3.138, Val loss: 2.760, Epoch time = 1.633s\n",
      "Epoch: 1861, Train loss: 2.986, Val loss: 3.305, Epoch time = 1.560s\n",
      "Epoch: 1862, Train loss: 3.016, Val loss: 2.890, Epoch time = 1.506s\n",
      "Epoch: 1863, Train loss: 3.082, Val loss: 2.529, Epoch time = 1.662s\n",
      "Epoch: 1864, Train loss: 3.803, Val loss: 2.896, Epoch time = 1.760s\n",
      "Epoch: 1865, Train loss: 3.420, Val loss: 3.024, Epoch time = 1.901s\n",
      "Epoch: 1866, Train loss: 3.268, Val loss: 3.104, Epoch time = 1.740s\n",
      "Epoch: 1867, Train loss: 2.951, Val loss: 2.860, Epoch time = 1.571s\n",
      "Epoch: 1868, Train loss: 2.758, Val loss: 2.970, Epoch time = 1.503s\n",
      "Epoch: 1869, Train loss: 2.986, Val loss: 2.745, Epoch time = 1.549s\n",
      "Epoch: 1870, Train loss: 3.059, Val loss: 3.206, Epoch time = 1.634s\n",
      "Epoch: 1871, Train loss: 3.355, Val loss: 3.051, Epoch time = 1.795s\n",
      "Epoch: 1872, Train loss: 3.242, Val loss: 2.864, Epoch time = 1.704s\n",
      "Epoch: 1873, Train loss: 3.825, Val loss: 2.581, Epoch time = 1.643s\n",
      "Epoch: 1874, Train loss: 3.112, Val loss: 3.119, Epoch time = 1.605s\n",
      "Epoch: 1875, Train loss: 3.322, Val loss: 3.115, Epoch time = 1.626s\n",
      "Epoch: 1876, Train loss: 3.252, Val loss: 2.992, Epoch time = 1.660s\n",
      "Epoch: 1877, Train loss: 3.201, Val loss: 2.654, Epoch time = 1.762s\n",
      "Epoch: 1878, Train loss: 3.552, Val loss: 4.185, Epoch time = 1.635s\n",
      "Epoch: 1879, Train loss: 3.184, Val loss: 2.785, Epoch time = 1.817s\n",
      "Epoch: 1880, Train loss: 3.871, Val loss: 2.890, Epoch time = 1.520s\n",
      "Epoch: 1881, Train loss: 3.434, Val loss: 2.782, Epoch time = 1.564s\n",
      "Epoch: 1882, Train loss: 2.907, Val loss: 3.169, Epoch time = 1.585s\n",
      "Epoch: 1883, Train loss: 2.838, Val loss: 2.579, Epoch time = 1.772s\n",
      "Epoch: 1884, Train loss: 3.226, Val loss: 2.890, Epoch time = 1.657s\n",
      "Epoch: 1885, Train loss: 2.850, Val loss: 2.903, Epoch time = 1.560s\n",
      "Epoch: 1886, Train loss: 3.414, Val loss: 3.105, Epoch time = 1.627s\n",
      "Epoch: 1887, Train loss: 3.683, Val loss: 2.839, Epoch time = 1.607s\n",
      "Epoch: 1888, Train loss: 3.344, Val loss: 2.868, Epoch time = 1.627s\n",
      "Epoch: 1889, Train loss: 3.557, Val loss: 2.986, Epoch time = 1.447s\n",
      "Epoch: 1890, Train loss: 3.048, Val loss: 3.067, Epoch time = 1.561s\n",
      "Epoch: 1891, Train loss: 3.108, Val loss: 3.086, Epoch time = 1.661s\n",
      "Epoch: 1892, Train loss: 3.671, Val loss: 3.000, Epoch time = 1.720s\n",
      "Epoch: 1893, Train loss: 3.023, Val loss: 3.079, Epoch time = 1.619s\n",
      "Epoch: 1894, Train loss: 2.924, Val loss: 2.908, Epoch time = 1.665s\n",
      "Epoch: 1895, Train loss: 2.994, Val loss: 4.214, Epoch time = 1.444s\n",
      "Epoch: 1896, Train loss: 2.856, Val loss: 2.781, Epoch time = 1.634s\n",
      "Epoch: 1897, Train loss: 3.329, Val loss: 3.115, Epoch time = 1.788s\n",
      "Epoch: 1898, Train loss: 3.039, Val loss: 2.852, Epoch time = 1.466s\n",
      "Epoch: 1899, Train loss: 3.648, Val loss: 3.153, Epoch time = 1.669s\n",
      "Epoch: 1900, Train loss: 3.928, Val loss: 3.087, Epoch time = 1.317s\n",
      "Epoch: 1901, Train loss: 3.264, Val loss: 2.861, Epoch time = 1.520s\n",
      "Epoch: 1902, Train loss: 3.664, Val loss: 2.814, Epoch time = 1.594s\n",
      "Epoch: 1903, Train loss: 3.041, Val loss: 3.287, Epoch time = 1.639s\n",
      "Epoch: 1904, Train loss: 2.744, Val loss: 2.667, Epoch time = 1.701s\n",
      "Epoch: 1905, Train loss: 4.120, Val loss: 3.354, Epoch time = 1.796s\n",
      "Epoch: 1906, Train loss: 3.364, Val loss: 2.616, Epoch time = 1.794s\n",
      "Epoch: 1907, Train loss: 2.948, Val loss: 2.750, Epoch time = 1.403s\n",
      "Epoch: 1908, Train loss: 3.492, Val loss: 2.784, Epoch time = 1.514s\n",
      "Epoch: 1909, Train loss: 3.357, Val loss: 2.917, Epoch time = 1.669s\n",
      "Epoch: 1910, Train loss: 2.947, Val loss: 3.292, Epoch time = 1.450s\n",
      "Epoch: 1911, Train loss: 3.032, Val loss: 2.865, Epoch time = 1.530s\n",
      "Epoch: 1912, Train loss: 2.788, Val loss: 2.768, Epoch time = 1.469s\n",
      "Epoch: 1913, Train loss: 3.135, Val loss: 2.903, Epoch time = 1.503s\n",
      "Epoch: 1914, Train loss: 3.119, Val loss: 2.875, Epoch time = 1.518s\n",
      "Epoch: 1915, Train loss: 3.039, Val loss: 2.653, Epoch time = 1.623s\n",
      "Epoch: 1916, Train loss: 3.238, Val loss: 2.959, Epoch time = 1.588s\n",
      "Epoch: 1917, Train loss: 3.075, Val loss: 3.166, Epoch time = 1.529s\n",
      "Epoch: 1918, Train loss: 3.143, Val loss: 4.182, Epoch time = 1.471s\n",
      "Epoch: 1919, Train loss: 3.162, Val loss: 3.027, Epoch time = 1.664s\n",
      "Epoch: 1920, Train loss: 3.132, Val loss: 2.852, Epoch time = 1.605s\n",
      "Epoch: 1921, Train loss: 2.752, Val loss: 2.563, Epoch time = 1.272s\n",
      "Epoch: 1922, Train loss: 3.216, Val loss: 2.538, Epoch time = 1.621s\n",
      "Epoch: 1923, Train loss: 3.438, Val loss: 3.023, Epoch time = 1.574s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1924, Train loss: 3.481, Val loss: 3.156, Epoch time = 1.593s\n",
      "Epoch: 1925, Train loss: 3.077, Val loss: 2.921, Epoch time = 1.567s\n",
      "Epoch: 1926, Train loss: 3.043, Val loss: 2.866, Epoch time = 1.536s\n",
      "Epoch: 1927, Train loss: 3.013, Val loss: 3.042, Epoch time = 1.692s\n",
      "Epoch: 1928, Train loss: 2.935, Val loss: 2.595, Epoch time = 1.645s\n",
      "Epoch: 1929, Train loss: 3.459, Val loss: 2.707, Epoch time = 1.706s\n",
      "Epoch: 1930, Train loss: 3.336, Val loss: 3.036, Epoch time = 1.574s\n",
      "Epoch: 1931, Train loss: 3.276, Val loss: 2.972, Epoch time = 1.730s\n",
      "Epoch: 1932, Train loss: 3.296, Val loss: 3.362, Epoch time = 1.722s\n",
      "Epoch: 1933, Train loss: 3.324, Val loss: 2.619, Epoch time = 1.531s\n",
      "Epoch: 1934, Train loss: 3.171, Val loss: 2.544, Epoch time = 1.559s\n",
      "Epoch: 1935, Train loss: 3.395, Val loss: 3.099, Epoch time = 1.793s\n",
      "Epoch: 1936, Train loss: 3.652, Val loss: 2.697, Epoch time = 1.572s\n",
      "Epoch: 1937, Train loss: 3.427, Val loss: 2.623, Epoch time = 1.626s\n",
      "Epoch: 1938, Train loss: 3.158, Val loss: 2.854, Epoch time = 1.569s\n",
      "Epoch: 1939, Train loss: 3.490, Val loss: 2.712, Epoch time = 1.718s\n",
      "Epoch: 1940, Train loss: 3.365, Val loss: 2.917, Epoch time = 1.554s\n",
      "Epoch: 1941, Train loss: 3.165, Val loss: 2.710, Epoch time = 1.624s\n",
      "Epoch: 1942, Train loss: 3.242, Val loss: 2.654, Epoch time = 1.622s\n",
      "Epoch: 1943, Train loss: 3.283, Val loss: 2.827, Epoch time = 1.618s\n",
      "Epoch: 1944, Train loss: 3.179, Val loss: 2.824, Epoch time = 1.585s\n",
      "Epoch: 1945, Train loss: 3.703, Val loss: 2.845, Epoch time = 1.882s\n",
      "Epoch: 1946, Train loss: 3.133, Val loss: 2.977, Epoch time = 1.847s\n",
      "Epoch: 1947, Train loss: 3.263, Val loss: 3.128, Epoch time = 1.517s\n",
      "Epoch: 1948, Train loss: 3.778, Val loss: 3.103, Epoch time = 1.703s\n",
      "Epoch: 1949, Train loss: 3.732, Val loss: 2.911, Epoch time = 1.775s\n",
      "Epoch: 1950, Train loss: 3.162, Val loss: 3.074, Epoch time = 1.495s\n",
      "Epoch: 1951, Train loss: 3.373, Val loss: 3.858, Epoch time = 1.575s\n",
      "Epoch: 1952, Train loss: 3.518, Val loss: 2.700, Epoch time = 1.453s\n",
      "Epoch: 1953, Train loss: 3.399, Val loss: 2.650, Epoch time = 1.636s\n",
      "Epoch: 1954, Train loss: 3.351, Val loss: 2.882, Epoch time = 1.617s\n",
      "Epoch: 1955, Train loss: 3.158, Val loss: 2.684, Epoch time = 1.575s\n",
      "Epoch: 1956, Train loss: 3.222, Val loss: 3.937, Epoch time = 1.405s\n",
      "Epoch: 1957, Train loss: 3.440, Val loss: 2.468, Epoch time = 1.726s\n",
      "Epoch: 1958, Train loss: 3.093, Val loss: 2.915, Epoch time = 1.860s\n",
      "Epoch: 1959, Train loss: 3.383, Val loss: 2.850, Epoch time = 1.907s\n",
      "Epoch: 1960, Train loss: 2.962, Val loss: 3.081, Epoch time = 2.028s\n",
      "Epoch: 1961, Train loss: 3.041, Val loss: 2.938, Epoch time = 1.562s\n",
      "Epoch: 1962, Train loss: 3.449, Val loss: 2.577, Epoch time = 1.633s\n",
      "Epoch: 1963, Train loss: 3.151, Val loss: 3.154, Epoch time = 1.579s\n",
      "Epoch: 1964, Train loss: 3.143, Val loss: 3.145, Epoch time = 1.555s\n",
      "Epoch: 1965, Train loss: 3.101, Val loss: 3.027, Epoch time = 1.543s\n",
      "Epoch: 1966, Train loss: 2.942, Val loss: 2.636, Epoch time = 1.515s\n",
      "Epoch: 1967, Train loss: 3.649, Val loss: 2.730, Epoch time = 1.702s\n",
      "Epoch: 1968, Train loss: 2.868, Val loss: 2.592, Epoch time = 1.414s\n",
      "Epoch: 1969, Train loss: 3.043, Val loss: 2.962, Epoch time = 1.729s\n",
      "Epoch: 1970, Train loss: 3.087, Val loss: 3.009, Epoch time = 1.704s\n",
      "Epoch: 1971, Train loss: 3.073, Val loss: 2.699, Epoch time = 1.579s\n",
      "Epoch: 1972, Train loss: 3.407, Val loss: 2.877, Epoch time = 1.409s\n",
      "Epoch: 1973, Train loss: 3.229, Val loss: 2.879, Epoch time = 1.409s\n",
      "Epoch: 1974, Train loss: 3.035, Val loss: 3.152, Epoch time = 1.534s\n",
      "Epoch: 1975, Train loss: 3.444, Val loss: 2.986, Epoch time = 1.540s\n",
      "Epoch: 1976, Train loss: 3.148, Val loss: 3.170, Epoch time = 1.766s\n",
      "Epoch: 1977, Train loss: 3.247, Val loss: 2.729, Epoch time = 1.672s\n",
      "Epoch: 1978, Train loss: 3.319, Val loss: 2.642, Epoch time = 1.670s\n",
      "Epoch: 1979, Train loss: 3.710, Val loss: 2.827, Epoch time = 1.783s\n",
      "Epoch: 1980, Train loss: 3.313, Val loss: 2.994, Epoch time = 2.449s\n",
      "Epoch: 1981, Train loss: 3.224, Val loss: 2.599, Epoch time = 1.942s\n",
      "Epoch: 1982, Train loss: 3.415, Val loss: 3.036, Epoch time = 1.792s\n",
      "Epoch: 1983, Train loss: 3.461, Val loss: 2.606, Epoch time = 1.724s\n",
      "Epoch: 1984, Train loss: 3.055, Val loss: 2.580, Epoch time = 1.619s\n",
      "Epoch: 1985, Train loss: 3.189, Val loss: 2.973, Epoch time = 1.710s\n",
      "Epoch: 1986, Train loss: 2.602, Val loss: 2.750, Epoch time = 1.499s\n",
      "Epoch: 1987, Train loss: 3.236, Val loss: 3.624, Epoch time = 1.743s\n",
      "Epoch: 1988, Train loss: 3.719, Val loss: 2.532, Epoch time = 1.592s\n",
      "Epoch: 1989, Train loss: 3.403, Val loss: 3.126, Epoch time = 1.803s\n",
      "Epoch: 1990, Train loss: 3.211, Val loss: 2.689, Epoch time = 1.566s\n",
      "Epoch: 1991, Train loss: 3.024, Val loss: 2.975, Epoch time = 1.656s\n",
      "Epoch: 1992, Train loss: 3.064, Val loss: 3.163, Epoch time = 1.572s\n",
      "Epoch: 1993, Train loss: 2.757, Val loss: 3.193, Epoch time = 1.523s\n",
      "Epoch: 1994, Train loss: 4.072, Val loss: 2.692, Epoch time = 1.668s\n",
      "Epoch: 1995, Train loss: 3.273, Val loss: 2.777, Epoch time = 1.768s\n",
      "Epoch: 1996, Train loss: 3.304, Val loss: 2.635, Epoch time = 1.695s\n",
      "Epoch: 1997, Train loss: 3.375, Val loss: 2.447, Epoch time = 1.776s\n",
      "Epoch: 1998, Train loss: 3.261, Val loss: 2.890, Epoch time = 1.867s\n",
      "Epoch: 1999, Train loss: 2.776, Val loss: 2.835, Epoch time = 2.054s\n",
      "Epoch: 2000, Train loss: 3.898, Val loss: 2.860, Epoch time = 1.485s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2000\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5480edd350>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=500\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0.]])\n",
      "[0 1 2 3 4 5] [1 2 6 3 4 5]\n",
      "tensor([[0.0021],\n",
      "        [1.0000],\n",
      "        [0.0692],\n",
      "        [0.9985],\n",
      "        [0.3015],\n",
      "        [0.0259]], grad_fn=<IndexBackward0>)\n",
      "[0]\n",
      "tensor([1])\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "src1, src2, y = collate_fn(10,-100,train=False)\n",
    "        \n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "print(y[0])\n",
    "#print(Ad[0])\n",
    "#torch.manual_seed(344)\n",
    "\n",
    "#Ad = torch.rand(1,3,4)\n",
    "#print(Ad[0])\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "\n",
    "#f=np.repeat(Ad[0].detach().numpy(), [2,2,2], axis=0)\n",
    "\n",
    "#print('f',f)\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "\n",
    "print(row_ind,col_ind)\n",
    "\n",
    "z=np.zeros(f.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "\n",
    "#print('z',z)        \n",
    "        \n",
    "z2 = np.zeros(f.shape)\n",
    "zero_col=np.where(~z.any(axis=0))[0]\n",
    "ind=torch.argmax(Ad[0][:,zero_col], dim=0)\n",
    "        \n",
    "print(Ad[0][:,zero_col])        \n",
    "print(np.where(~z.any(axis=0))[0])\n",
    "print(ind)\n",
    "print(z)\n",
    "\n",
    "for k,l in zip(ind,zero_col):\n",
    "    z2[k,l]=1\n",
    "    \n",
    "print(z+z2)    \n",
    "\n",
    "pp_A=postprocess(Ad)\n",
    "\n",
    "#print(pp_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "torch.Size([14, 26, 24])\n",
      "y tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "Ad tensor([[1.0000e+00, 1.2523e-03, 4.2685e-01],\n",
      "        [1.4013e-04, 9.9998e-01, 3.5553e-04],\n",
      "        [5.6119e-01, 3.4038e-01, 9.9999e-01]], grad_fn=<SliceBackward0>)\n",
      "pp [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "0 [1, 2, 3]\n",
      "e [4 5 6]\n",
      "1 [1, 2, 3]\n",
      "e [4 5 6]\n",
      "2 [1, 2, 3]\n",
      "e [4 5 6]\n",
      "mid [4 5 6]\n",
      "0 [4 5 6]\n",
      "e [7 8 9]\n",
      "1 [4 5 6]\n",
      "e [7 8 9]\n",
      "2 [4 5 6]\n",
      "e [7 8 9]\n",
      "mid [7 8 9]\n",
      "0 [7 8 9]\n",
      "e [10 11 12]\n",
      "1 [7 8 9]\n",
      "e [10 11 12]\n",
      "2 [7 8 9]\n",
      "e [10 11 12]\n",
      "mid [10 11 12]\n",
      "0 [10 11 12]\n",
      "e [13 14 15]\n",
      "1 [10 11 12]\n",
      "e [13 14 15]\n",
      "2 [10 11 12]\n",
      "e [13 14 15]\n",
      "mid [13 14 15]\n",
      "0 [13 14 15]\n",
      "e [16 17 18]\n",
      "1 [13 14 15]\n",
      "e [16 17 18]\n",
      "2 [13 14 15]\n",
      "e [16 17 18]\n",
      "mid [16 17 18]\n",
      "0 [16 17 18]\n",
      "e [19 20 21]\n",
      "1 [16 17 18]\n",
      "e [19 20 21]\n",
      "2 [16 17 18]\n",
      "e [19 20 21]\n",
      "mid [19 20 21]\n",
      "0 [19 20 21]\n",
      "e [22 23 24 25]\n",
      "1 [19 20 21]\n",
      "e [22 23 24 25]\n",
      "1 [19 20 21]\n",
      "e [22 23 24 25]\n",
      "2 [19 20 21]\n",
      "e [22 23 24 25]\n",
      "mid [22 23 24 25]\n",
      "0 [22 23 24 25]\n",
      "e [26 27 28 29]\n",
      "1 [22 23 24 25]\n",
      "e [26 27 28 29]\n",
      "2 [22 23 24 25]\n",
      "e [26 27 28 29]\n",
      "3 [22 23 24 25]\n",
      "e [26 27 28 29]\n",
      "mid [26 27 28 29]\n",
      "0 [26 27 28 29]\n",
      "e [30 31 32 33]\n",
      "1 [26 27 28 29]\n",
      "e [30 31 32 33]\n",
      "2 [26 27 28 29]\n",
      "e [30 31 32 33]\n",
      "3 [26 27 28 29]\n",
      "e [30 31 32 33]\n",
      "mid [30 31 32 33]\n",
      "0 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "1 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "2 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "3 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "mid [34 35 36 37]\n",
      "0 [34 35 36 37]\n",
      "e [38 39 40 41 42]\n",
      "1 [34 35 36 37]\n",
      "e [38 39 40 41 42]\n",
      "2 [34 35 36 37]\n",
      "e [38 39 40 41 42]\n",
      "3 [34 35 36 37]\n",
      "e [38 39 40 41 42]\n",
      "3 [34 35 36 37]\n",
      "e [38 39 40 41 42]\n",
      "mid [38 39 40 41 42]\n",
      "0 [38 39 40 41 42]\n",
      "e [43 44 45 46 47 48]\n",
      "1 [38 39 40 41 42]\n",
      "e [43 44 45 46 47 48]\n",
      "1 [38 39 40 41 42]\n",
      "e [43 44 45 46 47 48]\n",
      "2 [38 39 40 41 42]\n",
      "e [43 44 45 46 47 48]\n",
      "3 [38 39 40 41 42]\n",
      "e [43 44 45 46 47 48]\n",
      "4 [38 39 40 41 42]\n",
      "e [43 44 45 46 47 48]\n",
      "mid [43 44 45 46 47 48]\n",
      "0 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "1 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "1 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "2 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "3 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "4 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "5 [43 44 45 46 47 48]\n",
      "e [49 50 51 52 53 54 55]\n",
      "mid [49 50 51 52 53 54 55]\n",
      "0 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "1 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "2 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "3 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "4 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "5 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "6 [49 50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "mid [56 57 58 59 60 61 62]\n",
      "0 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "1 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "2 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "3 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "3 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "4 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "5 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "6 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69 70]\n",
      "mid [63 64 65 66 67 68 69 70]\n",
      "0 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "1 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "2 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "3 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "4 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "5 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "6 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "7 [63 64 65 66 67 68 69 70]\n",
      "e [71 72 73 74 75 76 77 78]\n",
      "mid [71 72 73 74 75 76 77 78]\n",
      "0 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "1 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "2 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "3 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "4 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "5 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "6 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "7 [71 72 73 74 75 76 77 78]\n",
      "e [79 80 81 82 83 84 85 86]\n",
      "mid [79 80 81 82 83 84 85 86]\n",
      "0 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "1 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "2 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "3 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "4 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "5 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "6 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "7 [79 80 81 82 83 84 85 86]\n",
      "e [87 88 89 90 91 92 93 94]\n",
      "mid [87 88 89 90 91 92 93 94]\n",
      "0 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "1 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "2 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "3 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "4 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "5 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "6 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "7 [87 88 89 90 91 92 93 94]\n",
      "e [ 95  96  97  98  99 100 101 102]\n",
      "mid [ 95  96  97  98  99 100 101 102]\n",
      "0 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "1 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "2 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "3 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "4 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "5 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "6 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "7 [ 95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110]\n",
      "mid [103 104 105 106 107 108 109 110]\n",
      "0 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "0 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "1 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "1 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "2 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "3 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "4 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "5 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "6 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "7 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "7 [103 104 105 106 107 108 109 110]\n",
      "e [111 112 113 114 115 116 117 118 119 120 121]\n",
      "mid [111 112 113 114 115 116 117 118 119 120 121]\n",
      "0 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "1 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "2 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "3 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "4 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "5 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "5 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "6 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "7 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "8 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "9 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "10 [111 112 113 114 115 116 117 118 119 120 121]\n",
      "e [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "mid [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "0 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "1 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "2 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "3 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "4 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "5 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "6 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "6 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "7 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "8 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "9 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "10 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "11 [122 123 124 125 126 127 128 129 130 131 132 133]\n",
      "e [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "mid [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "0 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "1 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "2 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "3 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "3 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "4 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "5 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "6 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "7 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "8 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "9 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "10 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "11 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "12 [134 135 136 137 138 139 140 141 142 143 144 145 146]\n",
      "e [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "mid [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "0 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "1 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "2 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "3 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "4 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "5 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "6 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "7 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "8 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "9 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "10 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "11 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "12 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "13 [147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "mid [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
      "mid [161 162 163 164 165 166 167 168 169 170 171 172 173 174]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recon\n",
    "run=13\n",
    "src1, src2, y = collate_fn(26,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "pp_A=postprocess_2(Ad)\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(14):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 13, 10, 6, 18]\n",
      "9\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  if __name__ == '__main__':\n",
      "/home/mo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135585\n"
     ]
    }
   ],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e2c9c290de89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
