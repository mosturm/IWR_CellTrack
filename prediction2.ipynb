{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    m_Ad = []\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        m_Ad.append(np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy()))\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ad = postprocess_MinCostAss(m_Ad,a)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 4 and the array at index 1 has size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1aa31e063d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 4 and the array at index 1 has size 6"
     ]
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class AdjacencyNonlearn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyNonlearn, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "      \n",
    "        \n",
    "        out_dec2=torch.transpose(src_t1,0,1) \n",
    "        out_dec1=torch.transpose(src_t2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 24 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.406, Val loss: 3.637, Epoch time = 2.360s\n",
      "Epoch: 2, Train loss: 3.937, Val loss: 3.943, Epoch time = 2.373s\n",
      "Epoch: 3, Train loss: 5.060, Val loss: 3.735, Epoch time = 2.253s\n",
      "Epoch: 4, Train loss: 4.464, Val loss: 3.859, Epoch time = 2.269s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_xyr.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_xyr.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcca0086cd0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack24.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack24.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1000\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k--- 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fd3fc07b888c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msrc_padding_mask2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mAd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_padding_mask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(Ad[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3782a41f0174>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_t1, src_t2, src_padding_mask1, src_padding_mask2)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_padding_mask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mout_dec1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_padding_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_padding_mask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    292\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                          memory_key_padding_mask=memory_key_padding_mask)\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    586\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m                            need_weights=False)[0]\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m                 attn_mask=attn_mask, average_attn_weights=average_attn_weights)\n\u001b[0m\u001b[1;32m   1161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_batched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5120\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatic_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5122\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5124\u001b[0m         \u001b[0;31m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=40)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(100):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "print(Ad[0])\n",
    "print(d[0])\n",
    "\n",
    "#Ad = torch.mul(Ad, d)\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "g=d[0].detach().numpy()\n",
    "\n",
    "print(np.multiply(f,g))\n",
    "print(y[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ad = postprocess_2(Ad)\n",
    "print(Ad[0])\n",
    "Ad=postprocess_MinCostAss(Ad)\n",
    "print(Ad[0])\n",
    "#torch.manual_seed(344)\n",
    "\n",
    "#Ad = torch.rand(1,3,4)\n",
    "#print(Ad[0])\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "\n",
    "l=np.ones(len(f))*2\n",
    "l=l.astype(int)\n",
    "f2=np.repeat(Ad[0].detach().numpy(), l, axis=0)\n",
    "\n",
    "#print('f2',f2)\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "\n",
    "#print(row_ind,col_ind)\n",
    "\n",
    "z=np.zeros(f.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "\n",
    "print(z)\n",
    "        \n",
    "f2[0::2, :] = z[:] \n",
    "        \n",
    "#print('f2',f2) \n",
    "\n",
    "\n",
    "row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind_f, col_ind_f):\n",
    "        z3[i,j]=1\n",
    "\n",
    "\n",
    "print(z3)\n",
    "\n",
    "f_add = z3[0::2, :] + z3[1::2, :]\n",
    "\n",
    "print('f_add',f_add)\n",
    "        \n",
    "z2 = np.zeros(f.shape)\n",
    "zero_col=np.where(~z.any(axis=0))[0]\n",
    "ind=torch.argmax(Ad[0][:,zero_col], dim=0)\n",
    "        \n",
    "print(Ad[0][:,zero_col])        \n",
    "print(np.where(~z.any(axis=0))[0])\n",
    "print(ind)\n",
    "print(z)\n",
    "\n",
    "for k,l in zip(ind,zero_col):\n",
    "    z2[k,l]=1\n",
    "    \n",
    "print(z+z2)    \n",
    "\n",
    "pp_A=postprocess(Ad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(pp_A[0])\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 31, 24])\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.INFEASIBLE\n",
      "y tensor([[1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Ad tensor([[1.0000e+00, 6.4315e-13, 6.9504e-10, 9.4253e-12, 1.0000e+00],\n",
      "        [3.4758e-08, 1.0254e-13, 1.0950e-16, 1.0000e+00, 5.3813e-12],\n",
      "        [1.5603e-10, 1.2858e-17, 1.0000e+00, 9.6070e-18, 2.2867e-11],\n",
      "        [7.8709e-16, 1.0000e+00, 2.7640e-14, 2.4762e-14, 4.8585e-16]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "pp [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "M0 [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "0 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "1 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "2 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "mid [5 6 7 8]\n",
      "M0 [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "0 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "1 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "2 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "mid [ 9 10 11 12]\n",
      "M0 [[1. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "1 [ 9 10 11 12]\n",
      "e [13 14 15 16 17]\n",
      "2 [ 9 10 11 12]\n",
      "e [13 14 15 16 17]\n",
      "3 [ 9 10 11 12]\n",
      "e [13 14 15 16 17]\n",
      "mid [13 14 15 16 17]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "M1 [[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "1 [13 14 15 16 17]\n",
      "e [18 19 20]\n",
      "3 [13 14 15 16 17]\n",
      "e [18 19 20]\n",
      "4 [13 14 15 16 17]\n",
      "e [18 19 20]\n",
      "mid [18 19 20]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "0 [18 19 20]\n",
      "e [21 22 23]\n",
      "1 [18 19 20]\n",
      "e [21 22 23]\n",
      "2 [18 19 20]\n",
      "e [21 22 23]\n",
      "mid [21 22 23]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "0 [21 22 23]\n",
      "e [24 25 26]\n",
      "1 [21 22 23]\n",
      "e [24 25 26]\n",
      "2 [21 22 23]\n",
      "e [24 25 26]\n",
      "mid [24 25 26]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "M1 [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "0 [24 25 26]\n",
      "e [27 28 29]\n",
      "1 [24 25 26]\n",
      "e [27 28 29]\n",
      "2 [24 25 26]\n",
      "e [27 28 29]\n",
      "mid [27 28 29]\n",
      "M0 [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "0 [27 28 29]\n",
      "e [30 31 32 33]\n",
      "1 [27 28 29]\n",
      "e [30 31 32 33]\n",
      "2 [27 28 29]\n",
      "e [30 31 32 33]\n",
      "mid [30 31 32 33]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]]\n",
      "M1 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 1. 0.]]\n",
      "0 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "2 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "3 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "3 [30 31 32 33]\n",
      "e [34 35 36 37]\n",
      "mid [34 35 36 37]\n",
      "M0 [[1. 0. 0. 1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]]\n",
      "0 [34 35 36 37]\n",
      "e [38 39 40 41 42 43 44 45]\n",
      "1 [34 35 36 37]\n",
      "e [38 39 40 41 42 43 44 45]\n",
      "2 [34 35 36 37]\n",
      "e [38 39 40 41 42 43 44 45]\n",
      "3 [34 35 36 37]\n",
      "e [38 39 40 41 42 43 44 45]\n",
      "3 [34 35 36 37]\n",
      "e [38 39 40 41 42 43 44 45]\n",
      "mid [38 39 40 41 42 43 44 45]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0 [38 39 40 41 42 43 44 45]\n",
      "e [46 47 48 49 50]\n",
      "1 [38 39 40 41 42 43 44 45]\n",
      "e [46 47 48 49 50]\n",
      "3 [38 39 40 41 42 43 44 45]\n",
      "e [46 47 48 49 50]\n",
      "4 [38 39 40 41 42 43 44 45]\n",
      "e [46 47 48 49 50]\n",
      "5 [38 39 40 41 42 43 44 45]\n",
      "e [46 47 48 49 50]\n",
      "mid [46 47 48 49 50]\n",
      "M0 [[1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "0 [46 47 48 49 50]\n",
      "e [51 52 53 54 55 56]\n",
      "1 [46 47 48 49 50]\n",
      "e [51 52 53 54 55 56]\n",
      "2 [46 47 48 49 50]\n",
      "e [51 52 53 54 55 56]\n",
      "3 [46 47 48 49 50]\n",
      "e [51 52 53 54 55 56]\n",
      "4 [46 47 48 49 50]\n",
      "e [51 52 53 54 55 56]\n",
      "mid [51 52 53 54 55 56]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "1 [51 52 53 54 55 56]\n",
      "e [57 58 59 60 61]\n",
      "2 [51 52 53 54 55 56]\n",
      "e [57 58 59 60 61]\n",
      "3 [51 52 53 54 55 56]\n",
      "e [57 58 59 60 61]\n",
      "4 [51 52 53 54 55 56]\n",
      "e [57 58 59 60 61]\n",
      "5 [51 52 53 54 55 56]\n",
      "e [57 58 59 60 61]\n",
      "mid [57 58 59 60 61]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "0 [57 58 59 60 61]\n",
      "e [62 63 64 65 66 67]\n",
      "0 [57 58 59 60 61]\n",
      "e [62 63 64 65 66 67]\n",
      "1 [57 58 59 60 61]\n",
      "e [62 63 64 65 66 67]\n",
      "2 [57 58 59 60 61]\n",
      "e [62 63 64 65 66 67]\n",
      "3 [57 58 59 60 61]\n",
      "e [62 63 64 65 66 67]\n",
      "4 [57 58 59 60 61]\n",
      "e [62 63 64 65 66 67]\n",
      "mid [62 63 64 65 66 67]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n",
      "0 [62 63 64 65 66 67]\n",
      "e [68 69 70 71 72 73]\n",
      "1 [62 63 64 65 66 67]\n",
      "e [68 69 70 71 72 73]\n",
      "2 [62 63 64 65 66 67]\n",
      "e [68 69 70 71 72 73]\n",
      "3 [62 63 64 65 66 67]\n",
      "e [68 69 70 71 72 73]\n",
      "4 [62 63 64 65 66 67]\n",
      "e [68 69 70 71 72 73]\n",
      "5 [62 63 64 65 66 67]\n",
      "e [68 69 70 71 72 73]\n",
      "mid [68 69 70 71 72 73]\n",
      "M0 [[1. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [68 69 70 71 72 73]\n",
      "e [74 75 76 77 78 79 80 81]\n",
      "1 [68 69 70 71 72 73]\n",
      "e [74 75 76 77 78 79 80 81]\n",
      "2 [68 69 70 71 72 73]\n",
      "e [74 75 76 77 78 79 80 81]\n",
      "3 [68 69 70 71 72 73]\n",
      "e [74 75 76 77 78 79 80 81]\n",
      "4 [68 69 70 71 72 73]\n",
      "e [74 75 76 77 78 79 80 81]\n",
      "5 [68 69 70 71 72 73]\n",
      "e [74 75 76 77 78 79 80 81]\n",
      "mid [74 75 76 77 78 79 80 81]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "0 [74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87]\n",
      "1 [74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87]\n",
      "2 [74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87]\n",
      "3 [74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87]\n",
      "6 [74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87]\n",
      "7 [74 75 76 77 78 79 80 81]\n",
      "e [82 83 84 85 86 87]\n",
      "mid [82 83 84 85 86 87]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [82 83 84 85 86 87]\n",
      "e [88 89 90 91 92 93 94 95]\n",
      "1 [82 83 84 85 86 87]\n",
      "e [88 89 90 91 92 93 94 95]\n",
      "2 [82 83 84 85 86 87]\n",
      "e [88 89 90 91 92 93 94 95]\n",
      "3 [82 83 84 85 86 87]\n",
      "e [88 89 90 91 92 93 94 95]\n",
      "4 [82 83 84 85 86 87]\n",
      "e [88 89 90 91 92 93 94 95]\n",
      "5 [82 83 84 85 86 87]\n",
      "e [88 89 90 91 92 93 94 95]\n",
      "mid [88 89 90 91 92 93 94 95]\n",
      "M0 [[1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [88 89 90 91 92 93 94 95]\n",
      "e [ 96  97  98  99 100 101 102 103]\n",
      "1 [88 89 90 91 92 93 94 95]\n",
      "e [ 96  97  98  99 100 101 102 103]\n",
      "2 [88 89 90 91 92 93 94 95]\n",
      "e [ 96  97  98  99 100 101 102 103]\n",
      "3 [88 89 90 91 92 93 94 95]\n",
      "e [ 96  97  98  99 100 101 102 103]\n",
      "4 [88 89 90 91 92 93 94 95]\n",
      "e [ 96  97  98  99 100 101 102 103]\n",
      "7 [88 89 90 91 92 93 94 95]\n",
      "e [ 96  97  98  99 100 101 102 103]\n",
      "mid [ 96  97  98  99 100 101 102 103]\n",
      "M0 [[1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "1 [ 96  97  98  99 100 101 102 103]\n",
      "e [104 105 106 107 108 109 110 111]\n",
      "2 [ 96  97  98  99 100 101 102 103]\n",
      "e [104 105 106 107 108 109 110 111]\n",
      "3 [ 96  97  98  99 100 101 102 103]\n",
      "e [104 105 106 107 108 109 110 111]\n",
      "4 [ 96  97  98  99 100 101 102 103]\n",
      "e [104 105 106 107 108 109 110 111]\n",
      "6 [ 96  97  98  99 100 101 102 103]\n",
      "e [104 105 106 107 108 109 110 111]\n",
      "7 [ 96  97  98  99 100 101 102 103]\n",
      "e [104 105 106 107 108 109 110 111]\n",
      "mid [104 105 106 107 108 109 110 111]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "2 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "3 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "3 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "4 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "5 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "6 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "7 [104 105 106 107 108 109 110 111]\n",
      "e [112 113 114 115 116 117 118]\n",
      "mid [112 113 114 115 116 117 118]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "1 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "2 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "3 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "4 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "5 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "6 [112 113 114 115 116 117 118]\n",
      "e [119 120 121 122 123 124 125]\n",
      "mid [119 120 121 122 123 124 125]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "1 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "2 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "3 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "4 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "5 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "6 [119 120 121 122 123 124 125]\n",
      "e [126 127 128 129 130 131 132]\n",
      "mid [126 127 128 129 130 131 132]\n",
      "M0 [[1. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "1 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "2 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "3 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "4 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "5 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "6 [126 127 128 129 130 131 132]\n",
      "e [133 134 135 136 137 138 139 140]\n",
      "mid [133 134 135 136 137 138 139 140]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "0 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "1 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "2 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "2 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "3 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "5 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "6 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "7 [133 134 135 136 137 138 139 140]\n",
      "e [141 142 143 144 145 146 147 148 149]\n",
      "mid [141 142 143 144 145 146 147 148 149]\n",
      "M0 [[1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "0 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "1 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "1 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "2 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "3 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "3 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "4 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "5 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "7 [141 142 143 144 145 146 147 148 149]\n",
      "e [150 151 152 153 154 155 156 157 158 159 160]\n",
      "mid [150 151 152 153 154 155 156 157 158 159 160]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "1 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "2 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "3 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "5 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "5 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "6 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "7 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "8 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "9 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "10 [150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "mid [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "1 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "2 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "3 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "4 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "5 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "6 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "7 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "8 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "9 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "11 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "11 [161 162 163 164 165 166 167 168 169 170 171 172]\n",
      "e [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "mid [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "0 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "1 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "2 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "3 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "4 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "5 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "7 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "8 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "9 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "10 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "11 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "12 [173 174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "mid [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "1 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "2 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "3 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "4 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "5 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "6 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "7 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "8 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "9 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "10 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "11 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "12 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211]\n",
      "mid [199 200 201 202 203 204 205 206 207 208 209 210 211]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recon\n",
    "run=7\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "a=0.1\n",
    "pp_A = complete_postprocess(Ad,d,a)\n",
    "\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
