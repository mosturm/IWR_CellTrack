{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch = [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            print('recon')\n",
    "        else:\n",
    "            E1,E2,A=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(14)\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "        \n",
    "        A=A[id1-1]\n",
    "        #print(A)\n",
    "        A=A[:,id2-1]\n",
    "        #print(A)\n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = np.random.randint(14)\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "        \n",
    "        A=A[id1-1]\n",
    "        #print(A)\n",
    "        A=A[:,id2-1]\n",
    "        #print(run,t,id1,id2)\n",
    "        #print(E1,E2)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "        \n",
    "        A=A[id1-1]\n",
    "        #print(A)\n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "       \n",
    "        #print(E1,E2)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A) \n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y = collate_fn(26,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen):\n",
    "        self.pen=pen\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y = collate_fn(26,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        capacities = np.concatenate((np.ones(left_n)*2,np.ones(len(con_e)),np.ones(right_n)))\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        \n",
    "        \n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "                                    \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        tasks = right_n\n",
    "        supplies= tasks \n",
    "        supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        supplies=np.append(supplies,-tasks)\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies,tasks)\n",
    "        \n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[1,2,3]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 24\n",
    "nhead= 6\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.281, Val loss: 3.525, Epoch time = 1.113s\n",
      "Epoch: 2, Train loss: 4.505, Val loss: 3.364, Epoch time = 1.307s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc919eef10>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=5000\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 5.83709189e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.84957532e-02 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.34522617e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.30926648e-03 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.36245217e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00 1.49288311e-07 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 4.14654729e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.82184771e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 5.71446362e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 4.16381383e-08 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 4.51137657e-06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 5.23725430e-05 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.72074527e-09 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-698ac0a6e9d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#print(Ad[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "src1, src2, y = collate_fn(10,-100,train=False)\n",
    "        \n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "print(y[0])\n",
    "#print(Ad[0])\n",
    "Ad = postprocess_2(Ad)\n",
    "print(Ad[0])\n",
    "Ad=postprocess_MinCostAss(Ad)\n",
    "print(Ad[0])\n",
    "#torch.manual_seed(344)\n",
    "\n",
    "#Ad = torch.rand(1,3,4)\n",
    "#print(Ad[0])\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "\n",
    "l=np.ones(len(f))*2\n",
    "l=l.astype(int)\n",
    "f2=np.repeat(Ad[0].detach().numpy(), l, axis=0)\n",
    "\n",
    "#print('f2',f2)\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "\n",
    "#print(row_ind,col_ind)\n",
    "\n",
    "z=np.zeros(f.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "\n",
    "print(z)\n",
    "        \n",
    "f2[0::2, :] = z[:] \n",
    "        \n",
    "#print('f2',f2) \n",
    "\n",
    "\n",
    "row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind_f, col_ind_f):\n",
    "        z3[i,j]=1\n",
    "\n",
    "\n",
    "print(z3)\n",
    "\n",
    "f_add = z3[0::2, :] + z3[1::2, :]\n",
    "\n",
    "print('f_add',f_add)\n",
    "        \n",
    "z2 = np.zeros(f.shape)\n",
    "zero_col=np.where(~z.any(axis=0))[0]\n",
    "ind=torch.argmax(Ad[0][:,zero_col], dim=0)\n",
    "        \n",
    "print(Ad[0][:,zero_col])        \n",
    "print(np.where(~z.any(axis=0))[0])\n",
    "print(ind)\n",
    "print(z)\n",
    "\n",
    "for k,l in zip(ind,zero_col):\n",
    "    z2[k,l]=1\n",
    "    \n",
    "print(z+z2)    \n",
    "\n",
    "pp_A=postprocess(Ad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(pp_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "recon\n",
      "torch.Size([29, 26, 24])\n",
      "y tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 1., 0.]])\n",
      "Ad [[0.00000000e+00 1.00000000e+00 3.14809370e-07 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.28340650e-06 1.00000000e+00]\n",
      " [1.00000000e+00 0.00000000e+00 9.90660369e-01 0.00000000e+00]]\n",
      "pp [[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "0 [1, 2, 3]\n",
      "e [4 5 6 7]\n",
      "1 [1, 2, 3]\n",
      "e [4 5 6 7]\n",
      "2 [1, 2, 3]\n",
      "e [4 5 6 7]\n",
      "2 [1, 2, 3]\n",
      "e [4 5 6 7]\n",
      "mid [4 5 6 7]\n",
      "0 [4 5 6 7]\n",
      "e [ 8  9 10 11 12]\n",
      "1 [4 5 6 7]\n",
      "e [ 8  9 10 11 12]\n",
      "1 [4 5 6 7]\n",
      "e [ 8  9 10 11 12]\n",
      "2 [4 5 6 7]\n",
      "e [ 8  9 10 11 12]\n",
      "3 [4 5 6 7]\n",
      "e [ 8  9 10 11 12]\n",
      "mid [ 8  9 10 11 12]\n",
      "0 [ 8  9 10 11 12]\n",
      "e [13 14 15 16 17 18]\n",
      "1 [ 8  9 10 11 12]\n",
      "e [13 14 15 16 17 18]\n",
      "1 [ 8  9 10 11 12]\n",
      "e [13 14 15 16 17 18]\n",
      "2 [ 8  9 10 11 12]\n",
      "e [13 14 15 16 17 18]\n",
      "3 [ 8  9 10 11 12]\n",
      "e [13 14 15 16 17 18]\n",
      "4 [ 8  9 10 11 12]\n",
      "e [13 14 15 16 17 18]\n",
      "mid [13 14 15 16 17 18]\n",
      "0 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "1 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "2 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "3 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "4 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "4 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "5 [13 14 15 16 17 18]\n",
      "e [19 20 21 22 23 24 25]\n",
      "mid [19 20 21 22 23 24 25]\n",
      "0 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "0 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "1 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "2 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "3 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "4 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "5 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "6 [19 20 21 22 23 24 25]\n",
      "e [26 27 28 29 30 31 32 33]\n",
      "mid [26 27 28 29 30 31 32 33]\n",
      "0 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "1 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "1 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "2 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "3 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "4 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "5 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "6 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "7 [26 27 28 29 30 31 32 33]\n",
      "e [34 35 36 37 38 39 40 41 42]\n",
      "mid [34 35 36 37 38 39 40 41 42]\n",
      "0 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "1 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "2 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "3 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "4 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "5 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "6 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "7 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "8 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "8 [34 35 36 37 38 39 40 41 42]\n",
      "e [43 44 45 46 47 48 49 50 51 52]\n",
      "mid [43 44 45 46 47 48 49 50 51 52]\n",
      "0 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "0 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "1 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "2 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "3 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "4 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "5 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "5 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "6 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "7 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "8 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "9 [43 44 45 46 47 48 49 50 51 52]\n",
      "e [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "mid [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "0 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "1 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "2 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "3 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "4 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "5 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "6 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "7 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "8 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "9 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "10 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "11 [53 54 55 56 57 58 59 60 61 62 63 64]\n",
      "e [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "mid [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "0 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "1 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "2 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "3 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "4 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "5 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "6 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "6 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "7 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "8 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "9 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "10 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "11 [65 66 67 68 69 70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "mid [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "0 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "1 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "2 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "3 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "4 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "5 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "6 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "7 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "8 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "9 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "10 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "11 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "12 [77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "e [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "mid [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "0 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "1 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "2 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "3 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "4 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "5 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "6 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "7 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "8 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "9 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "10 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "11 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "12 [ 90  91  92  93  94  95  96  97  98  99 100 101 102]\n",
      "e [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "mid [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "0 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "1 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "2 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "2 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "3 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "4 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "5 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "6 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "7 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "8 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "9 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "10 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "11 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "12 [103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "mid [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "0 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "1 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "2 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "3 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "3 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "4 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "5 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "6 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "7 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "8 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "9 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "10 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "11 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "12 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "13 [116 117 118 119 120 121 122 123 124 125 126 127 128 129]\n",
      "e [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "mid [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "0 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "1 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "2 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "3 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "4 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "5 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "6 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "7 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "8 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "9 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "10 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "11 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "12 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "13 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "14 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "14 [130 131 132 133 134 135 136 137 138 139 140 141 142 143 144]\n",
      "e [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "mid [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "0 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "1 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "2 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "3 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "4 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "5 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "6 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "7 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "8 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "9 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "10 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "11 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "12 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "13 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "14 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "14 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "15 [145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160]\n",
      "e [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "mid [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "0 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "1 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "2 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "3 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "4 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "5 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "6 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "7 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "8 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "9 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "10 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "11 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "12 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "13 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "14 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "15 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "16 [161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "e [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "mid [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "0 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "0 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "1 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "2 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "3 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "4 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "5 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "6 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "7 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "8 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "9 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "10 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "10 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "11 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "12 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "13 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "14 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "15 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "16 [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "e [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "mid [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "0 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "0 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "1 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "2 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "3 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "4 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "5 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "6 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "7 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "8 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "9 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "10 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "11 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "12 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "13 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "14 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "15 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "16 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "17 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "18 [195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
      " 213]\n",
      "e [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "mid [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "0 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "1 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "2 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "3 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "4 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "5 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "6 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "7 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "8 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "9 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "10 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "11 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "12 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "13 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "14 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "15 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "16 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "17 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "18 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "19 [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233]\n",
      "e [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "mid [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "0 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "1 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "1 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "2 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "2 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "3 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "4 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "5 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "6 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "7 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "8 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "9 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "10 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "11 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "12 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "13 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "14 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "15 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "16 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "17 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "18 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "19 [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253]\n",
      "e [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "mid [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "0 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "1 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "1 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "2 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "3 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "4 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "5 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "6 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "7 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "8 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "9 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "10 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "11 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "12 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "13 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "14 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "15 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "16 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "17 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "18 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "19 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "20 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "21 [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275]\n",
      "e [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "mid [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "0 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "1 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "2 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "3 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "4 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "5 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "6 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "7 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "8 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "9 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "10 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "11 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "12 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "12 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "13 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "14 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "15 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "16 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "17 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "18 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "19 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "20 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "21 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "22 [276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298]\n",
      "e [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "mid [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "0 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "1 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "2 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "3 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "4 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "5 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "5 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "6 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "7 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "8 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "9 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "10 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "11 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "12 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "13 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "13 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "14 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "15 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "16 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "17 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "18 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "19 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "20 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "21 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "22 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "22 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "23 [299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322]\n",
      "e [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "mid [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "0 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "1 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "2 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "3 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "3 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "4 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "5 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "6 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "7 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "8 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "9 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "10 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "11 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "12 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "13 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "14 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "15 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "16 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "17 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "18 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "19 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "20 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "21 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "22 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "23 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "23 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "24 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "25 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "26 [323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349]\n",
      "e [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "mid [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n",
      "mid [350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 369 370 371 372 373 374 375 376 377 378]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recon\n",
    "run=11\n",
    "src1, src2, y = collate_fn(26,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "transformer.load_state_dict(torch.load('AttTrack.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "Ad = postprocess_2(Ad)\n",
    "pp_A=postprocess_MinCostAss(Ad)\n",
    "#pp_A=postprocess_linAss(Ad)\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(14):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 29, 3, 13, 44]\n",
      "5\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  if __name__ == '__main__':\n",
      "/home/mo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e2c9c290de89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
