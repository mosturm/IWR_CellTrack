{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    m_Ad = []\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        m_Ad.append(np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy()))\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ad = postprocess_MinCostAss(m_Ad,a)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99, 0.87, 0.05, 0.08, 0.77, 0.11],\n",
       "       [0.05, 0.12, 0.19, 0.11, 0.14, 0.93],\n",
       "       [0.07, 0.12, 0.45, 0.89, 0.23, 0.05],\n",
       "       [0.04, 0.1 , 0.97, 0.65, 0.34, 0.02],\n",
       "       [1.  , 1.  , 1.  , 1.  , 1.  , 1.  ],\n",
       "       [1.  , 0.75, 0.07, 0.1 , 0.08, 0.8 ],\n",
       "       [1.  , 0.69, 0.07, 0.88, 0.34, 0.02],\n",
       "       [1.  , 0.1 , 0.9 , 0.05, 0.84, 0.02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1,\n",
    "                 out = False):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n",
    "    \n",
    "\n",
    "    \n",
    "class AdjacencyTransformer_2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = True, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05):\n",
    "        super(AdjacencyTransformer_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        #self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "        self.out=out \n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        #src1_emb = self.positional_encoding(src_t1)\n",
    "        #src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src_t1,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src_t2,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec1 = self.lin(out_dec1)\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class AdjacencyNonlearn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyNonlearn, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "      \n",
    "        \n",
    "        out_dec2=torch.transpose(src_t1,0,1) \n",
    "        out_dec1=torch.transpose(src_t2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 24 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.417, Val loss: 5.068, Epoch time = 2.658s\n",
      "Epoch: 2, Train loss: 5.761, Val loss: 3.708, Epoch time = 4.127s\n",
      "Epoch: 3, Train loss: 4.235, Val loss: 4.139, Epoch time = 3.787s\n",
      "Epoch: 4, Train loss: 4.747, Val loss: 4.179, Epoch time = 3.748s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_xyr.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_xyr.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2beec45d0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack24.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_AttTrack24.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1000\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k--- 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2b5a24050>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 31, 24])\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.INFEASIBLE\n",
      "y tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Ad tensor([[1.0000e+00, 4.1419e-09, 6.1942e-11, 1.0000e+00, 1.4290e-10],\n",
      "        [3.1586e-12, 6.4225e-11, 2.4955e-15, 2.0046e-11, 1.0000e+00],\n",
      "        [1.2560e-11, 9.4233e-13, 9.9979e-01, 3.4208e-14, 2.6567e-15],\n",
      "        [2.4875e-07, 1.0000e+00, 1.2060e-17, 1.1076e-09, 4.4607e-12]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "pp [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M0 [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "0 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "1 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "2 [2, 3, 4]\n",
      "e [5 6 7 8]\n",
      "mid [5 6 7 8]\n",
      "M0 [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "1 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "3 [5 6 7 8]\n",
      "e [ 9 10 11 12]\n",
      "mid [ 9 10 11 12]\n",
      "M0 [[1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "1 [ 9 10 11 12]\n",
      "e [13 14 15 16]\n",
      "2 [ 9 10 11 12]\n",
      "e [13 14 15 16]\n",
      "3 [ 9 10 11 12]\n",
      "e [13 14 15 16]\n",
      "mid [13 14 15 16]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 0.]]\n",
      "0 [13 14 15 16]\n",
      "e [17 18 19]\n",
      "1 [13 14 15 16]\n",
      "e [17 18 19]\n",
      "2 [13 14 15 16]\n",
      "e [17 18 19]\n",
      "mid [17 18 19]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "1 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "1 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "2 [17 18 19]\n",
      "e [20 21 22 23]\n",
      "mid [20 21 22 23]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "0 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "1 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "2 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "3 [20 21 22 23]\n",
      "e [24 25 26 27]\n",
      "mid [24 25 26 27]\n",
      "M0 [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "1 [24 25 26 27]\n",
      "e [28 29 30]\n",
      "2 [24 25 26 27]\n",
      "e [28 29 30]\n",
      "3 [24 25 26 27]\n",
      "e [28 29 30]\n",
      "mid [28 29 30]\n",
      "M0 [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 1. 0.]]\n",
      "0 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "1 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "2 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "2 [28 29 30]\n",
      "e [31 32 33 34]\n",
      "mid [31 32 33 34]\n",
      "M0 [[1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "0 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "1 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "2 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "3 [31 32 33 34]\n",
      "e [35 36 37 38 39]\n",
      "mid [35 36 37 38 39]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "0 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "1 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "2 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "3 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "4 [35 36 37 38 39]\n",
      "e [40 41 42 43 44]\n",
      "mid [40 41 42 43 44]\n",
      "M0 [[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "0 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "1 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "2 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "3 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "3 [40 41 42 43 44]\n",
      "e [45 46 47 48 49]\n",
      "mid [45 46 47 48 49]\n",
      "M0 [[1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n",
      "0 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "1 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "2 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "3 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "4 [45 46 47 48 49]\n",
      "e [50 51 52 53 54 55]\n",
      "mid [50 51 52 53 54 55]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "1 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "2 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "3 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "3 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "4 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "5 [50 51 52 53 54 55]\n",
      "e [56 57 58 59 60 61 62]\n",
      "mid [56 57 58 59 60 61 62]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0.]]\n",
      "M1 [[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]]\n",
      "0 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "2 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "3 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "4 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "5 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "6 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "6 [56 57 58 59 60 61 62]\n",
      "e [63 64 65 66 67 68 69]\n",
      "mid [63 64 65 66 67 68 69]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "1 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "2 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "3 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "4 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "5 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "6 [63 64 65 66 67 68 69]\n",
      "e [70 71 72 73 74 75 76]\n",
      "mid [70 71 72 73 74 75 76]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "1 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "2 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "3 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "4 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "4 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "5 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "6 [70 71 72 73 74 75 76]\n",
      "e [77 78 79 80 81 82 83 84]\n",
      "mid [77 78 79 80 81 82 83 84]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "2 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "3 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "4 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "5 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "6 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "7 [77 78 79 80 81 82 83 84]\n",
      "e [85 86 87 88 89 90 91]\n",
      "mid [85 86 87 88 89 90 91]\n",
      "M0 [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0.]]\n",
      "0 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "1 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "2 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "3 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "4 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "5 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "6 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "6 [85 86 87 88 89 90 91]\n",
      "e [ 92  93  94  95  96  97  98  99 100]\n",
      "mid [ 92  93  94  95  96  97  98  99 100]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "1 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "2 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "3 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "4 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "5 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "7 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "8 [ 92  93  94  95  96  97  98  99 100]\n",
      "e [101 102 103 104 105 106 107]\n",
      "mid [101 102 103 104 105 106 107]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "M1 [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0.]]\n",
      "0 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "1 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "2 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "3 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "4 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "5 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "6 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "6 [101 102 103 104 105 106 107]\n",
      "e [108 109 110 111 112 113 114 115]\n",
      "mid [108 109 110 111 112 113 114 115]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "1 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "2 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "3 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "4 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "5 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "7 [108 109 110 111 112 113 114 115]\n",
      "e [116 117 118 119 120 121 122]\n",
      "mid [116 117 118 119 120 121 122]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "1 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "1 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "2 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "3 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "4 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "5 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "5 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "6 [116 117 118 119 120 121 122]\n",
      "e [123 124 125 126 127 128 129 130 131]\n",
      "mid [123 124 125 126 127 128 129 130 131]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "0 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "2 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "3 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "4 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "5 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "6 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "6 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "7 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "8 [123 124 125 126 127 128 129 130 131]\n",
      "e [132 133 134 135 136 137 138 139 140 141]\n",
      "mid [132 133 134 135 136 137 138 139 140 141]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "M1 [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "0 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "1 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "2 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "3 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "4 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "5 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "6 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "7 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "8 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "9 [132 133 134 135 136 137 138 139 140 141]\n",
      "e [142 143 144 145 146 147 148 149 150 151]\n",
      "mid [142 143 144 145 146 147 148 149 150 151]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "0 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "1 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "2 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "3 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "4 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "5 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "6 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "7 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "8 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "9 [142 143 144 145 146 147 148 149 150 151]\n",
      "e [152 153 154 155 156 157 158 159 160 161]\n",
      "mid [152 153 154 155 156 157 158 159 160 161]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "1 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "2 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "3 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "3 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "4 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "5 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "6 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "7 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "8 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "9 [152 153 154 155 156 157 158 159 160 161]\n",
      "e [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "mid [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "1 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "2 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "3 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "4 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "5 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "5 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "6 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "7 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "9 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "10 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "11 [162 163 164 165 166 167 168 169 170 171 172 173]\n",
      "e [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "mid [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "1 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "2 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "2 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "3 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "4 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "5 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "6 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "7 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "8 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "9 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "10 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "11 [174 175 176 177 178 179 180 181 182 183 184 185]\n",
      "e [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "mid [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "0 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "1 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "1 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "2 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "3 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "4 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "5 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "6 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "7 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "8 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "9 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "10 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "11 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "12 [186 187 188 189 190 191 192 193 194 195 196 197 198]\n",
      "e [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "mid [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "M0 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "M1 [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "0 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "1 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "2 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "3 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "4 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "4 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "5 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "6 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "7 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "8 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "9 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "10 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "11 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "12 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "13 [199 200 201 202 203 204 205 206 207 208 209 210 211 212]\n",
      "e [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n",
      "mid [213 214 215 216 217 218 219 220 221 222 223 224 225 226 227]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recon\n",
    "run=14\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "a=0.1\n",
    "pp_A = complete_postprocess(Ad,d,a)\n",
    "\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.596, Val loss: 3.768, Epoch time = 2.374s\n",
      "Epoch: 2, Train loss: 2.828, Val loss: 2.704, Epoch time = 3.196s\n",
      "Epoch: 3, Train loss: 3.165, Val loss: 2.762, Epoch time = 2.835s\n",
      "Epoch: 4, Train loss: 2.785, Val loss: 3.779, Epoch time = 2.928s\n",
      "Epoch: 5, Train loss: 2.871, Val loss: 2.652, Epoch time = 3.031s\n",
      "Epoch: 6, Train loss: 2.653, Val loss: 2.882, Epoch time = 2.964s\n",
      "Epoch: 7, Train loss: 2.643, Val loss: 3.090, Epoch time = 3.410s\n",
      "Epoch: 8, Train loss: 2.432, Val loss: 2.802, Epoch time = 3.017s\n",
      "Epoch: 9, Train loss: 2.753, Val loss: 2.826, Epoch time = 3.428s\n",
      "Epoch: 10, Train loss: 2.502, Val loss: 2.781, Epoch time = 3.050s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2a8e4e610>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 24 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 31, 24])\n",
      "(14, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mo/anaconda3/lib/python3.7/site-packages/umap/umap_.py:2345: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  \"n_neighbors is larger than the dataset size; truncating to \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack24.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=5\n",
    "t= 16\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:][1:]\n",
    "src_t2=src_t2[:,t,:][1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:][1:]\n",
    "b=b[:,t,:][1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lott\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
