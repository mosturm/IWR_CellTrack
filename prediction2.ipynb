{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch = [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            print('recon')\n",
    "        else:\n",
    "            E1,E2,A=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(0,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(0,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(0,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A) \n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    #print('l',loss)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen):\n",
    "        self.pen=pen\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            \n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        capacities = np.concatenate((np.ones(left_n)*2,np.ones(len(con_e)),np.ones(right_n)))\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        \n",
    "        \n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "                                    \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        tasks = right_n\n",
    "        supplies= tasks \n",
    "        supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        supplies=np.append(supplies,-tasks)\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies,tasks)\n",
    "        \n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[1,2,3]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2989,  0.1060, -0.3849,  0.0870, -0.0560,  0.0396,  0.1314, -0.0205,\n",
       "          -0.2284,  0.0301, -0.0564,  0.0535, -0.2161,  0.2185,  0.1809, -0.1231,\n",
       "           0.0745,  0.2609, -0.2124,  0.0797, -0.3811,  0.2696, -0.2651,  0.0625],\n",
       "         [-0.5921, -0.8265, -0.5655, -0.4372,  0.7434,  0.0848,  0.6664,  0.2394,\n",
       "          -0.6807,  0.4427,  0.0224,  0.5424,  0.0611,  0.1056,  0.4451, -0.8501,\n",
       "          -0.0047,  0.1626, -0.6274, -0.4652, -0.7067,  0.2744, -0.5523,  0.1458],\n",
       "         [-0.4943, -0.0417, -0.7102,  0.4135, -0.0712,  0.2063,  0.1424, -0.5491,\n",
       "          -1.3747,  0.6234,  0.4917, -0.3429,  0.0135, -0.3737, -0.0464, -0.0278,\n",
       "           0.8128,  0.6122, -0.1552, -0.1411, -0.5529,  0.2041,  0.1676,  0.5213],\n",
       "         [-0.5473, -0.2509, -0.6775, -0.4086,  0.2637,  0.0069,  0.5632, -0.1953,\n",
       "          -0.0500,  0.1437,  0.7105,  0.4545, -0.8521,  0.1403,  1.0465,  0.0202,\n",
       "          -0.3701,  0.1657, -0.2657, -0.2317, -0.7280,  0.0476, -0.7415,  0.6158],\n",
       "         [-0.3909, -0.2775, -0.8953, -0.3891, -0.3336,  0.1642,  0.4014, -0.2618,\n",
       "           0.0774,  0.6131,  0.4922,  0.4519, -0.8608,  0.3974,  1.0518,  0.2152,\n",
       "           0.1132,  0.3978,  0.0976, -0.3074, -0.3807,  0.2828, -0.7660,  0.5966]]),\n",
       " tensor([[-0.2989,  0.1060, -0.3849,  0.0870, -0.0560,  0.0396,  0.1314, -0.0205,\n",
       "          -0.2284,  0.0301, -0.0564,  0.0535, -0.2161,  0.2185,  0.1809, -0.1231,\n",
       "           0.0745,  0.2609, -0.2124,  0.0797, -0.3811,  0.2696, -0.2651,  0.0625],\n",
       "         [-0.4331, -0.2152, -0.7733, -0.2970, -0.0537,  0.0812,  0.1567, -0.2344,\n",
       "           0.0370,  0.6649,  0.4307,  0.3321, -0.9111,  0.4948,  1.1234,  0.1386,\n",
       "          -0.0047,  0.3912,  0.1027, -0.5307, -0.4453,  0.1771, -0.8420,  0.6161],\n",
       "         [-0.6326, -0.8244, -0.4791, -0.4424,  0.5759,  0.2048,  0.5780,  0.4286,\n",
       "          -0.7409,  0.1528, -0.0098,  0.5589, -0.0391,  0.1354,  0.6796, -0.8025,\n",
       "          -0.0433,  0.1641, -0.6855, -0.1161, -0.5257, -0.0213, -0.6069,  0.3221],\n",
       "         [-0.3861,  0.0137, -0.7497,  0.6006, -0.1295,  0.0814,  0.1395, -0.3927,\n",
       "          -1.2551,  0.6011,  0.5374, -0.4326, -0.2768, -0.4772, -0.0256, -0.0811,\n",
       "           0.6624,  0.6937, -0.1836, -0.0684, -0.5004,  0.3956,  0.3255,  0.7262],\n",
       "         [-0.5616, -0.4493, -0.4503, -0.3598,  0.3853,  0.1865,  0.6661,  0.1721,\n",
       "          -0.3107,  0.2126,  0.4601,  0.4435, -0.5495,  0.2474,  1.1025, -0.1620,\n",
       "          -0.2852,  0.2652, -0.3843, -0.1489, -0.7277, -0.1685, -0.8449,  0.6240],\n",
       "         [-0.6211, -0.7314, -0.4101, -0.4180,  0.4885,  0.2897,  0.6889,  0.4289,\n",
       "          -0.6192,  0.4050,  0.0790,  0.3472,  0.0436,  0.2436,  0.7291, -0.7014,\n",
       "          -0.1053,  0.1654, -0.5344, -0.1909, -0.6062,  0.0360, -0.6314,  0.6063]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 1.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[5, 6]])\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "loadgraph(run=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class AdjacencyTransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyTransformerDecoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = src1_emb\n",
    "        out2 = src2_emb\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        #out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 24 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.016, Val loss: 3.878, Epoch time = 1.513s\n",
      "Epoch: 2, Train loss: 4.086, Val loss: 3.798, Epoch time = 1.648s\n",
      "Epoch: 3, Train loss: 3.421, Val loss: 3.749, Epoch time = 1.870s\n",
      "Epoch: 4, Train loss: 3.681, Val loss: 3.731, Epoch time = 1.936s\n",
      "Epoch: 5, Train loss: 4.241, Val loss: 4.083, Epoch time = 1.832s\n",
      "Epoch: 6, Train loss: 4.128, Val loss: 3.722, Epoch time = 1.737s\n",
      "Epoch: 7, Train loss: 3.526, Val loss: 3.903, Epoch time = 1.805s\n",
      "Epoch: 8, Train loss: 4.159, Val loss: 3.769, Epoch time = 1.782s\n",
      "Epoch: 9, Train loss: 3.463, Val loss: 4.077, Epoch time = 1.946s\n",
      "Epoch: 10, Train loss: 3.934, Val loss: 4.085, Epoch time = 1.978s\n",
      "Epoch: 11, Train loss: 3.513, Val loss: 3.659, Epoch time = 2.039s\n",
      "Epoch: 12, Train loss: 3.373, Val loss: 4.080, Epoch time = 2.036s\n",
      "Epoch: 13, Train loss: 4.006, Val loss: 5.249, Epoch time = 2.045s\n",
      "Epoch: 14, Train loss: 3.817, Val loss: 3.778, Epoch time = 1.872s\n",
      "Epoch: 15, Train loss: 3.916, Val loss: 3.780, Epoch time = 2.167s\n",
      "Epoch: 16, Train loss: 3.347, Val loss: 3.800, Epoch time = 2.061s\n",
      "Epoch: 17, Train loss: 4.006, Val loss: 4.740, Epoch time = 1.990s\n",
      "Epoch: 18, Train loss: 3.864, Val loss: 3.743, Epoch time = 2.030s\n",
      "Epoch: 19, Train loss: 3.454, Val loss: 3.688, Epoch time = 2.009s\n",
      "Epoch: 20, Train loss: 4.321, Val loss: 3.941, Epoch time = 2.045s\n",
      "Epoch: 21, Train loss: 4.114, Val loss: 3.969, Epoch time = 1.999s\n",
      "Epoch: 22, Train loss: 3.864, Val loss: 3.622, Epoch time = 2.097s\n",
      "Epoch: 23, Train loss: 3.642, Val loss: 3.657, Epoch time = 2.027s\n",
      "Epoch: 24, Train loss: 3.612, Val loss: 4.051, Epoch time = 2.041s\n",
      "Epoch: 25, Train loss: 3.883, Val loss: 4.018, Epoch time = 2.060s\n",
      "Epoch: 26, Train loss: 4.157, Val loss: 3.579, Epoch time = 2.033s\n",
      "Epoch: 27, Train loss: 3.493, Val loss: 3.643, Epoch time = 1.946s\n",
      "Epoch: 28, Train loss: 4.173, Val loss: 3.595, Epoch time = 1.902s\n",
      "Epoch: 29, Train loss: 4.315, Val loss: 3.558, Epoch time = 1.880s\n",
      "Epoch: 30, Train loss: 3.370, Val loss: 3.438, Epoch time = 1.842s\n",
      "Epoch: 31, Train loss: 4.032, Val loss: 3.737, Epoch time = 1.920s\n",
      "Epoch: 32, Train loss: 4.131, Val loss: 3.630, Epoch time = 1.754s\n",
      "Epoch: 33, Train loss: 3.577, Val loss: 3.599, Epoch time = 1.930s\n",
      "Epoch: 34, Train loss: 3.772, Val loss: 3.911, Epoch time = 1.913s\n",
      "Epoch: 35, Train loss: 3.603, Val loss: 4.274, Epoch time = 1.947s\n",
      "Epoch: 36, Train loss: 3.551, Val loss: 3.721, Epoch time = 1.843s\n",
      "Epoch: 37, Train loss: 4.235, Val loss: 3.621, Epoch time = 1.873s\n",
      "Epoch: 38, Train loss: 4.021, Val loss: 3.659, Epoch time = 1.910s\n",
      "Epoch: 39, Train loss: 3.618, Val loss: 4.158, Epoch time = 1.918s\n",
      "Epoch: 40, Train loss: 3.261, Val loss: 3.566, Epoch time = 1.877s\n",
      "Epoch: 41, Train loss: 3.548, Val loss: 3.628, Epoch time = 1.843s\n",
      "Epoch: 42, Train loss: 3.455, Val loss: 3.648, Epoch time = 1.908s\n",
      "Epoch: 43, Train loss: 3.555, Val loss: 3.468, Epoch time = 1.954s\n",
      "Epoch: 44, Train loss: 4.124, Val loss: 5.103, Epoch time = 1.818s\n",
      "Epoch: 45, Train loss: 3.610, Val loss: 3.617, Epoch time = 1.844s\n",
      "Epoch: 46, Train loss: 3.996, Val loss: 3.595, Epoch time = 1.791s\n",
      "Epoch: 47, Train loss: 4.086, Val loss: 3.919, Epoch time = 1.830s\n",
      "Epoch: 48, Train loss: 4.028, Val loss: 4.126, Epoch time = 1.828s\n",
      "Epoch: 49, Train loss: 4.186, Val loss: 4.009, Epoch time = 1.722s\n",
      "Epoch: 50, Train loss: 3.841, Val loss: 3.688, Epoch time = 1.706s\n",
      "Epoch: 51, Train loss: 4.058, Val loss: 4.459, Epoch time = 1.723s\n",
      "Epoch: 52, Train loss: 4.614, Val loss: 3.706, Epoch time = 1.764s\n",
      "Epoch: 53, Train loss: 3.795, Val loss: 5.034, Epoch time = 1.806s\n",
      "Epoch: 54, Train loss: 3.948, Val loss: 3.728, Epoch time = 1.804s\n",
      "Epoch: 55, Train loss: 3.485, Val loss: 3.696, Epoch time = 1.757s\n",
      "Epoch: 56, Train loss: 3.545, Val loss: 3.887, Epoch time = 1.767s\n",
      "Epoch: 57, Train loss: 3.907, Val loss: 3.898, Epoch time = 1.773s\n",
      "Epoch: 58, Train loss: 3.496, Val loss: 4.094, Epoch time = 1.837s\n",
      "Epoch: 59, Train loss: 3.701, Val loss: 3.524, Epoch time = 1.977s\n",
      "Epoch: 60, Train loss: 3.630, Val loss: 3.768, Epoch time = 1.856s\n",
      "Epoch: 61, Train loss: 4.271, Val loss: 3.524, Epoch time = 1.810s\n",
      "Epoch: 62, Train loss: 3.313, Val loss: 3.685, Epoch time = 1.799s\n",
      "Epoch: 63, Train loss: 3.664, Val loss: 3.416, Epoch time = 1.887s\n",
      "Epoch: 64, Train loss: 3.953, Val loss: 3.451, Epoch time = 3.061s\n",
      "Epoch: 65, Train loss: 3.643, Val loss: 3.770, Epoch time = 2.414s\n",
      "Epoch: 66, Train loss: 4.054, Val loss: 3.958, Epoch time = 1.910s\n",
      "Epoch: 67, Train loss: 4.084, Val loss: 4.002, Epoch time = 1.789s\n",
      "Epoch: 68, Train loss: 3.476, Val loss: 3.675, Epoch time = 1.913s\n",
      "Epoch: 69, Train loss: 3.377, Val loss: 4.968, Epoch time = 2.366s\n",
      "Epoch: 70, Train loss: 3.948, Val loss: 3.600, Epoch time = 1.919s\n",
      "Epoch: 71, Train loss: 3.964, Val loss: 3.367, Epoch time = 1.927s\n",
      "Epoch: 72, Train loss: 3.595, Val loss: 3.811, Epoch time = 1.738s\n",
      "Epoch: 73, Train loss: 3.845, Val loss: 3.808, Epoch time = 1.976s\n",
      "Epoch: 74, Train loss: 3.574, Val loss: 3.579, Epoch time = 1.691s\n",
      "Epoch: 75, Train loss: 4.078, Val loss: 3.797, Epoch time = 1.763s\n",
      "Epoch: 76, Train loss: 3.879, Val loss: 3.742, Epoch time = 1.752s\n",
      "Epoch: 77, Train loss: 3.674, Val loss: 3.500, Epoch time = 1.760s\n",
      "Epoch: 78, Train loss: 4.251, Val loss: 3.738, Epoch time = 1.681s\n",
      "Epoch: 79, Train loss: 3.773, Val loss: 3.547, Epoch time = 1.710s\n",
      "Epoch: 80, Train loss: 3.786, Val loss: 3.539, Epoch time = 1.933s\n",
      "Epoch: 81, Train loss: 3.229, Val loss: 3.837, Epoch time = 1.764s\n",
      "Epoch: 82, Train loss: 3.219, Val loss: 3.836, Epoch time = 1.820s\n",
      "Epoch: 83, Train loss: 3.079, Val loss: 3.874, Epoch time = 2.333s\n",
      "Epoch: 84, Train loss: 3.366, Val loss: 3.667, Epoch time = 1.924s\n",
      "Epoch: 85, Train loss: 4.084, Val loss: 3.971, Epoch time = 1.866s\n",
      "Epoch: 86, Train loss: 3.814, Val loss: 3.366, Epoch time = 1.811s\n",
      "Epoch: 87, Train loss: 3.690, Val loss: 3.884, Epoch time = 1.781s\n",
      "Epoch: 88, Train loss: 4.095, Val loss: 3.876, Epoch time = 1.717s\n",
      "Epoch: 89, Train loss: 3.955, Val loss: 3.596, Epoch time = 1.734s\n",
      "Epoch: 90, Train loss: 4.092, Val loss: 3.334, Epoch time = 1.748s\n",
      "Epoch: 91, Train loss: 2.840, Val loss: 3.798, Epoch time = 1.862s\n",
      "Epoch: 92, Train loss: 3.682, Val loss: 3.319, Epoch time = 1.804s\n",
      "Epoch: 93, Train loss: 3.440, Val loss: 3.802, Epoch time = 1.767s\n",
      "Epoch: 94, Train loss: 3.867, Val loss: 3.433, Epoch time = 1.745s\n",
      "Epoch: 95, Train loss: 3.849, Val loss: 3.867, Epoch time = 1.887s\n",
      "Epoch: 96, Train loss: 3.661, Val loss: 3.687, Epoch time = 1.806s\n",
      "Epoch: 97, Train loss: 3.772, Val loss: 3.560, Epoch time = 1.715s\n",
      "Epoch: 98, Train loss: 3.883, Val loss: 3.376, Epoch time = 1.743s\n",
      "Epoch: 99, Train loss: 3.459, Val loss: 3.805, Epoch time = 1.684s\n",
      "Epoch: 100, Train loss: 3.650, Val loss: 3.664, Epoch time = 1.688s\n",
      "Epoch: 101, Train loss: 4.210, Val loss: 3.622, Epoch time = 1.690s\n",
      "Epoch: 102, Train loss: 3.483, Val loss: 4.240, Epoch time = 1.794s\n",
      "Epoch: 103, Train loss: 3.103, Val loss: 3.459, Epoch time = 1.896s\n",
      "Epoch: 104, Train loss: 3.801, Val loss: 4.003, Epoch time = 1.967s\n",
      "Epoch: 105, Train loss: 3.596, Val loss: 3.802, Epoch time = 1.768s\n",
      "Epoch: 106, Train loss: 3.794, Val loss: 3.614, Epoch time = 1.853s\n",
      "Epoch: 107, Train loss: 3.647, Val loss: 3.167, Epoch time = 1.889s\n",
      "Epoch: 108, Train loss: 3.853, Val loss: 3.561, Epoch time = 1.821s\n",
      "Epoch: 109, Train loss: 3.836, Val loss: 3.344, Epoch time = 1.799s\n",
      "Epoch: 110, Train loss: 4.100, Val loss: 3.517, Epoch time = 1.946s\n",
      "Epoch: 111, Train loss: 3.962, Val loss: 3.697, Epoch time = 1.855s\n",
      "Epoch: 112, Train loss: 4.107, Val loss: 3.343, Epoch time = 1.978s\n",
      "Epoch: 113, Train loss: 3.637, Val loss: 3.244, Epoch time = 1.760s\n",
      "Epoch: 114, Train loss: 3.511, Val loss: 3.423, Epoch time = 2.079s\n",
      "Epoch: 115, Train loss: 3.453, Val loss: 4.587, Epoch time = 2.312s\n",
      "Epoch: 116, Train loss: 3.404, Val loss: 3.939, Epoch time = 1.791s\n",
      "Epoch: 117, Train loss: 4.011, Val loss: 3.719, Epoch time = 1.989s\n",
      "Epoch: 118, Train loss: 3.377, Val loss: 3.881, Epoch time = 1.807s\n",
      "Epoch: 119, Train loss: 4.191, Val loss: 3.262, Epoch time = 1.747s\n",
      "Epoch: 120, Train loss: 3.518, Val loss: 4.425, Epoch time = 1.867s\n",
      "Epoch: 121, Train loss: 3.748, Val loss: 3.448, Epoch time = 2.604s\n",
      "Epoch: 122, Train loss: 3.591, Val loss: 3.471, Epoch time = 1.947s\n",
      "Epoch: 123, Train loss: 3.007, Val loss: 3.359, Epoch time = 1.975s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train loss: 3.105, Val loss: 3.698, Epoch time = 1.900s\n",
      "Epoch: 125, Train loss: 3.237, Val loss: 3.679, Epoch time = 1.936s\n",
      "Epoch: 126, Train loss: 3.846, Val loss: 3.649, Epoch time = 1.832s\n",
      "Epoch: 127, Train loss: 3.809, Val loss: 4.197, Epoch time = 1.820s\n",
      "Epoch: 128, Train loss: 3.079, Val loss: 3.307, Epoch time = 1.855s\n",
      "Epoch: 129, Train loss: 3.851, Val loss: 3.273, Epoch time = 1.817s\n",
      "Epoch: 130, Train loss: 3.988, Val loss: 3.688, Epoch time = 1.834s\n",
      "Epoch: 131, Train loss: 3.265, Val loss: 3.407, Epoch time = 1.753s\n",
      "Epoch: 132, Train loss: 3.594, Val loss: 3.678, Epoch time = 1.788s\n",
      "Epoch: 133, Train loss: 3.909, Val loss: 3.541, Epoch time = 1.636s\n",
      "Epoch: 134, Train loss: 3.352, Val loss: 3.274, Epoch time = 1.717s\n",
      "Epoch: 135, Train loss: 3.014, Val loss: 3.357, Epoch time = 1.859s\n",
      "Epoch: 136, Train loss: 3.721, Val loss: 4.118, Epoch time = 1.698s\n",
      "Epoch: 137, Train loss: 3.881, Val loss: 3.272, Epoch time = 1.678s\n",
      "Epoch: 138, Train loss: 3.934, Val loss: 3.931, Epoch time = 1.685s\n",
      "Epoch: 139, Train loss: 3.106, Val loss: 3.305, Epoch time = 1.924s\n",
      "Epoch: 140, Train loss: 3.438, Val loss: 3.578, Epoch time = 1.847s\n",
      "Epoch: 141, Train loss: 3.980, Val loss: 3.567, Epoch time = 1.702s\n",
      "Epoch: 142, Train loss: 4.079, Val loss: 3.555, Epoch time = 1.733s\n",
      "Epoch: 143, Train loss: 3.814, Val loss: 4.113, Epoch time = 1.926s\n",
      "Epoch: 144, Train loss: 3.788, Val loss: 3.723, Epoch time = 1.759s\n",
      "Epoch: 145, Train loss: 3.552, Val loss: 3.584, Epoch time = 1.741s\n",
      "Epoch: 146, Train loss: 3.402, Val loss: 3.865, Epoch time = 1.745s\n",
      "Epoch: 147, Train loss: 4.246, Val loss: 3.438, Epoch time = 1.731s\n",
      "Epoch: 148, Train loss: 3.539, Val loss: 3.607, Epoch time = 1.711s\n",
      "Epoch: 149, Train loss: 3.870, Val loss: 3.826, Epoch time = 1.835s\n",
      "Epoch: 150, Train loss: 3.678, Val loss: 3.400, Epoch time = 1.769s\n",
      "Epoch: 151, Train loss: 3.262, Val loss: 3.626, Epoch time = 1.732s\n",
      "Epoch: 152, Train loss: 3.784, Val loss: 3.193, Epoch time = 1.739s\n",
      "Epoch: 153, Train loss: 3.176, Val loss: 3.199, Epoch time = 1.683s\n",
      "Epoch: 154, Train loss: 3.369, Val loss: 3.565, Epoch time = 1.904s\n",
      "Epoch: 155, Train loss: 3.457, Val loss: 3.542, Epoch time = 1.919s\n",
      "Epoch: 156, Train loss: 4.073, Val loss: 3.858, Epoch time = 1.882s\n",
      "Epoch: 157, Train loss: 3.936, Val loss: 3.940, Epoch time = 1.874s\n",
      "Epoch: 158, Train loss: 3.520, Val loss: 3.591, Epoch time = 1.858s\n",
      "Epoch: 159, Train loss: 3.640, Val loss: 3.802, Epoch time = 1.927s\n",
      "Epoch: 160, Train loss: 3.450, Val loss: 3.476, Epoch time = 1.803s\n",
      "Epoch: 161, Train loss: 3.024, Val loss: 3.451, Epoch time = 1.692s\n",
      "Epoch: 162, Train loss: 3.848, Val loss: 3.655, Epoch time = 1.718s\n",
      "Epoch: 163, Train loss: 3.225, Val loss: 3.794, Epoch time = 1.691s\n",
      "Epoch: 164, Train loss: 3.590, Val loss: 3.154, Epoch time = 1.760s\n",
      "Epoch: 165, Train loss: 3.851, Val loss: 3.280, Epoch time = 1.703s\n",
      "Epoch: 166, Train loss: 4.131, Val loss: 3.669, Epoch time = 1.806s\n",
      "Epoch: 167, Train loss: 3.696, Val loss: 3.635, Epoch time = 1.894s\n",
      "Epoch: 168, Train loss: 3.495, Val loss: 3.552, Epoch time = 1.789s\n",
      "Epoch: 169, Train loss: 3.677, Val loss: 3.161, Epoch time = 1.719s\n",
      "Epoch: 170, Train loss: 3.948, Val loss: 3.516, Epoch time = 1.777s\n",
      "Epoch: 171, Train loss: 3.870, Val loss: 3.041, Epoch time = 1.861s\n",
      "Epoch: 172, Train loss: 3.066, Val loss: 3.306, Epoch time = 1.784s\n",
      "Epoch: 173, Train loss: 4.339, Val loss: 3.651, Epoch time = 1.790s\n",
      "Epoch: 174, Train loss: 3.420, Val loss: 3.815, Epoch time = 1.814s\n",
      "Epoch: 175, Train loss: 3.294, Val loss: 4.193, Epoch time = 1.792s\n",
      "Epoch: 176, Train loss: 3.581, Val loss: 3.728, Epoch time = 1.729s\n",
      "Epoch: 177, Train loss: 3.612, Val loss: 3.405, Epoch time = 1.753s\n",
      "Epoch: 178, Train loss: 4.131, Val loss: 4.068, Epoch time = 2.564s\n",
      "Epoch: 179, Train loss: 3.778, Val loss: 3.663, Epoch time = 2.276s\n",
      "Epoch: 180, Train loss: 3.373, Val loss: 3.566, Epoch time = 1.901s\n",
      "Epoch: 181, Train loss: 3.355, Val loss: 3.778, Epoch time = 1.872s\n",
      "Epoch: 182, Train loss: 3.295, Val loss: 3.013, Epoch time = 1.837s\n",
      "Epoch: 183, Train loss: 3.603, Val loss: 3.541, Epoch time = 1.751s\n",
      "Epoch: 184, Train loss: 3.452, Val loss: 3.546, Epoch time = 1.831s\n",
      "Epoch: 185, Train loss: 3.109, Val loss: 3.337, Epoch time = 1.955s\n",
      "Epoch: 186, Train loss: 3.877, Val loss: 3.371, Epoch time = 2.231s\n",
      "Epoch: 187, Train loss: 3.611, Val loss: 3.643, Epoch time = 2.304s\n",
      "Epoch: 188, Train loss: 3.398, Val loss: 3.499, Epoch time = 1.915s\n",
      "Epoch: 189, Train loss: 3.179, Val loss: 3.459, Epoch time = 1.798s\n",
      "Epoch: 190, Train loss: 3.376, Val loss: 3.216, Epoch time = 2.733s\n",
      "Epoch: 191, Train loss: 3.219, Val loss: 3.585, Epoch time = 1.979s\n",
      "Epoch: 192, Train loss: 3.086, Val loss: 3.442, Epoch time = 1.807s\n",
      "Epoch: 193, Train loss: 3.477, Val loss: 3.557, Epoch time = 1.823s\n",
      "Epoch: 194, Train loss: 4.193, Val loss: 3.393, Epoch time = 1.957s\n",
      "Epoch: 195, Train loss: 3.478, Val loss: 3.402, Epoch time = 1.762s\n",
      "Epoch: 196, Train loss: 3.118, Val loss: 3.182, Epoch time = 1.846s\n",
      "Epoch: 197, Train loss: 3.918, Val loss: 3.698, Epoch time = 1.844s\n",
      "Epoch: 198, Train loss: 3.544, Val loss: 3.618, Epoch time = 1.880s\n",
      "Epoch: 199, Train loss: 3.496, Val loss: 3.164, Epoch time = 1.830s\n",
      "Epoch: 200, Train loss: 3.574, Val loss: 3.529, Epoch time = 2.120s\n",
      "Epoch: 201, Train loss: 3.023, Val loss: 3.616, Epoch time = 2.052s\n",
      "Epoch: 202, Train loss: 3.684, Val loss: 3.364, Epoch time = 1.771s\n",
      "Epoch: 203, Train loss: 3.570, Val loss: 4.087, Epoch time = 1.843s\n",
      "Epoch: 204, Train loss: 3.698, Val loss: 3.388, Epoch time = 1.945s\n",
      "Epoch: 205, Train loss: 3.752, Val loss: 3.536, Epoch time = 1.824s\n",
      "Epoch: 206, Train loss: 3.488, Val loss: 3.233, Epoch time = 1.842s\n",
      "Epoch: 207, Train loss: 3.250, Val loss: 3.320, Epoch time = 2.008s\n",
      "Epoch: 208, Train loss: 3.326, Val loss: 3.769, Epoch time = 1.742s\n",
      "Epoch: 209, Train loss: 3.403, Val loss: 3.411, Epoch time = 1.870s\n",
      "Epoch: 210, Train loss: 3.631, Val loss: 3.840, Epoch time = 1.879s\n",
      "Epoch: 211, Train loss: 3.443, Val loss: 3.390, Epoch time = 1.882s\n",
      "Epoch: 212, Train loss: 3.205, Val loss: 3.303, Epoch time = 1.769s\n",
      "Epoch: 213, Train loss: 4.055, Val loss: 3.235, Epoch time = 1.742s\n",
      "Epoch: 214, Train loss: 3.781, Val loss: 3.384, Epoch time = 1.802s\n",
      "Epoch: 215, Train loss: 3.694, Val loss: 3.257, Epoch time = 1.880s\n",
      "Epoch: 216, Train loss: 3.910, Val loss: 4.551, Epoch time = 1.784s\n",
      "Epoch: 217, Train loss: 3.614, Val loss: 3.200, Epoch time = 1.873s\n",
      "Epoch: 218, Train loss: 3.188, Val loss: 3.158, Epoch time = 1.704s\n",
      "Epoch: 219, Train loss: 3.598, Val loss: 4.051, Epoch time = 1.689s\n",
      "Epoch: 220, Train loss: 3.566, Val loss: 3.462, Epoch time = 1.751s\n",
      "Epoch: 221, Train loss: 4.002, Val loss: 3.610, Epoch time = 1.848s\n",
      "Epoch: 222, Train loss: 3.358, Val loss: 3.465, Epoch time = 1.772s\n",
      "Epoch: 223, Train loss: 2.958, Val loss: 3.647, Epoch time = 1.857s\n",
      "Epoch: 224, Train loss: 3.400, Val loss: 3.312, Epoch time = 1.856s\n",
      "Epoch: 225, Train loss: 3.300, Val loss: 3.346, Epoch time = 1.995s\n",
      "Epoch: 226, Train loss: 3.365, Val loss: 3.159, Epoch time = 1.828s\n",
      "Epoch: 227, Train loss: 3.595, Val loss: 3.107, Epoch time = 1.776s\n",
      "Epoch: 228, Train loss: 3.142, Val loss: 3.121, Epoch time = 1.740s\n",
      "Epoch: 229, Train loss: 3.299, Val loss: 3.011, Epoch time = 1.761s\n",
      "Epoch: 230, Train loss: 3.141, Val loss: 4.390, Epoch time = 1.793s\n",
      "Epoch: 231, Train loss: 3.748, Val loss: 3.200, Epoch time = 2.350s\n",
      "Epoch: 232, Train loss: 3.233, Val loss: 3.515, Epoch time = 1.897s\n",
      "Epoch: 233, Train loss: 3.087, Val loss: 3.551, Epoch time = 1.809s\n",
      "Epoch: 234, Train loss: 3.704, Val loss: 3.340, Epoch time = 1.753s\n",
      "Epoch: 235, Train loss: 2.967, Val loss: 3.345, Epoch time = 1.820s\n",
      "Epoch: 236, Train loss: 2.955, Val loss: 3.574, Epoch time = 1.767s\n",
      "Epoch: 237, Train loss: 3.806, Val loss: 3.655, Epoch time = 1.740s\n",
      "Epoch: 238, Train loss: 3.562, Val loss: 3.352, Epoch time = 1.941s\n",
      "Epoch: 239, Train loss: 3.230, Val loss: 3.388, Epoch time = 1.812s\n",
      "Epoch: 240, Train loss: 3.709, Val loss: 3.425, Epoch time = 1.725s\n",
      "Epoch: 241, Train loss: 3.660, Val loss: 3.640, Epoch time = 1.698s\n",
      "Epoch: 242, Train loss: 2.903, Val loss: 2.962, Epoch time = 1.715s\n",
      "Epoch: 243, Train loss: 3.333, Val loss: 3.316, Epoch time = 1.780s\n",
      "Epoch: 244, Train loss: 3.342, Val loss: 3.301, Epoch time = 1.767s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245, Train loss: 3.383, Val loss: 3.525, Epoch time = 1.814s\n",
      "Epoch: 246, Train loss: 2.944, Val loss: 3.456, Epoch time = 1.832s\n",
      "Epoch: 247, Train loss: 3.613, Val loss: 4.675, Epoch time = 1.771s\n",
      "Epoch: 248, Train loss: 3.738, Val loss: 3.664, Epoch time = 1.901s\n",
      "Epoch: 249, Train loss: 3.408, Val loss: 3.371, Epoch time = 1.834s\n",
      "Epoch: 250, Train loss: 3.667, Val loss: 3.396, Epoch time = 1.778s\n",
      "Epoch: 251, Train loss: 3.737, Val loss: 3.109, Epoch time = 1.815s\n",
      "Epoch: 252, Train loss: 3.443, Val loss: 3.262, Epoch time = 1.715s\n",
      "Epoch: 253, Train loss: 3.296, Val loss: 3.109, Epoch time = 2.348s\n",
      "Epoch: 254, Train loss: 3.056, Val loss: 3.463, Epoch time = 1.745s\n",
      "Epoch: 255, Train loss: 3.655, Val loss: 3.665, Epoch time = 1.922s\n",
      "Epoch: 256, Train loss: 3.307, Val loss: 3.070, Epoch time = 1.789s\n",
      "Epoch: 257, Train loss: 3.143, Val loss: 3.152, Epoch time = 1.789s\n",
      "Epoch: 258, Train loss: 3.406, Val loss: 3.391, Epoch time = 1.717s\n",
      "Epoch: 259, Train loss: 3.349, Val loss: 3.341, Epoch time = 2.020s\n",
      "Epoch: 260, Train loss: 3.496, Val loss: 3.404, Epoch time = 1.843s\n",
      "Epoch: 261, Train loss: 3.815, Val loss: 3.573, Epoch time = 1.737s\n",
      "Epoch: 262, Train loss: 3.517, Val loss: 3.111, Epoch time = 1.858s\n",
      "Epoch: 263, Train loss: 3.209, Val loss: 3.939, Epoch time = 1.827s\n",
      "Epoch: 264, Train loss: 3.220, Val loss: 3.571, Epoch time = 1.964s\n",
      "Epoch: 265, Train loss: 3.232, Val loss: 3.427, Epoch time = 1.712s\n",
      "Epoch: 266, Train loss: 3.596, Val loss: 3.508, Epoch time = 1.854s\n",
      "Epoch: 267, Train loss: 3.097, Val loss: 3.257, Epoch time = 1.737s\n",
      "Epoch: 268, Train loss: 3.262, Val loss: 3.333, Epoch time = 1.755s\n",
      "Epoch: 269, Train loss: 3.653, Val loss: 3.234, Epoch time = 1.735s\n",
      "Epoch: 270, Train loss: 3.557, Val loss: 3.661, Epoch time = 1.985s\n",
      "Epoch: 271, Train loss: 3.131, Val loss: 3.463, Epoch time = 1.825s\n",
      "Epoch: 272, Train loss: 3.577, Val loss: 3.418, Epoch time = 1.920s\n",
      "Epoch: 273, Train loss: 3.376, Val loss: 3.953, Epoch time = 1.745s\n",
      "Epoch: 274, Train loss: 3.424, Val loss: 3.427, Epoch time = 1.707s\n",
      "Epoch: 275, Train loss: 3.418, Val loss: 3.559, Epoch time = 1.774s\n",
      "Epoch: 276, Train loss: 3.534, Val loss: 4.212, Epoch time = 2.001s\n",
      "Epoch: 277, Train loss: 3.496, Val loss: 3.709, Epoch time = 2.211s\n",
      "Epoch: 278, Train loss: 3.094, Val loss: 3.535, Epoch time = 1.766s\n",
      "Epoch: 279, Train loss: 3.699, Val loss: 3.605, Epoch time = 1.733s\n",
      "Epoch: 280, Train loss: 2.977, Val loss: 3.499, Epoch time = 2.136s\n",
      "Epoch: 281, Train loss: 3.329, Val loss: 3.225, Epoch time = 2.053s\n",
      "Epoch: 282, Train loss: 3.762, Val loss: 3.294, Epoch time = 2.537s\n",
      "Epoch: 283, Train loss: 3.107, Val loss: 3.273, Epoch time = 1.888s\n",
      "Epoch: 284, Train loss: 4.216, Val loss: 3.416, Epoch time = 1.741s\n",
      "Epoch: 285, Train loss: 3.396, Val loss: 2.898, Epoch time = 1.816s\n",
      "Epoch: 286, Train loss: 3.339, Val loss: 3.988, Epoch time = 1.703s\n",
      "Epoch: 287, Train loss: 3.054, Val loss: 3.393, Epoch time = 1.804s\n",
      "Epoch: 288, Train loss: 3.493, Val loss: 3.739, Epoch time = 1.808s\n",
      "Epoch: 289, Train loss: 3.387, Val loss: 3.339, Epoch time = 1.692s\n",
      "Epoch: 290, Train loss: 3.691, Val loss: 3.479, Epoch time = 1.792s\n",
      "Epoch: 291, Train loss: 3.152, Val loss: 3.336, Epoch time = 1.850s\n",
      "Epoch: 292, Train loss: 3.177, Val loss: 3.254, Epoch time = 1.928s\n",
      "Epoch: 293, Train loss: 4.077, Val loss: 3.027, Epoch time = 1.813s\n",
      "Epoch: 294, Train loss: 3.064, Val loss: 3.511, Epoch time = 1.717s\n",
      "Epoch: 295, Train loss: 3.435, Val loss: 3.467, Epoch time = 1.751s\n",
      "Epoch: 296, Train loss: 3.589, Val loss: 3.280, Epoch time = 1.724s\n",
      "Epoch: 297, Train loss: 2.877, Val loss: 4.030, Epoch time = 1.902s\n",
      "Epoch: 298, Train loss: 3.111, Val loss: 3.510, Epoch time = 1.973s\n",
      "Epoch: 299, Train loss: 3.329, Val loss: 3.300, Epoch time = 1.829s\n",
      "Epoch: 300, Train loss: 3.344, Val loss: 3.570, Epoch time = 1.863s\n",
      "Epoch: 301, Train loss: 3.684, Val loss: 3.381, Epoch time = 1.942s\n",
      "Epoch: 302, Train loss: 3.470, Val loss: 3.109, Epoch time = 1.919s\n",
      "Epoch: 303, Train loss: 4.039, Val loss: 3.183, Epoch time = 1.894s\n",
      "Epoch: 304, Train loss: 3.697, Val loss: 3.433, Epoch time = 1.824s\n",
      "Epoch: 305, Train loss: 3.120, Val loss: 3.270, Epoch time = 2.055s\n",
      "Epoch: 306, Train loss: 3.832, Val loss: 3.080, Epoch time = 1.754s\n",
      "Epoch: 307, Train loss: 3.189, Val loss: 3.368, Epoch time = 2.630s\n",
      "Epoch: 308, Train loss: 3.612, Val loss: 3.213, Epoch time = 1.818s\n",
      "Epoch: 309, Train loss: 3.288, Val loss: 3.127, Epoch time = 1.907s\n",
      "Epoch: 310, Train loss: 3.012, Val loss: 3.652, Epoch time = 2.079s\n",
      "Epoch: 311, Train loss: 3.605, Val loss: 3.294, Epoch time = 1.938s\n",
      "Epoch: 312, Train loss: 3.537, Val loss: 3.196, Epoch time = 1.836s\n",
      "Epoch: 313, Train loss: 3.685, Val loss: 3.344, Epoch time = 2.013s\n",
      "Epoch: 314, Train loss: 4.292, Val loss: 4.019, Epoch time = 2.064s\n",
      "Epoch: 315, Train loss: 3.569, Val loss: 3.106, Epoch time = 1.816s\n",
      "Epoch: 316, Train loss: 3.737, Val loss: 3.375, Epoch time = 1.889s\n",
      "Epoch: 317, Train loss: 3.723, Val loss: 3.362, Epoch time = 2.256s\n",
      "Epoch: 318, Train loss: 3.338, Val loss: 3.474, Epoch time = 1.998s\n",
      "Epoch: 319, Train loss: 3.532, Val loss: 3.144, Epoch time = 1.941s\n",
      "Epoch: 320, Train loss: 3.465, Val loss: 3.467, Epoch time = 2.002s\n",
      "Epoch: 321, Train loss: 3.635, Val loss: 3.160, Epoch time = 1.960s\n",
      "Epoch: 322, Train loss: 3.018, Val loss: 3.305, Epoch time = 1.862s\n",
      "Epoch: 323, Train loss: 3.498, Val loss: 3.158, Epoch time = 1.815s\n",
      "Epoch: 324, Train loss: 3.844, Val loss: 3.165, Epoch time = 1.804s\n",
      "Epoch: 325, Train loss: 3.458, Val loss: 3.485, Epoch time = 1.868s\n",
      "Epoch: 326, Train loss: 3.422, Val loss: 3.396, Epoch time = 1.775s\n",
      "Epoch: 327, Train loss: 3.406, Val loss: 3.206, Epoch time = 1.789s\n",
      "Epoch: 328, Train loss: 3.072, Val loss: 3.238, Epoch time = 1.721s\n",
      "Epoch: 329, Train loss: 3.873, Val loss: 3.098, Epoch time = 1.841s\n",
      "Epoch: 330, Train loss: 3.244, Val loss: 3.179, Epoch time = 1.767s\n",
      "Epoch: 331, Train loss: 3.378, Val loss: 3.183, Epoch time = 1.876s\n",
      "Epoch: 332, Train loss: 3.198, Val loss: 3.338, Epoch time = 1.888s\n",
      "Epoch: 333, Train loss: 3.501, Val loss: 3.202, Epoch time = 1.831s\n",
      "Epoch: 334, Train loss: 3.366, Val loss: 3.217, Epoch time = 1.986s\n",
      "Epoch: 335, Train loss: 2.957, Val loss: 3.476, Epoch time = 1.840s\n",
      "Epoch: 336, Train loss: 3.205, Val loss: 3.440, Epoch time = 1.824s\n",
      "Epoch: 337, Train loss: 3.440, Val loss: 3.435, Epoch time = 1.756s\n",
      "Epoch: 338, Train loss: 4.152, Val loss: 3.386, Epoch time = 1.995s\n",
      "Epoch: 339, Train loss: 3.514, Val loss: 3.251, Epoch time = 1.735s\n",
      "Epoch: 340, Train loss: 2.757, Val loss: 3.254, Epoch time = 1.792s\n",
      "Epoch: 341, Train loss: 3.025, Val loss: 3.447, Epoch time = 1.757s\n",
      "Epoch: 342, Train loss: 3.590, Val loss: 3.461, Epoch time = 1.781s\n",
      "Epoch: 343, Train loss: 3.828, Val loss: 3.510, Epoch time = 2.045s\n",
      "Epoch: 344, Train loss: 4.050, Val loss: 3.166, Epoch time = 1.993s\n",
      "Epoch: 345, Train loss: 3.190, Val loss: 3.289, Epoch time = 2.010s\n",
      "Epoch: 346, Train loss: 3.511, Val loss: 3.570, Epoch time = 1.855s\n",
      "Epoch: 347, Train loss: 3.295, Val loss: 3.100, Epoch time = 1.896s\n",
      "Epoch: 348, Train loss: 3.320, Val loss: 3.714, Epoch time = 1.853s\n",
      "Epoch: 349, Train loss: 3.203, Val loss: 3.327, Epoch time = 1.851s\n",
      "Epoch: 350, Train loss: 3.535, Val loss: 3.356, Epoch time = 1.922s\n",
      "Epoch: 351, Train loss: 3.774, Val loss: 3.283, Epoch time = 1.992s\n",
      "Epoch: 352, Train loss: 2.990, Val loss: 3.282, Epoch time = 2.777s\n",
      "Epoch: 353, Train loss: 2.972, Val loss: 3.370, Epoch time = 2.821s\n",
      "Epoch: 354, Train loss: 3.512, Val loss: 3.213, Epoch time = 2.137s\n",
      "Epoch: 355, Train loss: 3.260, Val loss: 3.454, Epoch time = 1.963s\n",
      "Epoch: 356, Train loss: 3.516, Val loss: 3.292, Epoch time = 1.950s\n",
      "Epoch: 357, Train loss: 3.225, Val loss: 3.174, Epoch time = 1.967s\n",
      "Epoch: 358, Train loss: 3.571, Val loss: 3.497, Epoch time = 2.893s\n",
      "Epoch: 359, Train loss: 3.404, Val loss: 3.271, Epoch time = 1.815s\n",
      "Epoch: 360, Train loss: 3.282, Val loss: 3.400, Epoch time = 1.845s\n",
      "Epoch: 361, Train loss: 3.311, Val loss: 3.431, Epoch time = 1.990s\n",
      "Epoch: 362, Train loss: 3.357, Val loss: 3.169, Epoch time = 1.876s\n",
      "Epoch: 363, Train loss: 3.602, Val loss: 3.820, Epoch time = 1.786s\n",
      "Epoch: 364, Train loss: 3.814, Val loss: 3.636, Epoch time = 1.832s\n",
      "Epoch: 365, Train loss: 3.839, Val loss: 3.312, Epoch time = 1.814s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366, Train loss: 3.443, Val loss: 3.218, Epoch time = 1.709s\n",
      "Epoch: 367, Train loss: 2.966, Val loss: 3.232, Epoch time = 1.826s\n",
      "Epoch: 368, Train loss: 2.966, Val loss: 3.049, Epoch time = 1.763s\n",
      "Epoch: 369, Train loss: 3.386, Val loss: 3.414, Epoch time = 1.684s\n",
      "Epoch: 370, Train loss: 3.576, Val loss: 3.530, Epoch time = 1.766s\n",
      "Epoch: 371, Train loss: 3.142, Val loss: 3.444, Epoch time = 1.781s\n",
      "Epoch: 372, Train loss: 3.527, Val loss: 3.012, Epoch time = 1.789s\n",
      "Epoch: 373, Train loss: 3.689, Val loss: 3.090, Epoch time = 1.742s\n",
      "Epoch: 374, Train loss: 3.594, Val loss: 3.499, Epoch time = 1.839s\n",
      "Epoch: 375, Train loss: 3.095, Val loss: 3.183, Epoch time = 1.914s\n",
      "Epoch: 376, Train loss: 3.303, Val loss: 3.524, Epoch time = 1.962s\n",
      "Epoch: 377, Train loss: 3.471, Val loss: 3.497, Epoch time = 1.884s\n",
      "Epoch: 378, Train loss: 3.586, Val loss: 3.315, Epoch time = 1.908s\n",
      "Epoch: 379, Train loss: 3.925, Val loss: 3.445, Epoch time = 2.007s\n",
      "Epoch: 380, Train loss: 3.092, Val loss: 3.068, Epoch time = 2.006s\n",
      "Epoch: 381, Train loss: 4.067, Val loss: 3.168, Epoch time = 1.894s\n",
      "Epoch: 382, Train loss: 3.082, Val loss: 3.105, Epoch time = 1.910s\n",
      "Epoch: 383, Train loss: 2.866, Val loss: 3.574, Epoch time = 1.977s\n",
      "Epoch: 384, Train loss: 3.408, Val loss: 3.069, Epoch time = 1.885s\n",
      "Epoch: 385, Train loss: 3.062, Val loss: 3.091, Epoch time = 1.869s\n",
      "Epoch: 386, Train loss: 2.962, Val loss: 3.215, Epoch time = 1.928s\n",
      "Epoch: 387, Train loss: 3.591, Val loss: 3.150, Epoch time = 1.926s\n",
      "Epoch: 388, Train loss: 3.409, Val loss: 3.175, Epoch time = 1.977s\n",
      "Epoch: 389, Train loss: 3.416, Val loss: 3.118, Epoch time = 2.737s\n",
      "Epoch: 390, Train loss: 3.578, Val loss: 3.368, Epoch time = 1.999s\n",
      "Epoch: 391, Train loss: 3.286, Val loss: 2.879, Epoch time = 1.926s\n",
      "Epoch: 392, Train loss: 3.206, Val loss: 3.097, Epoch time = 2.192s\n",
      "Epoch: 393, Train loss: 3.716, Val loss: 2.955, Epoch time = 2.556s\n",
      "Epoch: 394, Train loss: 3.593, Val loss: 3.140, Epoch time = 2.188s\n",
      "Epoch: 395, Train loss: 3.540, Val loss: 3.553, Epoch time = 1.890s\n",
      "Epoch: 396, Train loss: 2.996, Val loss: 3.144, Epoch time = 1.805s\n",
      "Epoch: 397, Train loss: 3.146, Val loss: 3.540, Epoch time = 1.874s\n",
      "Epoch: 398, Train loss: 2.924, Val loss: 3.027, Epoch time = 2.249s\n",
      "Epoch: 399, Train loss: 3.540, Val loss: 3.212, Epoch time = 1.738s\n",
      "Epoch: 400, Train loss: 2.720, Val loss: 4.377, Epoch time = 1.832s\n",
      "Epoch: 401, Train loss: 3.305, Val loss: 3.200, Epoch time = 1.747s\n",
      "Epoch: 402, Train loss: 3.952, Val loss: 3.115, Epoch time = 1.693s\n",
      "Epoch: 403, Train loss: 3.395, Val loss: 3.154, Epoch time = 1.742s\n",
      "Epoch: 404, Train loss: 3.387, Val loss: 3.269, Epoch time = 1.726s\n",
      "Epoch: 405, Train loss: 3.145, Val loss: 3.278, Epoch time = 1.730s\n",
      "Epoch: 406, Train loss: 2.817, Val loss: 2.930, Epoch time = 1.712s\n",
      "Epoch: 407, Train loss: 3.197, Val loss: 3.378, Epoch time = 1.734s\n",
      "Epoch: 408, Train loss: 2.859, Val loss: 3.819, Epoch time = 1.720s\n",
      "Epoch: 409, Train loss: 3.094, Val loss: 3.224, Epoch time = 1.704s\n",
      "Epoch: 410, Train loss: 3.492, Val loss: 3.277, Epoch time = 1.744s\n",
      "Epoch: 411, Train loss: 2.973, Val loss: 2.969, Epoch time = 1.716s\n",
      "Epoch: 412, Train loss: 2.937, Val loss: 2.918, Epoch time = 1.693s\n",
      "Epoch: 413, Train loss: 3.145, Val loss: 3.337, Epoch time = 1.714s\n",
      "Epoch: 414, Train loss: 3.170, Val loss: 3.301, Epoch time = 1.688s\n",
      "Epoch: 415, Train loss: 3.232, Val loss: 3.391, Epoch time = 1.856s\n",
      "Epoch: 416, Train loss: 2.873, Val loss: 3.300, Epoch time = 1.747s\n",
      "Epoch: 417, Train loss: 2.832, Val loss: 3.460, Epoch time = 1.720s\n",
      "Epoch: 418, Train loss: 3.627, Val loss: 3.456, Epoch time = 1.730s\n",
      "Epoch: 419, Train loss: 3.000, Val loss: 3.085, Epoch time = 1.778s\n",
      "Epoch: 420, Train loss: 2.930, Val loss: 3.562, Epoch time = 1.824s\n",
      "Epoch: 421, Train loss: 3.984, Val loss: 3.574, Epoch time = 1.664s\n",
      "Epoch: 422, Train loss: 3.500, Val loss: 3.025, Epoch time = 1.781s\n",
      "Epoch: 423, Train loss: 4.182, Val loss: 3.058, Epoch time = 1.705s\n",
      "Epoch: 424, Train loss: 3.181, Val loss: 3.279, Epoch time = 1.704s\n",
      "Epoch: 425, Train loss: 3.625, Val loss: 3.220, Epoch time = 1.853s\n",
      "Epoch: 426, Train loss: 3.726, Val loss: 3.134, Epoch time = 1.700s\n",
      "Epoch: 427, Train loss: 3.200, Val loss: 3.343, Epoch time = 1.630s\n",
      "Epoch: 428, Train loss: 3.203, Val loss: 3.141, Epoch time = 1.882s\n",
      "Epoch: 429, Train loss: 3.593, Val loss: 3.254, Epoch time = 1.772s\n",
      "Epoch: 430, Train loss: 3.221, Val loss: 3.458, Epoch time = 1.782s\n",
      "Epoch: 431, Train loss: 3.297, Val loss: 3.394, Epoch time = 1.847s\n",
      "Epoch: 432, Train loss: 3.227, Val loss: 2.965, Epoch time = 1.758s\n",
      "Epoch: 433, Train loss: 3.127, Val loss: 3.009, Epoch time = 1.764s\n",
      "Epoch: 434, Train loss: 3.682, Val loss: 3.258, Epoch time = 1.907s\n",
      "Epoch: 435, Train loss: 2.922, Val loss: 2.968, Epoch time = 1.830s\n",
      "Epoch: 436, Train loss: 2.897, Val loss: 3.350, Epoch time = 1.833s\n",
      "Epoch: 437, Train loss: 3.598, Val loss: 3.346, Epoch time = 1.902s\n",
      "Epoch: 438, Train loss: 3.590, Val loss: 3.079, Epoch time = 1.658s\n",
      "Epoch: 439, Train loss: 3.472, Val loss: 3.238, Epoch time = 1.828s\n",
      "Epoch: 440, Train loss: 3.583, Val loss: 3.271, Epoch time = 1.824s\n",
      "Epoch: 441, Train loss: 3.416, Val loss: 3.419, Epoch time = 1.876s\n",
      "Epoch: 442, Train loss: 3.414, Val loss: 3.090, Epoch time = 1.726s\n",
      "Epoch: 443, Train loss: 3.111, Val loss: 3.192, Epoch time = 1.716s\n",
      "Epoch: 444, Train loss: 2.619, Val loss: 2.922, Epoch time = 1.712s\n",
      "Epoch: 445, Train loss: 2.962, Val loss: 3.342, Epoch time = 1.699s\n",
      "Epoch: 446, Train loss: 3.734, Val loss: 3.040, Epoch time = 1.714s\n",
      "Epoch: 447, Train loss: 3.458, Val loss: 3.039, Epoch time = 1.707s\n",
      "Epoch: 448, Train loss: 3.486, Val loss: 3.031, Epoch time = 1.801s\n",
      "Epoch: 449, Train loss: 3.513, Val loss: 3.529, Epoch time = 1.782s\n",
      "Epoch: 450, Train loss: 2.832, Val loss: 3.214, Epoch time = 1.699s\n",
      "Epoch: 451, Train loss: 3.317, Val loss: 3.145, Epoch time = 1.706s\n",
      "Epoch: 452, Train loss: 3.161, Val loss: 3.140, Epoch time = 1.725s\n",
      "Epoch: 453, Train loss: 3.709, Val loss: 3.192, Epoch time = 1.747s\n",
      "Epoch: 454, Train loss: 2.702, Val loss: 3.038, Epoch time = 1.707s\n",
      "Epoch: 455, Train loss: 3.240, Val loss: 3.048, Epoch time = 1.794s\n",
      "Epoch: 456, Train loss: 3.111, Val loss: 2.942, Epoch time = 1.648s\n",
      "Epoch: 457, Train loss: 3.236, Val loss: 3.412, Epoch time = 1.725s\n",
      "Epoch: 458, Train loss: 3.998, Val loss: 3.179, Epoch time = 1.731s\n",
      "Epoch: 459, Train loss: 3.395, Val loss: 3.373, Epoch time = 1.758s\n",
      "Epoch: 460, Train loss: 4.059, Val loss: 3.379, Epoch time = 1.737s\n",
      "Epoch: 461, Train loss: 3.033, Val loss: 3.517, Epoch time = 1.712s\n",
      "Epoch: 462, Train loss: 2.984, Val loss: 3.148, Epoch time = 1.756s\n",
      "Epoch: 463, Train loss: 3.484, Val loss: 2.963, Epoch time = 1.678s\n",
      "Epoch: 464, Train loss: 3.173, Val loss: 3.062, Epoch time = 1.703s\n",
      "Epoch: 465, Train loss: 3.525, Val loss: 3.331, Epoch time = 1.772s\n",
      "Epoch: 466, Train loss: 3.414, Val loss: 3.402, Epoch time = 1.649s\n",
      "Epoch: 467, Train loss: 3.192, Val loss: 3.859, Epoch time = 1.676s\n",
      "Epoch: 468, Train loss: 2.914, Val loss: 3.329, Epoch time = 1.820s\n",
      "Epoch: 469, Train loss: 3.185, Val loss: 3.195, Epoch time = 1.724s\n",
      "Epoch: 470, Train loss: 3.424, Val loss: 3.759, Epoch time = 1.690s\n",
      "Epoch: 471, Train loss: 3.416, Val loss: 3.251, Epoch time = 1.916s\n",
      "Epoch: 472, Train loss: 3.361, Val loss: 3.097, Epoch time = 1.743s\n",
      "Epoch: 473, Train loss: 3.422, Val loss: 3.354, Epoch time = 1.789s\n",
      "Epoch: 474, Train loss: 3.190, Val loss: 3.397, Epoch time = 1.744s\n",
      "Epoch: 475, Train loss: 3.315, Val loss: 2.972, Epoch time = 1.767s\n",
      "Epoch: 476, Train loss: 3.219, Val loss: 3.027, Epoch time = 1.706s\n",
      "Epoch: 477, Train loss: 3.147, Val loss: 3.058, Epoch time = 1.711s\n",
      "Epoch: 478, Train loss: 2.872, Val loss: 3.313, Epoch time = 1.649s\n",
      "Epoch: 479, Train loss: 3.284, Val loss: 2.834, Epoch time = 1.717s\n",
      "Epoch: 480, Train loss: 3.098, Val loss: 2.913, Epoch time = 1.715s\n",
      "Epoch: 481, Train loss: 3.894, Val loss: 3.203, Epoch time = 1.802s\n",
      "Epoch: 482, Train loss: 3.091, Val loss: 3.110, Epoch time = 1.734s\n",
      "Epoch: 483, Train loss: 3.132, Val loss: 3.659, Epoch time = 1.986s\n",
      "Epoch: 484, Train loss: 3.634, Val loss: 3.390, Epoch time = 1.773s\n",
      "Epoch: 485, Train loss: 3.047, Val loss: 3.183, Epoch time = 1.755s\n",
      "Epoch: 486, Train loss: 3.065, Val loss: 3.041, Epoch time = 1.672s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 487, Train loss: 3.094, Val loss: 3.045, Epoch time = 1.670s\n",
      "Epoch: 488, Train loss: 3.138, Val loss: 3.184, Epoch time = 1.699s\n",
      "Epoch: 489, Train loss: 3.121, Val loss: 3.471, Epoch time = 1.645s\n",
      "Epoch: 490, Train loss: 3.134, Val loss: 3.123, Epoch time = 1.695s\n",
      "Epoch: 491, Train loss: 2.893, Val loss: 3.121, Epoch time = 1.693s\n",
      "Epoch: 492, Train loss: 3.069, Val loss: 3.263, Epoch time = 1.781s\n",
      "Epoch: 493, Train loss: 3.359, Val loss: 3.120, Epoch time = 1.685s\n",
      "Epoch: 494, Train loss: 3.176, Val loss: 2.800, Epoch time = 1.725s\n",
      "Epoch: 495, Train loss: 3.345, Val loss: 3.136, Epoch time = 1.793s\n",
      "Epoch: 496, Train loss: 3.015, Val loss: 3.113, Epoch time = 1.924s\n",
      "Epoch: 497, Train loss: 3.072, Val loss: 3.224, Epoch time = 1.822s\n",
      "Epoch: 498, Train loss: 2.954, Val loss: 3.052, Epoch time = 1.866s\n",
      "Epoch: 499, Train loss: 4.037, Val loss: 3.136, Epoch time = 1.694s\n",
      "Epoch: 500, Train loss: 3.298, Val loss: 3.227, Epoch time = 1.670s\n",
      "Epoch: 501, Train loss: 3.224, Val loss: 3.128, Epoch time = 1.668s\n",
      "Epoch: 502, Train loss: 3.406, Val loss: 3.122, Epoch time = 1.772s\n",
      "Epoch: 503, Train loss: 3.251, Val loss: 3.219, Epoch time = 1.697s\n",
      "Epoch: 504, Train loss: 3.034, Val loss: 3.417, Epoch time = 1.810s\n",
      "Epoch: 505, Train loss: 3.292, Val loss: 3.871, Epoch time = 1.724s\n",
      "Epoch: 506, Train loss: 3.417, Val loss: 3.449, Epoch time = 1.737s\n",
      "Epoch: 507, Train loss: 2.945, Val loss: 3.298, Epoch time = 1.805s\n",
      "Epoch: 508, Train loss: 3.007, Val loss: 4.184, Epoch time = 1.770s\n",
      "Epoch: 509, Train loss: 3.400, Val loss: 2.808, Epoch time = 1.760s\n",
      "Epoch: 510, Train loss: 3.521, Val loss: 3.250, Epoch time = 1.694s\n",
      "Epoch: 511, Train loss: 3.037, Val loss: 2.988, Epoch time = 1.813s\n",
      "Epoch: 512, Train loss: 2.767, Val loss: 3.115, Epoch time = 1.855s\n",
      "Epoch: 513, Train loss: 3.398, Val loss: 3.369, Epoch time = 1.795s\n",
      "Epoch: 514, Train loss: 2.863, Val loss: 2.916, Epoch time = 1.842s\n",
      "Epoch: 515, Train loss: 3.786, Val loss: 3.754, Epoch time = 1.677s\n",
      "Epoch: 516, Train loss: 3.155, Val loss: 3.022, Epoch time = 1.738s\n",
      "Epoch: 517, Train loss: 3.059, Val loss: 3.811, Epoch time = 1.878s\n",
      "Epoch: 518, Train loss: 2.974, Val loss: 3.260, Epoch time = 1.733s\n",
      "Epoch: 519, Train loss: 2.834, Val loss: 3.458, Epoch time = 1.853s\n",
      "Epoch: 520, Train loss: 3.334, Val loss: 2.996, Epoch time = 1.812s\n",
      "Epoch: 521, Train loss: 3.650, Val loss: 3.090, Epoch time = 1.691s\n",
      "Epoch: 522, Train loss: 2.913, Val loss: 2.792, Epoch time = 1.769s\n",
      "Epoch: 523, Train loss: 2.807, Val loss: 3.252, Epoch time = 1.748s\n",
      "Epoch: 524, Train loss: 3.149, Val loss: 3.404, Epoch time = 1.876s\n",
      "Epoch: 525, Train loss: 2.899, Val loss: 2.892, Epoch time = 1.828s\n",
      "Epoch: 526, Train loss: 3.040, Val loss: 3.135, Epoch time = 1.741s\n",
      "Epoch: 527, Train loss: 3.349, Val loss: 2.942, Epoch time = 1.755s\n",
      "Epoch: 528, Train loss: 3.520, Val loss: 3.056, Epoch time = 1.704s\n",
      "Epoch: 529, Train loss: 3.346, Val loss: 3.044, Epoch time = 1.798s\n",
      "Epoch: 530, Train loss: 2.757, Val loss: 3.087, Epoch time = 1.752s\n",
      "Epoch: 531, Train loss: 3.542, Val loss: 3.192, Epoch time = 1.766s\n",
      "Epoch: 532, Train loss: 3.125, Val loss: 3.553, Epoch time = 1.660s\n",
      "Epoch: 533, Train loss: 3.549, Val loss: 3.233, Epoch time = 1.721s\n",
      "Epoch: 534, Train loss: 3.722, Val loss: 3.236, Epoch time = 1.808s\n",
      "Epoch: 535, Train loss: 3.307, Val loss: 2.948, Epoch time = 1.674s\n",
      "Epoch: 536, Train loss: 3.709, Val loss: 3.077, Epoch time = 1.651s\n",
      "Epoch: 537, Train loss: 3.091, Val loss: 3.293, Epoch time = 1.981s\n",
      "Epoch: 538, Train loss: 2.815, Val loss: 3.858, Epoch time = 1.909s\n",
      "Epoch: 539, Train loss: 3.063, Val loss: 3.190, Epoch time = 1.783s\n",
      "Epoch: 540, Train loss: 3.434, Val loss: 2.901, Epoch time = 1.832s\n",
      "Epoch: 541, Train loss: 2.865, Val loss: 3.202, Epoch time = 1.893s\n",
      "Epoch: 542, Train loss: 3.403, Val loss: 3.092, Epoch time = 1.849s\n",
      "Epoch: 543, Train loss: 3.333, Val loss: 3.224, Epoch time = 1.721s\n",
      "Epoch: 544, Train loss: 3.403, Val loss: 3.288, Epoch time = 1.746s\n",
      "Epoch: 545, Train loss: 3.548, Val loss: 3.096, Epoch time = 1.850s\n",
      "Epoch: 546, Train loss: 3.158, Val loss: 3.849, Epoch time = 1.694s\n",
      "Epoch: 547, Train loss: 2.894, Val loss: 3.089, Epoch time = 1.696s\n",
      "Epoch: 548, Train loss: 3.095, Val loss: 2.919, Epoch time = 1.726s\n",
      "Epoch: 549, Train loss: 3.470, Val loss: 3.025, Epoch time = 1.653s\n",
      "Epoch: 550, Train loss: 3.430, Val loss: 2.980, Epoch time = 1.724s\n",
      "Epoch: 551, Train loss: 3.322, Val loss: 3.006, Epoch time = 1.863s\n",
      "Epoch: 552, Train loss: 2.853, Val loss: 3.140, Epoch time = 1.849s\n",
      "Epoch: 553, Train loss: 3.171, Val loss: 3.562, Epoch time = 1.916s\n",
      "Epoch: 554, Train loss: 3.485, Val loss: 3.446, Epoch time = 1.996s\n",
      "Epoch: 555, Train loss: 3.056, Val loss: 3.056, Epoch time = 1.810s\n",
      "Epoch: 556, Train loss: 3.173, Val loss: 3.184, Epoch time = 1.665s\n",
      "Epoch: 557, Train loss: 3.215, Val loss: 3.207, Epoch time = 1.728s\n",
      "Epoch: 558, Train loss: 3.113, Val loss: 3.048, Epoch time = 1.659s\n",
      "Epoch: 559, Train loss: 3.124, Val loss: 3.244, Epoch time = 1.753s\n",
      "Epoch: 560, Train loss: 3.593, Val loss: 3.237, Epoch time = 1.792s\n",
      "Epoch: 561, Train loss: 3.005, Val loss: 3.245, Epoch time = 1.668s\n",
      "Epoch: 562, Train loss: 3.305, Val loss: 3.272, Epoch time = 1.747s\n",
      "Epoch: 563, Train loss: 2.929, Val loss: 3.063, Epoch time = 1.861s\n",
      "Epoch: 564, Train loss: 3.081, Val loss: 2.941, Epoch time = 1.811s\n",
      "Epoch: 565, Train loss: 3.423, Val loss: 3.029, Epoch time = 1.817s\n",
      "Epoch: 566, Train loss: 2.621, Val loss: 3.103, Epoch time = 1.818s\n",
      "Epoch: 567, Train loss: 3.352, Val loss: 3.117, Epoch time = 1.835s\n",
      "Epoch: 568, Train loss: 2.909, Val loss: 3.334, Epoch time = 1.754s\n",
      "Epoch: 569, Train loss: 3.211, Val loss: 2.838, Epoch time = 1.705s\n",
      "Epoch: 570, Train loss: 3.028, Val loss: 3.005, Epoch time = 1.789s\n",
      "Epoch: 571, Train loss: 3.034, Val loss: 2.687, Epoch time = 1.735s\n",
      "Epoch: 572, Train loss: 3.130, Val loss: 3.441, Epoch time = 1.856s\n",
      "Epoch: 573, Train loss: 3.084, Val loss: 2.958, Epoch time = 1.805s\n",
      "Epoch: 574, Train loss: 3.008, Val loss: 3.075, Epoch time = 1.958s\n",
      "Epoch: 575, Train loss: 2.931, Val loss: 3.240, Epoch time = 1.823s\n",
      "Epoch: 576, Train loss: 3.399, Val loss: 3.026, Epoch time = 1.865s\n",
      "Epoch: 577, Train loss: 3.040, Val loss: 3.015, Epoch time = 1.912s\n",
      "Epoch: 578, Train loss: 3.013, Val loss: 2.771, Epoch time = 1.713s\n",
      "Epoch: 579, Train loss: 3.373, Val loss: 2.882, Epoch time = 1.720s\n",
      "Epoch: 580, Train loss: 3.613, Val loss: 3.169, Epoch time = 1.649s\n",
      "Epoch: 581, Train loss: 3.337, Val loss: 3.415, Epoch time = 1.706s\n",
      "Epoch: 582, Train loss: 3.200, Val loss: 3.122, Epoch time = 1.750s\n",
      "Epoch: 583, Train loss: 3.578, Val loss: 3.009, Epoch time = 1.644s\n",
      "Epoch: 584, Train loss: 2.949, Val loss: 3.255, Epoch time = 1.821s\n",
      "Epoch: 585, Train loss: 2.917, Val loss: 3.150, Epoch time = 1.742s\n",
      "Epoch: 586, Train loss: 2.948, Val loss: 3.846, Epoch time = 1.708s\n",
      "Epoch: 587, Train loss: 3.840, Val loss: 3.199, Epoch time = 1.791s\n",
      "Epoch: 588, Train loss: 3.750, Val loss: 3.257, Epoch time = 1.798s\n",
      "Epoch: 589, Train loss: 3.201, Val loss: 3.269, Epoch time = 1.714s\n",
      "Epoch: 590, Train loss: 3.298, Val loss: 3.054, Epoch time = 1.711s\n",
      "Epoch: 591, Train loss: 3.117, Val loss: 3.121, Epoch time = 1.804s\n",
      "Epoch: 592, Train loss: 2.747, Val loss: 3.372, Epoch time = 1.766s\n",
      "Epoch: 593, Train loss: 3.188, Val loss: 3.731, Epoch time = 1.769s\n",
      "Epoch: 594, Train loss: 3.324, Val loss: 2.862, Epoch time = 1.798s\n",
      "Epoch: 595, Train loss: 3.153, Val loss: 2.872, Epoch time = 1.673s\n",
      "Epoch: 596, Train loss: 3.601, Val loss: 3.092, Epoch time = 1.742s\n",
      "Epoch: 597, Train loss: 3.289, Val loss: 4.070, Epoch time = 1.686s\n",
      "Epoch: 598, Train loss: 3.221, Val loss: 3.250, Epoch time = 1.665s\n",
      "Epoch: 599, Train loss: 2.799, Val loss: 3.136, Epoch time = 1.869s\n",
      "Epoch: 600, Train loss: 3.106, Val loss: 2.909, Epoch time = 1.669s\n",
      "Epoch: 601, Train loss: 2.762, Val loss: 3.039, Epoch time = 1.765s\n",
      "Epoch: 602, Train loss: 2.875, Val loss: 3.054, Epoch time = 1.682s\n",
      "Epoch: 603, Train loss: 3.071, Val loss: 2.946, Epoch time = 1.609s\n",
      "Epoch: 604, Train loss: 3.215, Val loss: 3.052, Epoch time = 1.739s\n",
      "Epoch: 605, Train loss: 2.875, Val loss: 2.987, Epoch time = 1.706s\n",
      "Epoch: 606, Train loss: 3.737, Val loss: 3.231, Epoch time = 1.793s\n",
      "Epoch: 607, Train loss: 3.136, Val loss: 2.840, Epoch time = 1.718s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 608, Train loss: 2.870, Val loss: 2.663, Epoch time = 1.757s\n",
      "Epoch: 609, Train loss: 3.344, Val loss: 3.338, Epoch time = 1.757s\n",
      "Epoch: 610, Train loss: 2.779, Val loss: 2.774, Epoch time = 1.826s\n",
      "Epoch: 611, Train loss: 2.893, Val loss: 3.185, Epoch time = 1.687s\n",
      "Epoch: 612, Train loss: 2.890, Val loss: 3.101, Epoch time = 1.748s\n",
      "Epoch: 613, Train loss: 3.339, Val loss: 3.243, Epoch time = 1.708s\n",
      "Epoch: 614, Train loss: 3.453, Val loss: 3.545, Epoch time = 1.701s\n",
      "Epoch: 615, Train loss: 3.104, Val loss: 3.625, Epoch time = 1.705s\n",
      "Epoch: 616, Train loss: 2.876, Val loss: 3.170, Epoch time = 1.770s\n",
      "Epoch: 617, Train loss: 3.538, Val loss: 3.234, Epoch time = 1.757s\n",
      "Epoch: 618, Train loss: 3.309, Val loss: 3.066, Epoch time = 1.734s\n",
      "Epoch: 619, Train loss: 3.104, Val loss: 3.096, Epoch time = 1.938s\n",
      "Epoch: 620, Train loss: 3.278, Val loss: 3.104, Epoch time = 1.820s\n",
      "Epoch: 621, Train loss: 3.593, Val loss: 3.208, Epoch time = 1.701s\n",
      "Epoch: 622, Train loss: 3.021, Val loss: 2.599, Epoch time = 1.722s\n",
      "Epoch: 623, Train loss: 3.068, Val loss: 3.354, Epoch time = 1.692s\n",
      "Epoch: 624, Train loss: 3.349, Val loss: 3.075, Epoch time = 1.835s\n",
      "Epoch: 625, Train loss: 2.665, Val loss: 2.821, Epoch time = 1.715s\n",
      "Epoch: 626, Train loss: 3.429, Val loss: 3.313, Epoch time = 1.768s\n",
      "Epoch: 627, Train loss: 3.358, Val loss: 3.043, Epoch time = 1.794s\n",
      "Epoch: 628, Train loss: 3.132, Val loss: 3.689, Epoch time = 1.832s\n",
      "Epoch: 629, Train loss: 2.983, Val loss: 3.228, Epoch time = 1.903s\n",
      "Epoch: 630, Train loss: 3.023, Val loss: 3.183, Epoch time = 1.697s\n",
      "Epoch: 631, Train loss: 3.160, Val loss: 3.054, Epoch time = 1.816s\n",
      "Epoch: 632, Train loss: 3.040, Val loss: 2.936, Epoch time = 1.700s\n",
      "Epoch: 633, Train loss: 3.420, Val loss: 3.371, Epoch time = 1.739s\n",
      "Epoch: 634, Train loss: 3.263, Val loss: 3.190, Epoch time = 1.714s\n",
      "Epoch: 635, Train loss: 3.642, Val loss: 3.277, Epoch time = 1.692s\n",
      "Epoch: 636, Train loss: 3.328, Val loss: 3.279, Epoch time = 1.659s\n",
      "Epoch: 637, Train loss: 3.344, Val loss: 3.508, Epoch time = 1.655s\n",
      "Epoch: 638, Train loss: 3.274, Val loss: 2.774, Epoch time = 1.762s\n",
      "Epoch: 639, Train loss: 3.713, Val loss: 3.982, Epoch time = 1.634s\n",
      "Epoch: 640, Train loss: 3.262, Val loss: 3.210, Epoch time = 1.685s\n",
      "Epoch: 641, Train loss: 3.089, Val loss: 3.139, Epoch time = 1.746s\n",
      "Epoch: 642, Train loss: 3.042, Val loss: 3.087, Epoch time = 1.866s\n",
      "Epoch: 643, Train loss: 2.786, Val loss: 2.875, Epoch time = 1.708s\n",
      "Epoch: 644, Train loss: 2.928, Val loss: 2.856, Epoch time = 1.740s\n",
      "Epoch: 645, Train loss: 3.658, Val loss: 3.296, Epoch time = 1.711s\n",
      "Epoch: 646, Train loss: 3.908, Val loss: 2.874, Epoch time = 1.756s\n",
      "Epoch: 647, Train loss: 3.542, Val loss: 3.070, Epoch time = 1.695s\n",
      "Epoch: 648, Train loss: 3.493, Val loss: 3.069, Epoch time = 1.698s\n",
      "Epoch: 649, Train loss: 3.107, Val loss: 3.930, Epoch time = 1.712s\n",
      "Epoch: 650, Train loss: 3.334, Val loss: 4.224, Epoch time = 1.683s\n",
      "Epoch: 651, Train loss: 2.729, Val loss: 3.140, Epoch time = 1.681s\n",
      "Epoch: 652, Train loss: 3.761, Val loss: 3.375, Epoch time = 1.719s\n",
      "Epoch: 653, Train loss: 3.281, Val loss: 2.993, Epoch time = 1.794s\n",
      "Epoch: 654, Train loss: 2.718, Val loss: 2.978, Epoch time = 1.742s\n",
      "Epoch: 655, Train loss: 2.866, Val loss: 3.170, Epoch time = 1.707s\n",
      "Epoch: 656, Train loss: 3.302, Val loss: 3.111, Epoch time = 1.814s\n",
      "Epoch: 657, Train loss: 3.393, Val loss: 3.192, Epoch time = 1.797s\n",
      "Epoch: 658, Train loss: 3.126, Val loss: 2.818, Epoch time = 1.907s\n",
      "Epoch: 659, Train loss: 3.345, Val loss: 3.211, Epoch time = 1.945s\n",
      "Epoch: 660, Train loss: 3.310, Val loss: 3.200, Epoch time = 1.822s\n",
      "Epoch: 661, Train loss: 3.088, Val loss: 3.027, Epoch time = 1.784s\n",
      "Epoch: 662, Train loss: 2.922, Val loss: 3.388, Epoch time = 1.888s\n",
      "Epoch: 663, Train loss: 2.823, Val loss: 2.893, Epoch time = 1.629s\n",
      "Epoch: 664, Train loss: 3.689, Val loss: 3.013, Epoch time = 1.902s\n",
      "Epoch: 665, Train loss: 3.321, Val loss: 3.045, Epoch time = 1.763s\n",
      "Epoch: 666, Train loss: 3.671, Val loss: 2.912, Epoch time = 1.722s\n",
      "Epoch: 667, Train loss: 2.946, Val loss: 3.324, Epoch time = 1.696s\n",
      "Epoch: 668, Train loss: 3.474, Val loss: 3.181, Epoch time = 1.915s\n",
      "Epoch: 669, Train loss: 3.365, Val loss: 3.161, Epoch time = 1.675s\n",
      "Epoch: 670, Train loss: 3.276, Val loss: 3.107, Epoch time = 1.907s\n",
      "Epoch: 671, Train loss: 2.949, Val loss: 3.542, Epoch time = 1.791s\n",
      "Epoch: 672, Train loss: 3.080, Val loss: 3.366, Epoch time = 1.751s\n",
      "Epoch: 673, Train loss: 2.939, Val loss: 2.848, Epoch time = 1.800s\n",
      "Epoch: 674, Train loss: 3.128, Val loss: 3.642, Epoch time = 1.829s\n",
      "Epoch: 675, Train loss: 2.996, Val loss: 3.337, Epoch time = 1.687s\n",
      "Epoch: 676, Train loss: 3.023, Val loss: 2.807, Epoch time = 1.774s\n",
      "Epoch: 677, Train loss: 2.867, Val loss: 2.977, Epoch time = 1.787s\n",
      "Epoch: 678, Train loss: 3.174, Val loss: 3.041, Epoch time = 1.894s\n",
      "Epoch: 679, Train loss: 3.460, Val loss: 4.223, Epoch time = 1.844s\n",
      "Epoch: 680, Train loss: 3.454, Val loss: 3.217, Epoch time = 1.946s\n",
      "Epoch: 681, Train loss: 3.419, Val loss: 3.420, Epoch time = 1.835s\n",
      "Epoch: 682, Train loss: 3.435, Val loss: 2.825, Epoch time = 1.666s\n",
      "Epoch: 683, Train loss: 3.336, Val loss: 3.034, Epoch time = 1.761s\n",
      "Epoch: 684, Train loss: 3.787, Val loss: 3.053, Epoch time = 1.813s\n",
      "Epoch: 685, Train loss: 2.731, Val loss: 3.523, Epoch time = 1.899s\n",
      "Epoch: 686, Train loss: 3.764, Val loss: 2.923, Epoch time = 1.692s\n",
      "Epoch: 687, Train loss: 3.487, Val loss: 3.017, Epoch time = 1.811s\n",
      "Epoch: 688, Train loss: 2.965, Val loss: 3.116, Epoch time = 1.706s\n",
      "Epoch: 689, Train loss: 2.832, Val loss: 3.356, Epoch time = 1.831s\n",
      "Epoch: 690, Train loss: 3.106, Val loss: 3.013, Epoch time = 1.837s\n",
      "Epoch: 691, Train loss: 3.202, Val loss: 3.520, Epoch time = 1.794s\n",
      "Epoch: 692, Train loss: 3.316, Val loss: 2.880, Epoch time = 1.935s\n",
      "Epoch: 693, Train loss: 3.002, Val loss: 3.297, Epoch time = 1.758s\n",
      "Epoch: 694, Train loss: 2.830, Val loss: 2.866, Epoch time = 1.829s\n",
      "Epoch: 695, Train loss: 3.269, Val loss: 2.915, Epoch time = 1.878s\n",
      "Epoch: 696, Train loss: 2.789, Val loss: 3.226, Epoch time = 1.747s\n",
      "Epoch: 697, Train loss: 2.994, Val loss: 3.065, Epoch time = 1.785s\n",
      "Epoch: 698, Train loss: 2.833, Val loss: 3.031, Epoch time = 1.841s\n",
      "Epoch: 699, Train loss: 3.632, Val loss: 3.634, Epoch time = 1.751s\n",
      "Epoch: 700, Train loss: 3.063, Val loss: 3.147, Epoch time = 1.784s\n",
      "Epoch: 701, Train loss: 3.076, Val loss: 3.122, Epoch time = 1.689s\n",
      "Epoch: 702, Train loss: 3.344, Val loss: 3.084, Epoch time = 1.685s\n",
      "Epoch: 703, Train loss: 3.746, Val loss: 2.944, Epoch time = 1.848s\n",
      "Epoch: 704, Train loss: 2.905, Val loss: 3.107, Epoch time = 1.901s\n",
      "Epoch: 705, Train loss: 2.828, Val loss: 3.215, Epoch time = 1.936s\n",
      "Epoch: 706, Train loss: 3.372, Val loss: 3.259, Epoch time = 1.971s\n",
      "Epoch: 707, Train loss: 2.915, Val loss: 2.857, Epoch time = 1.966s\n",
      "Epoch: 708, Train loss: 3.833, Val loss: 2.987, Epoch time = 1.782s\n",
      "Epoch: 709, Train loss: 3.465, Val loss: 2.687, Epoch time = 1.781s\n",
      "Epoch: 710, Train loss: 2.975, Val loss: 3.264, Epoch time = 1.708s\n",
      "Epoch: 711, Train loss: 3.158, Val loss: 3.422, Epoch time = 1.804s\n",
      "Epoch: 712, Train loss: 3.668, Val loss: 3.453, Epoch time = 1.889s\n",
      "Epoch: 713, Train loss: 3.243, Val loss: 3.033, Epoch time = 2.008s\n",
      "Epoch: 714, Train loss: 2.940, Val loss: 3.282, Epoch time = 1.999s\n",
      "Epoch: 715, Train loss: 3.288, Val loss: 3.201, Epoch time = 1.698s\n",
      "Epoch: 716, Train loss: 3.500, Val loss: 3.252, Epoch time = 1.883s\n",
      "Epoch: 717, Train loss: 3.655, Val loss: 3.126, Epoch time = 1.760s\n",
      "Epoch: 718, Train loss: 3.489, Val loss: 3.175, Epoch time = 1.828s\n",
      "Epoch: 719, Train loss: 3.479, Val loss: 3.289, Epoch time = 1.782s\n",
      "Epoch: 720, Train loss: 3.059, Val loss: 2.928, Epoch time = 1.778s\n",
      "Epoch: 721, Train loss: 3.496, Val loss: 4.109, Epoch time = 1.939s\n",
      "Epoch: 722, Train loss: 3.262, Val loss: 3.314, Epoch time = 1.707s\n",
      "Epoch: 723, Train loss: 2.981, Val loss: 3.554, Epoch time = 1.794s\n",
      "Epoch: 724, Train loss: 3.257, Val loss: 3.198, Epoch time = 1.759s\n",
      "Epoch: 725, Train loss: 3.116, Val loss: 3.665, Epoch time = 1.864s\n",
      "Epoch: 726, Train loss: 2.888, Val loss: 3.195, Epoch time = 1.725s\n",
      "Epoch: 727, Train loss: 3.122, Val loss: 2.910, Epoch time = 1.919s\n",
      "Epoch: 728, Train loss: 2.764, Val loss: 3.057, Epoch time = 1.714s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 729, Train loss: 3.400, Val loss: 2.850, Epoch time = 1.954s\n",
      "Epoch: 730, Train loss: 3.734, Val loss: 2.976, Epoch time = 1.844s\n",
      "Epoch: 731, Train loss: 3.463, Val loss: 3.317, Epoch time = 1.841s\n",
      "Epoch: 732, Train loss: 3.136, Val loss: 3.553, Epoch time = 1.907s\n",
      "Epoch: 733, Train loss: 3.217, Val loss: 2.937, Epoch time = 1.867s\n",
      "Epoch: 734, Train loss: 2.967, Val loss: 3.712, Epoch time = 1.751s\n",
      "Epoch: 735, Train loss: 3.099, Val loss: 3.208, Epoch time = 1.807s\n",
      "Epoch: 736, Train loss: 2.858, Val loss: 3.002, Epoch time = 1.930s\n",
      "Epoch: 737, Train loss: 3.861, Val loss: 2.900, Epoch time = 1.786s\n",
      "Epoch: 738, Train loss: 2.934, Val loss: 3.020, Epoch time = 1.878s\n",
      "Epoch: 739, Train loss: 3.865, Val loss: 3.048, Epoch time = 1.696s\n",
      "Epoch: 740, Train loss: 2.703, Val loss: 2.948, Epoch time = 1.816s\n",
      "Epoch: 741, Train loss: 3.317, Val loss: 3.031, Epoch time = 1.818s\n",
      "Epoch: 742, Train loss: 2.766, Val loss: 3.235, Epoch time = 1.865s\n",
      "Epoch: 743, Train loss: 3.045, Val loss: 3.170, Epoch time = 1.680s\n",
      "Epoch: 744, Train loss: 3.908, Val loss: 3.357, Epoch time = 1.740s\n",
      "Epoch: 745, Train loss: 3.443, Val loss: 2.562, Epoch time = 1.845s\n",
      "Epoch: 746, Train loss: 3.289, Val loss: 3.238, Epoch time = 1.865s\n",
      "Epoch: 747, Train loss: 3.059, Val loss: 2.929, Epoch time = 1.739s\n",
      "Epoch: 748, Train loss: 3.263, Val loss: 3.181, Epoch time = 1.888s\n",
      "Epoch: 749, Train loss: 3.179, Val loss: 2.902, Epoch time = 1.853s\n",
      "Epoch: 750, Train loss: 3.113, Val loss: 2.835, Epoch time = 1.810s\n",
      "Epoch: 751, Train loss: 3.168, Val loss: 3.036, Epoch time = 1.818s\n",
      "Epoch: 752, Train loss: 3.854, Val loss: 3.196, Epoch time = 1.707s\n",
      "Epoch: 753, Train loss: 3.347, Val loss: 3.002, Epoch time = 1.780s\n",
      "Epoch: 754, Train loss: 3.394, Val loss: 2.931, Epoch time = 1.671s\n",
      "Epoch: 755, Train loss: 2.867, Val loss: 3.256, Epoch time = 1.785s\n",
      "Epoch: 756, Train loss: 3.658, Val loss: 2.724, Epoch time = 1.768s\n",
      "Epoch: 757, Train loss: 3.266, Val loss: 3.129, Epoch time = 1.801s\n",
      "Epoch: 758, Train loss: 2.885, Val loss: 2.971, Epoch time = 1.827s\n",
      "Epoch: 759, Train loss: 2.916, Val loss: 3.541, Epoch time = 1.919s\n",
      "Epoch: 760, Train loss: 2.668, Val loss: 3.117, Epoch time = 1.959s\n",
      "Epoch: 761, Train loss: 3.032, Val loss: 3.154, Epoch time = 1.827s\n",
      "Epoch: 762, Train loss: 3.227, Val loss: 2.842, Epoch time = 1.760s\n",
      "Epoch: 763, Train loss: 3.116, Val loss: 2.877, Epoch time = 1.794s\n",
      "Epoch: 764, Train loss: 3.317, Val loss: 3.058, Epoch time = 1.793s\n",
      "Epoch: 765, Train loss: 3.844, Val loss: 3.213, Epoch time = 1.740s\n",
      "Epoch: 766, Train loss: 3.030, Val loss: 3.437, Epoch time = 1.810s\n",
      "Epoch: 767, Train loss: 2.815, Val loss: 2.811, Epoch time = 1.840s\n",
      "Epoch: 768, Train loss: 3.413, Val loss: 2.773, Epoch time = 1.837s\n",
      "Epoch: 769, Train loss: 3.023, Val loss: 2.729, Epoch time = 1.803s\n",
      "Epoch: 770, Train loss: 3.032, Val loss: 3.021, Epoch time = 1.705s\n",
      "Epoch: 771, Train loss: 3.592, Val loss: 2.997, Epoch time = 1.738s\n",
      "Epoch: 772, Train loss: 2.939, Val loss: 3.286, Epoch time = 1.790s\n",
      "Epoch: 773, Train loss: 3.259, Val loss: 3.080, Epoch time = 1.781s\n",
      "Epoch: 774, Train loss: 3.017, Val loss: 2.762, Epoch time = 1.946s\n",
      "Epoch: 775, Train loss: 2.833, Val loss: 3.588, Epoch time = 1.991s\n",
      "Epoch: 776, Train loss: 3.427, Val loss: 3.059, Epoch time = 1.663s\n",
      "Epoch: 777, Train loss: 3.436, Val loss: 3.287, Epoch time = 1.673s\n",
      "Epoch: 778, Train loss: 3.032, Val loss: 3.086, Epoch time = 1.808s\n",
      "Epoch: 779, Train loss: 3.286, Val loss: 3.407, Epoch time = 1.954s\n",
      "Epoch: 780, Train loss: 3.415, Val loss: 2.827, Epoch time = 1.741s\n",
      "Epoch: 781, Train loss: 3.643, Val loss: 2.822, Epoch time = 1.732s\n",
      "Epoch: 782, Train loss: 3.353, Val loss: 2.700, Epoch time = 1.806s\n",
      "Epoch: 783, Train loss: 3.083, Val loss: 4.012, Epoch time = 1.856s\n",
      "Epoch: 784, Train loss: 3.409, Val loss: 3.122, Epoch time = 1.827s\n",
      "Epoch: 785, Train loss: 3.007, Val loss: 3.401, Epoch time = 1.727s\n",
      "Epoch: 786, Train loss: 3.350, Val loss: 3.895, Epoch time = 1.682s\n",
      "Epoch: 787, Train loss: 3.421, Val loss: 3.418, Epoch time = 1.807s\n",
      "Epoch: 788, Train loss: 3.445, Val loss: 2.910, Epoch time = 1.770s\n",
      "Epoch: 789, Train loss: 3.049, Val loss: 2.968, Epoch time = 1.738s\n",
      "Epoch: 790, Train loss: 3.038, Val loss: 3.052, Epoch time = 1.754s\n",
      "Epoch: 791, Train loss: 2.707, Val loss: 3.052, Epoch time = 1.704s\n",
      "Epoch: 792, Train loss: 3.533, Val loss: 3.499, Epoch time = 1.878s\n",
      "Epoch: 793, Train loss: 3.142, Val loss: 3.138, Epoch time = 1.691s\n",
      "Epoch: 794, Train loss: 3.325, Val loss: 3.201, Epoch time = 1.649s\n",
      "Epoch: 795, Train loss: 2.948, Val loss: 3.539, Epoch time = 1.672s\n",
      "Epoch: 796, Train loss: 3.125, Val loss: 2.794, Epoch time = 1.758s\n",
      "Epoch: 797, Train loss: 3.521, Val loss: 2.918, Epoch time = 1.799s\n",
      "Epoch: 798, Train loss: 3.621, Val loss: 2.985, Epoch time = 1.758s\n",
      "Epoch: 799, Train loss: 3.481, Val loss: 2.920, Epoch time = 1.721s\n",
      "Epoch: 800, Train loss: 3.536, Val loss: 3.308, Epoch time = 1.729s\n",
      "Epoch: 801, Train loss: 3.347, Val loss: 2.826, Epoch time = 1.773s\n",
      "Epoch: 802, Train loss: 3.025, Val loss: 3.843, Epoch time = 1.836s\n",
      "Epoch: 803, Train loss: 3.392, Val loss: 3.088, Epoch time = 1.845s\n",
      "Epoch: 804, Train loss: 3.175, Val loss: 3.218, Epoch time = 1.834s\n",
      "Epoch: 805, Train loss: 3.173, Val loss: 2.872, Epoch time = 1.674s\n",
      "Epoch: 806, Train loss: 3.039, Val loss: 3.499, Epoch time = 1.643s\n",
      "Epoch: 807, Train loss: 3.154, Val loss: 3.782, Epoch time = 1.742s\n",
      "Epoch: 808, Train loss: 3.264, Val loss: 3.579, Epoch time = 1.721s\n",
      "Epoch: 809, Train loss: 3.471, Val loss: 2.789, Epoch time = 1.705s\n",
      "Epoch: 810, Train loss: 3.085, Val loss: 2.921, Epoch time = 1.772s\n",
      "Epoch: 811, Train loss: 3.667, Val loss: 3.084, Epoch time = 1.745s\n",
      "Epoch: 812, Train loss: 3.296, Val loss: 3.326, Epoch time = 1.810s\n",
      "Epoch: 813, Train loss: 3.441, Val loss: 3.105, Epoch time = 1.758s\n",
      "Epoch: 814, Train loss: 3.368, Val loss: 3.049, Epoch time = 1.759s\n",
      "Epoch: 815, Train loss: 3.298, Val loss: 3.062, Epoch time = 1.744s\n",
      "Epoch: 816, Train loss: 2.701, Val loss: 3.095, Epoch time = 1.767s\n",
      "Epoch: 817, Train loss: 3.262, Val loss: 2.906, Epoch time = 1.752s\n",
      "Epoch: 818, Train loss: 3.354, Val loss: 2.960, Epoch time = 1.876s\n",
      "Epoch: 819, Train loss: 3.359, Val loss: 3.068, Epoch time = 2.086s\n",
      "Epoch: 820, Train loss: 2.718, Val loss: 2.900, Epoch time = 2.024s\n",
      "Epoch: 821, Train loss: 3.136, Val loss: 3.202, Epoch time = 2.135s\n",
      "Epoch: 822, Train loss: 3.131, Val loss: 3.406, Epoch time = 2.009s\n",
      "Epoch: 823, Train loss: 2.960, Val loss: 3.434, Epoch time = 1.829s\n",
      "Epoch: 824, Train loss: 3.175, Val loss: 3.009, Epoch time = 1.887s\n",
      "Epoch: 825, Train loss: 2.617, Val loss: 3.258, Epoch time = 2.114s\n",
      "Epoch: 826, Train loss: 3.038, Val loss: 3.229, Epoch time = 2.008s\n",
      "Epoch: 827, Train loss: 3.323, Val loss: 2.956, Epoch time = 1.967s\n",
      "Epoch: 828, Train loss: 3.303, Val loss: 3.301, Epoch time = 1.702s\n",
      "Epoch: 829, Train loss: 2.932, Val loss: 3.126, Epoch time = 1.848s\n",
      "Epoch: 830, Train loss: 3.108, Val loss: 3.049, Epoch time = 1.800s\n",
      "Epoch: 831, Train loss: 3.057, Val loss: 3.116, Epoch time = 1.774s\n",
      "Epoch: 832, Train loss: 2.645, Val loss: 3.090, Epoch time = 1.840s\n",
      "Epoch: 833, Train loss: 3.154, Val loss: 3.050, Epoch time = 1.750s\n",
      "Epoch: 834, Train loss: 3.590, Val loss: 3.119, Epoch time = 1.845s\n",
      "Epoch: 835, Train loss: 3.248, Val loss: 2.757, Epoch time = 1.795s\n",
      "Epoch: 836, Train loss: 3.652, Val loss: 2.991, Epoch time = 1.812s\n",
      "Epoch: 837, Train loss: 3.126, Val loss: 3.022, Epoch time = 1.660s\n",
      "Epoch: 838, Train loss: 3.137, Val loss: 3.117, Epoch time = 1.810s\n",
      "Epoch: 839, Train loss: 3.202, Val loss: 3.014, Epoch time = 1.729s\n",
      "Epoch: 840, Train loss: 2.941, Val loss: 2.732, Epoch time = 1.787s\n",
      "Epoch: 841, Train loss: 3.547, Val loss: 3.441, Epoch time = 1.778s\n",
      "Epoch: 842, Train loss: 2.890, Val loss: 3.124, Epoch time = 1.946s\n",
      "Epoch: 843, Train loss: 2.894, Val loss: 2.870, Epoch time = 1.837s\n",
      "Epoch: 844, Train loss: 3.191, Val loss: 2.949, Epoch time = 1.712s\n",
      "Epoch: 845, Train loss: 2.822, Val loss: 3.023, Epoch time = 1.730s\n",
      "Epoch: 846, Train loss: 2.960, Val loss: 2.915, Epoch time = 1.800s\n",
      "Epoch: 847, Train loss: 3.069, Val loss: 2.708, Epoch time = 1.732s\n",
      "Epoch: 848, Train loss: 3.698, Val loss: 2.825, Epoch time = 1.816s\n",
      "Epoch: 849, Train loss: 3.153, Val loss: 3.084, Epoch time = 1.735s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 850, Train loss: 3.456, Val loss: 3.408, Epoch time = 1.678s\n",
      "Epoch: 851, Train loss: 3.257, Val loss: 3.027, Epoch time = 1.696s\n",
      "Epoch: 852, Train loss: 2.809, Val loss: 3.099, Epoch time = 1.704s\n",
      "Epoch: 853, Train loss: 3.059, Val loss: 3.189, Epoch time = 1.709s\n",
      "Epoch: 854, Train loss: 2.683, Val loss: 3.104, Epoch time = 1.729s\n",
      "Epoch: 855, Train loss: 2.948, Val loss: 3.173, Epoch time = 1.700s\n",
      "Epoch: 856, Train loss: 3.489, Val loss: 2.728, Epoch time = 1.731s\n",
      "Epoch: 857, Train loss: 2.983, Val loss: 2.986, Epoch time = 1.861s\n",
      "Epoch: 858, Train loss: 3.119, Val loss: 2.978, Epoch time = 1.778s\n",
      "Epoch: 859, Train loss: 3.427, Val loss: 2.601, Epoch time = 1.695s\n",
      "Epoch: 860, Train loss: 3.147, Val loss: 2.933, Epoch time = 1.829s\n",
      "Epoch: 861, Train loss: 2.941, Val loss: 3.082, Epoch time = 1.790s\n",
      "Epoch: 862, Train loss: 3.034, Val loss: 4.128, Epoch time = 1.792s\n",
      "Epoch: 863, Train loss: 2.819, Val loss: 2.946, Epoch time = 1.702s\n",
      "Epoch: 864, Train loss: 3.451, Val loss: 3.276, Epoch time = 1.835s\n",
      "Epoch: 865, Train loss: 2.634, Val loss: 3.017, Epoch time = 1.762s\n",
      "Epoch: 866, Train loss: 3.012, Val loss: 3.268, Epoch time = 1.691s\n",
      "Epoch: 867, Train loss: 3.483, Val loss: 3.068, Epoch time = 1.837s\n",
      "Epoch: 868, Train loss: 3.074, Val loss: 3.166, Epoch time = 1.895s\n",
      "Epoch: 869, Train loss: 3.222, Val loss: 3.069, Epoch time = 2.187s\n",
      "Epoch: 870, Train loss: 3.255, Val loss: 2.749, Epoch time = 1.965s\n",
      "Epoch: 871, Train loss: 3.251, Val loss: 3.137, Epoch time = 1.892s\n",
      "Epoch: 872, Train loss: 2.806, Val loss: 2.922, Epoch time = 1.782s\n",
      "Epoch: 873, Train loss: 3.217, Val loss: 2.940, Epoch time = 1.703s\n",
      "Epoch: 874, Train loss: 3.374, Val loss: 3.694, Epoch time = 1.757s\n",
      "Epoch: 875, Train loss: 2.996, Val loss: 3.100, Epoch time = 1.898s\n",
      "Epoch: 876, Train loss: 2.810, Val loss: 2.851, Epoch time = 1.880s\n",
      "Epoch: 877, Train loss: 3.399, Val loss: 3.147, Epoch time = 1.749s\n",
      "Epoch: 878, Train loss: 2.940, Val loss: 3.366, Epoch time = 1.872s\n",
      "Epoch: 879, Train loss: 3.539, Val loss: 3.742, Epoch time = 1.727s\n",
      "Epoch: 880, Train loss: 3.020, Val loss: 2.955, Epoch time = 1.726s\n",
      "Epoch: 881, Train loss: 2.545, Val loss: 2.945, Epoch time = 1.665s\n",
      "Epoch: 882, Train loss: 3.548, Val loss: 3.219, Epoch time = 1.678s\n",
      "Epoch: 883, Train loss: 2.800, Val loss: 3.197, Epoch time = 1.717s\n",
      "Epoch: 884, Train loss: 3.454, Val loss: 3.087, Epoch time = 1.706s\n",
      "Epoch: 885, Train loss: 3.351, Val loss: 2.777, Epoch time = 1.969s\n",
      "Epoch: 886, Train loss: 2.716, Val loss: 3.057, Epoch time = 1.929s\n",
      "Epoch: 887, Train loss: 2.804, Val loss: 2.822, Epoch time = 2.024s\n",
      "Epoch: 888, Train loss: 2.872, Val loss: 2.993, Epoch time = 1.911s\n",
      "Epoch: 889, Train loss: 3.370, Val loss: 3.168, Epoch time = 1.688s\n",
      "Epoch: 890, Train loss: 3.687, Val loss: 2.879, Epoch time = 2.125s\n",
      "Epoch: 891, Train loss: 2.865, Val loss: 2.969, Epoch time = 2.095s\n",
      "Epoch: 892, Train loss: 2.682, Val loss: 2.942, Epoch time = 1.791s\n",
      "Epoch: 893, Train loss: 3.229, Val loss: 3.109, Epoch time = 1.732s\n",
      "Epoch: 894, Train loss: 3.109, Val loss: 3.648, Epoch time = 1.772s\n",
      "Epoch: 895, Train loss: 3.299, Val loss: 3.020, Epoch time = 1.789s\n",
      "Epoch: 896, Train loss: 2.794, Val loss: 3.063, Epoch time = 1.651s\n",
      "Epoch: 897, Train loss: 3.049, Val loss: 2.992, Epoch time = 1.783s\n",
      "Epoch: 898, Train loss: 3.119, Val loss: 3.136, Epoch time = 1.848s\n",
      "Epoch: 899, Train loss: 2.728, Val loss: 2.738, Epoch time = 1.796s\n",
      "Epoch: 900, Train loss: 2.625, Val loss: 3.416, Epoch time = 1.719s\n",
      "Epoch: 901, Train loss: 2.869, Val loss: 2.808, Epoch time = 1.786s\n",
      "Epoch: 902, Train loss: 3.448, Val loss: 3.214, Epoch time = 1.706s\n",
      "Epoch: 903, Train loss: 3.321, Val loss: 3.977, Epoch time = 1.744s\n",
      "Epoch: 904, Train loss: 3.772, Val loss: 2.932, Epoch time = 1.861s\n",
      "Epoch: 905, Train loss: 3.400, Val loss: 3.013, Epoch time = 2.016s\n",
      "Epoch: 906, Train loss: 3.584, Val loss: 2.861, Epoch time = 1.825s\n",
      "Epoch: 907, Train loss: 2.908, Val loss: 3.216, Epoch time = 1.789s\n",
      "Epoch: 908, Train loss: 3.258, Val loss: 3.142, Epoch time = 2.075s\n",
      "Epoch: 909, Train loss: 3.179, Val loss: 3.039, Epoch time = 1.818s\n",
      "Epoch: 910, Train loss: 3.164, Val loss: 2.898, Epoch time = 1.928s\n",
      "Epoch: 911, Train loss: 3.114, Val loss: 3.106, Epoch time = 2.202s\n",
      "Epoch: 912, Train loss: 3.506, Val loss: 2.953, Epoch time = 1.848s\n",
      "Epoch: 913, Train loss: 3.253, Val loss: 2.792, Epoch time = 1.729s\n",
      "Epoch: 914, Train loss: 2.734, Val loss: 3.122, Epoch time = 1.793s\n",
      "Epoch: 915, Train loss: 2.922, Val loss: 3.000, Epoch time = 1.780s\n",
      "Epoch: 916, Train loss: 3.118, Val loss: 3.218, Epoch time = 1.811s\n",
      "Epoch: 917, Train loss: 2.916, Val loss: 3.070, Epoch time = 1.861s\n",
      "Epoch: 918, Train loss: 3.026, Val loss: 3.094, Epoch time = 1.858s\n",
      "Epoch: 919, Train loss: 2.930, Val loss: 2.781, Epoch time = 1.773s\n",
      "Epoch: 920, Train loss: 3.049, Val loss: 3.009, Epoch time = 1.798s\n",
      "Epoch: 921, Train loss: 3.051, Val loss: 3.269, Epoch time = 1.792s\n",
      "Epoch: 922, Train loss: 3.248, Val loss: 3.423, Epoch time = 1.775s\n",
      "Epoch: 923, Train loss: 3.199, Val loss: 3.147, Epoch time = 1.775s\n",
      "Epoch: 924, Train loss: 3.250, Val loss: 4.513, Epoch time = 1.857s\n",
      "Epoch: 925, Train loss: 2.678, Val loss: 3.127, Epoch time = 1.764s\n",
      "Epoch: 926, Train loss: 3.279, Val loss: 3.053, Epoch time = 1.946s\n",
      "Epoch: 927, Train loss: 3.272, Val loss: 3.071, Epoch time = 1.876s\n",
      "Epoch: 928, Train loss: 3.010, Val loss: 2.824, Epoch time = 1.860s\n",
      "Epoch: 929, Train loss: 3.004, Val loss: 3.280, Epoch time = 1.964s\n",
      "Epoch: 930, Train loss: 3.258, Val loss: 3.019, Epoch time = 1.808s\n",
      "Epoch: 931, Train loss: 2.804, Val loss: 2.844, Epoch time = 1.960s\n",
      "Epoch: 932, Train loss: 2.917, Val loss: 3.410, Epoch time = 1.805s\n",
      "Epoch: 933, Train loss: 3.319, Val loss: 3.078, Epoch time = 1.895s\n",
      "Epoch: 934, Train loss: 3.848, Val loss: 3.252, Epoch time = 2.083s\n",
      "Epoch: 935, Train loss: 3.240, Val loss: 3.451, Epoch time = 1.917s\n",
      "Epoch: 936, Train loss: 2.934, Val loss: 3.171, Epoch time = 2.088s\n",
      "Epoch: 937, Train loss: 2.698, Val loss: 3.277, Epoch time = 2.085s\n",
      "Epoch: 938, Train loss: 3.165, Val loss: 2.623, Epoch time = 1.825s\n",
      "Epoch: 939, Train loss: 3.150, Val loss: 3.161, Epoch time = 1.864s\n",
      "Epoch: 940, Train loss: 3.452, Val loss: 2.837, Epoch time = 1.740s\n",
      "Epoch: 941, Train loss: 3.690, Val loss: 2.982, Epoch time = 1.785s\n",
      "Epoch: 942, Train loss: 3.351, Val loss: 3.096, Epoch time = 2.253s\n",
      "Epoch: 943, Train loss: 3.492, Val loss: 2.880, Epoch time = 2.191s\n",
      "Epoch: 944, Train loss: 3.476, Val loss: 2.648, Epoch time = 1.984s\n",
      "Epoch: 945, Train loss: 3.135, Val loss: 3.412, Epoch time = 1.966s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2000\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_xyr.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_xyr.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./train_loss_xyr.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=500\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src1, src2, y = collate_fn(10,-100,train=False)\n",
    "        \n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "print(y[0])\n",
    "#print(Ad[0])\n",
    "Ad = postprocess_2(Ad)\n",
    "print(Ad[0])\n",
    "Ad=postprocess_MinCostAss(Ad)\n",
    "print(Ad[0])\n",
    "#torch.manual_seed(344)\n",
    "\n",
    "#Ad = torch.rand(1,3,4)\n",
    "#print(Ad[0])\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "\n",
    "l=np.ones(len(f))*2\n",
    "l=l.astype(int)\n",
    "f2=np.repeat(Ad[0].detach().numpy(), l, axis=0)\n",
    "\n",
    "#print('f2',f2)\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "\n",
    "#print(row_ind,col_ind)\n",
    "\n",
    "z=np.zeros(f.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "\n",
    "print(z)\n",
    "        \n",
    "f2[0::2, :] = z[:] \n",
    "        \n",
    "#print('f2',f2) \n",
    "\n",
    "\n",
    "row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind_f, col_ind_f):\n",
    "        z3[i,j]=1\n",
    "\n",
    "\n",
    "print(z3)\n",
    "\n",
    "f_add = z3[0::2, :] + z3[1::2, :]\n",
    "\n",
    "print('f_add',f_add)\n",
    "        \n",
    "z2 = np.zeros(f.shape)\n",
    "zero_col=np.where(~z.any(axis=0))[0]\n",
    "ind=torch.argmax(Ad[0][:,zero_col], dim=0)\n",
    "        \n",
    "print(Ad[0][:,zero_col])        \n",
    "print(np.where(~z.any(axis=0))[0])\n",
    "print(ind)\n",
    "print(z)\n",
    "\n",
    "for k,l in zip(ind,zero_col):\n",
    "    z2[k,l]=1\n",
    "    \n",
    "print(z+z2)    \n",
    "\n",
    "pp_A=postprocess(Ad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(pp_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon\n",
    "run=14\n",
    "src1, src2, y = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "#transformer.load_state_dict(torch.load('AttTrack.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "Ad = postprocess_2(Ad)\n",
    "pp_A=postprocess_MinCostAss(Ad)\n",
    "#pp_A=postprocess_linAss(Ad)\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
