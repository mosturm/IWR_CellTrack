{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph()\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j)\n",
    "            print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:])\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,11)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(11,15)\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen):\n",
    "        self.pen=pen\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            \n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        print(end_nodes)\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        print(capacities)\n",
    "        \n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "                                    \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        tasks = np.max([right_n,left_n])\n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-tasks)\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        print(supplies)\n",
    "        print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "        \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[1,2,3]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                if M[z,j]!=0:\n",
    "                    print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                    print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "        \n",
    "        e_start=e_mid\n",
    "        print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d):\n",
    "    \n",
    "    m_Ad = []\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        m_Ad.append(np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy()))\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Ad = postprocess_MinCostAss(m_Ad)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0267,  0.1453, -0.0789,  0.0489, -0.1336, -0.2113, -0.1248, -0.0305,\n",
       "           0.0090,  0.0653,  0.0127, -0.0921, -0.1803, -0.1376,  0.0537,  0.0335,\n",
       "           0.0454,  0.1683, -0.0115,  0.0873, -0.1704, -0.0194,  0.0500,  0.1184,\n",
       "           0.0970,  0.2225,  0.2227, -0.3404, -0.1093,  0.2371,  0.1008, -0.0813,\n",
       "           0.0330, -0.1236, -0.2342, -0.0426, -0.0024,  0.0392,  0.1696,  0.4908,\n",
       "           0.2130, -0.2124, -0.2138, -0.2236, -0.1587, -0.1036,  0.0067,  0.2044],\n",
       "         [ 0.0687,  0.1966, -0.0357, -0.1585,  0.0976, -0.1245, -0.2950, -0.1241,\n",
       "          -0.2730, -0.1747, -0.1520, -0.6228, -1.0923, -0.3612,  0.2027,  0.4117,\n",
       "           0.0437, -0.3532,  0.3607,  0.4315,  0.3988,  0.1283, -0.0242,  0.1762,\n",
       "           1.1894,  0.2108,  0.0277, -0.7799, -0.0039, -0.0396, -0.1685,  0.1206,\n",
       "           0.3721, -0.3239, -0.1013,  0.0453,  0.2031,  0.0191,  0.4528,  0.0820,\n",
       "           0.4366,  0.0410, -0.3237,  0.1421, -0.1213, -0.2404, -0.1972,  0.3101],\n",
       "         [-0.1329,  0.8042, -0.4529, -0.4201, -0.2204, -0.4756, -0.3550, -0.0903,\n",
       "          -0.5442, -0.1605, -0.0243,  0.0034,  0.1245, -0.4345,  0.1624,  0.1443,\n",
       "           0.1265, -0.0553,  0.3242,  0.6024, -0.3167, -0.1998,  0.2334,  0.1277,\n",
       "           0.1243,  0.4320, -0.0883, -0.2110, -0.3864, -0.0192,  0.4299,  0.3818,\n",
       "          -0.1161, -0.1511, -0.3110,  0.0174, -0.1766,  0.5170,  0.1618,  0.6186,\n",
       "           0.6530, -0.3794, -0.0786, -0.1740,  0.3585,  0.5984, -0.1307,  0.4516],\n",
       "         [ 0.0843,  0.3595,  0.0915, -0.0747, -0.4412,  0.1035,  0.2700, -0.3083,\n",
       "          -0.1506, -0.1496, -0.0511, -0.1559, -0.7954, -0.4190, -0.1173, -0.0455,\n",
       "          -0.0100, -0.0099,  0.4113,  0.1865,  0.0703, -0.1187,  0.0873,  0.0281,\n",
       "           0.7508,  0.2778, -0.1153, -1.0088, -0.1764,  0.0767,  0.0497,  0.1298,\n",
       "           0.5450, -0.3311, -0.5917,  0.1286,  0.3909,  0.3046,  0.0947,  0.0640,\n",
       "           0.4094, -0.2666, -0.3485, -0.1728, -0.2709,  0.3598, -0.0253, -0.4220],\n",
       "         [ 0.3830,  0.5518,  0.3810,  0.4975,  0.1912,  0.1566, -0.0336,  0.1889,\n",
       "          -0.0321,  0.0471,  0.1715,  0.3349, -0.3534, -0.1395,  0.1585,  0.3259,\n",
       "           0.1793,  0.0385, -0.3919,  0.2911, -0.5058,  0.0461,  0.3744, -0.4513,\n",
       "           0.1782,  0.1757,  0.8158, -0.5481,  0.0981,  0.3074, -0.1241,  0.2865,\n",
       "           0.1174, -0.2526, -0.8013,  0.1766, -0.5267, -0.0436,  0.4484,  0.5275,\n",
       "           0.3245,  0.0817, -0.0790,  0.5812, -0.5180,  0.2509, -0.1535,  0.5734],\n",
       "         [-0.3419,  0.8358, -0.5446, -0.4813, -0.0946, -0.4290, -0.2724,  0.1433,\n",
       "          -0.6247, -0.1955, -0.1192, -0.0708,  0.0956, -0.2141,  0.1566,  0.0780,\n",
       "           0.2727,  0.0160,  0.2292,  0.5196, -0.3812, -0.2636,  0.1615, -0.0261,\n",
       "           0.0592,  0.3323, -0.2005, -0.2480, -0.1695, -0.2285,  0.5816,  0.3716,\n",
       "          -0.2227, -0.2131, -0.3123, -0.1552, -0.0128,  0.5522,  0.2727,  0.5793,\n",
       "           0.7433, -0.0572, -0.2012, -0.0667, -0.0969,  0.4989, -0.4023,  0.3425],\n",
       "         [ 0.2999,  0.1649, -0.1946,  0.3348,  0.2978,  0.2711,  0.1543,  0.2395,\n",
       "           0.2270, -0.2144,  0.2896,  0.1895, -0.1147, -0.0749,  0.2354, -0.1558,\n",
       "           0.3703, -0.2063, -0.7283,  0.4107, -0.6267,  0.3291,  0.3691, -0.1139,\n",
       "           0.2636,  0.1958,  0.7094, -0.5356,  0.6304,  0.0213,  0.1697, -0.1377,\n",
       "           0.0176, -0.4906, -0.8174, -0.1305, -0.1907,  0.1090,  0.1080,  0.5028,\n",
       "           0.6242, -0.0863, -0.6203,  0.2884, -0.0519,  0.1131,  0.1292,  0.3099],\n",
       "         [-0.1551,  0.8735, -0.6649, -0.2736, -0.1314, -0.2511, -0.3989, -0.0382,\n",
       "          -0.2675, -0.1817, -0.2680, -0.1459,  0.0376, -0.3679, -0.1049,  0.1404,\n",
       "           0.1316,  0.0154,  0.0745,  0.5673, -0.4248, -0.0041,  0.1755,  0.2029,\n",
       "           0.2579,  0.4698, -0.0528,  0.0402, -0.1326, -0.1170,  0.3971,  0.6850,\n",
       "          -0.2350, -0.3790, -0.4341, -0.0227, -0.1804,  0.5503,  0.0790,  0.6279,\n",
       "           0.5558, -0.3708, -0.2566, -0.3470,  0.3541,  0.6027,  0.0437,  0.5241],\n",
       "         [ 0.1858,  0.0937,  0.0787,  0.4424,  0.3238,  0.2683, -0.0368, -0.0104,\n",
       "          -0.1333, -0.0310,  0.3918,  0.2599, -0.2217, -0.0503,  0.0757,  0.0943,\n",
       "           0.1922, -0.2809, -0.5614,  0.3866, -0.5911,  0.1957,  0.4616, -0.4474,\n",
       "           0.1138,  0.1449,  0.7547, -0.6074,  0.2776,  0.2164, -0.2693,  0.1335,\n",
       "           0.1901, -0.4469, -0.7259,  0.0500, -0.3068,  0.0122,  0.5243,  0.5299,\n",
       "           0.5472, -0.1240, -0.3081,  0.4021, -0.3035,  0.2814, -0.1206,  0.4129]]),\n",
       " tensor([[-2.6663e-02,  1.4534e-01, -7.8945e-02,  4.8944e-02, -1.3359e-01,\n",
       "          -2.1131e-01, -1.2484e-01, -3.0539e-02,  8.9566e-03,  6.5341e-02,\n",
       "           1.2721e-02, -9.2142e-02, -1.8031e-01, -1.3760e-01,  5.3685e-02,\n",
       "           3.3521e-02,  4.5426e-02,  1.6829e-01, -1.1451e-02,  8.7343e-02,\n",
       "          -1.7035e-01, -1.9422e-02,  5.0044e-02,  1.1842e-01,  9.6953e-02,\n",
       "           2.2250e-01,  2.2272e-01, -3.4036e-01, -1.0935e-01,  2.3706e-01,\n",
       "           1.0083e-01, -8.1283e-02,  3.3036e-02, -1.2360e-01, -2.3415e-01,\n",
       "          -4.2587e-02, -2.4326e-03,  3.9160e-02,  1.6963e-01,  4.9077e-01,\n",
       "           2.1298e-01, -2.1235e-01, -2.1377e-01, -2.2364e-01, -1.5873e-01,\n",
       "          -1.0364e-01,  6.7279e-03,  2.0440e-01],\n",
       "         [ 2.2387e-01,  1.3142e-01, -1.6672e-01,  3.5190e-01,  1.9494e-01,\n",
       "           2.3527e-01,  4.0279e-02,  1.2960e-01,  3.3711e-01, -2.3855e-01,\n",
       "           3.5439e-01,  1.9542e-02, -3.8410e-01, -9.7311e-02,  3.3283e-01,\n",
       "          -2.0627e-01,  2.9987e-01, -1.7992e-02, -8.8506e-01,  3.8998e-01,\n",
       "          -5.6969e-01,  4.4747e-01,  4.0241e-01, -3.5429e-02,  2.1451e-01,\n",
       "           5.9378e-02,  6.0630e-01, -7.1846e-01,  4.9672e-01,  3.4613e-02,\n",
       "           4.2378e-01, -2.1310e-01, -9.6190e-02, -3.0515e-01, -6.9871e-01,\n",
       "           7.4866e-02, -2.8189e-01,  1.6801e-01,  6.9431e-02,  4.1774e-01,\n",
       "           7.0445e-01,  1.3356e-02, -6.7130e-01,  3.5597e-01,  2.8566e-01,\n",
       "           1.0833e-01,  1.1373e-01,  3.1568e-01],\n",
       "         [-2.5483e-01,  9.2326e-01, -4.5269e-01, -1.4846e-01,  4.0377e-02,\n",
       "          -2.5857e-01, -5.2921e-01, -1.5628e-01, -1.2279e-01, -1.6099e-02,\n",
       "          -1.0328e-01, -3.2364e-01, -7.9060e-02, -4.7824e-01, -1.1219e-01,\n",
       "           2.0764e-02,  7.8735e-02,  4.7536e-02,  1.1240e-01,  1.5070e-01,\n",
       "          -2.9114e-01, -7.7502e-02,  5.8261e-02,  1.3294e-01,  5.3703e-02,\n",
       "           5.4573e-01, -1.3627e-02,  1.1206e-01, -9.4352e-03, -2.5652e-02,\n",
       "           4.9437e-01,  8.3707e-01, -1.2219e-01, -3.8434e-01, -3.4595e-01,\n",
       "           3.4381e-02, -1.7899e-01,  6.4808e-01,  2.6152e-01,  3.9779e-01,\n",
       "           5.3809e-01, -4.1612e-01, -5.0397e-01, -8.4321e-02,  3.4402e-01,\n",
       "           4.6298e-01,  3.6683e-01,  4.7988e-01],\n",
       "         [-1.3973e-01,  7.7755e-01, -4.8803e-01, -4.3289e-01, -1.9046e-01,\n",
       "          -4.7094e-01, -4.5837e-01, -1.0272e-01, -3.2038e-01, -2.1097e-02,\n",
       "           1.3992e-01, -9.8675e-02,  2.4905e-01, -5.0763e-01,  1.5480e-01,\n",
       "           1.5607e-01, -4.8021e-03, -2.4240e-01,  8.3589e-02,  5.3794e-01,\n",
       "          -1.2039e-01, -1.6576e-01,  2.9070e-01,  7.1651e-02, -2.5141e-02,\n",
       "           5.9065e-01, -1.2404e-01, -2.5287e-02, -4.3976e-01,  7.9650e-03,\n",
       "           4.4267e-01,  5.2006e-01,  1.2625e-02, -3.6027e-01, -3.7107e-01,\n",
       "           1.2882e-01, -1.9627e-01,  6.9210e-01,  3.4604e-01,  4.4671e-01,\n",
       "           4.7187e-01, -3.2367e-01, -3.2150e-01, -5.4937e-02,  1.4808e-01,\n",
       "           4.6953e-01,  2.0503e-02,  4.7949e-01],\n",
       "         [ 2.6386e-01,  2.7174e-01, -4.7065e-02,  4.0438e-01,  2.0050e-01,\n",
       "           2.6472e-01, -3.6525e-02, -1.2268e-01,  1.0375e-02, -1.9661e-01,\n",
       "           3.7305e-01,  2.5597e-01, -3.2442e-01,  2.7769e-02, -3.4282e-02,\n",
       "           1.1382e-01, -6.5076e-03, -1.0703e-01, -7.1317e-01,  3.6707e-01,\n",
       "          -7.3793e-01,  2.4137e-01,  3.3207e-01, -4.1273e-01,  1.8640e-01,\n",
       "           1.0813e-01,  5.0959e-01, -7.7612e-01,  1.8657e-01,  2.1409e-01,\n",
       "          -1.4428e-01,  9.2952e-02,  1.5128e-01, -1.6694e-01, -6.6363e-01,\n",
       "           8.7505e-02, -4.9038e-01, -1.7085e-03,  4.4741e-01,  5.3869e-01,\n",
       "           4.7427e-01,  1.8765e-02, -2.5666e-01,  4.3261e-01, -1.7509e-01,\n",
       "           3.3080e-01, -3.2162e-01,  4.3678e-01],\n",
       "         [-2.9230e-01,  8.1986e-01, -5.4769e-01, -6.3091e-01, -2.5194e-02,\n",
       "          -5.5292e-01, -3.7758e-01,  3.5058e-02, -5.0978e-01, -9.2138e-02,\n",
       "          -2.9891e-02, -1.0356e-01,  1.7915e-01, -1.9524e-01,  1.7006e-01,\n",
       "           1.5457e-01,  2.9867e-01, -6.7527e-02,  1.1139e-01,  3.8461e-01,\n",
       "          -2.4743e-01, -1.6745e-01,  1.9338e-01, -3.0682e-03, -7.6078e-02,\n",
       "           4.8389e-01, -1.6770e-01, -1.4592e-01, -1.2420e-01, -1.9317e-01,\n",
       "           5.8500e-01,  4.0284e-01, -1.0727e-01, -4.1891e-01, -3.3388e-01,\n",
       "          -7.4726e-02,  7.8783e-02,  5.7389e-01,  4.5140e-01,  4.9650e-01,\n",
       "           6.8975e-01, -1.8919e-02, -3.2197e-01,  2.2939e-02, -2.7111e-01,\n",
       "           4.3380e-01, -1.1544e-01,  2.4709e-01],\n",
       "         [ 4.2115e-02,  2.7510e-01,  5.0699e-02, -2.6091e-01, -3.3232e-01,\n",
       "           7.2824e-02,  2.2393e-01, -3.2449e-02, -1.1166e-01, -9.1420e-02,\n",
       "          -1.1037e-01, -2.7216e-01, -8.2734e-01, -5.8693e-01, -3.8280e-02,\n",
       "           1.0102e-01,  9.0822e-02, -1.6577e-01,  2.2252e-01,  2.2984e-02,\n",
       "           3.3983e-01,  4.6229e-02,  1.8455e-01, -1.0636e-01,  8.2303e-01,\n",
       "           2.8253e-01,  2.3112e-02, -8.4345e-01, -1.1823e-01,  1.0609e-05,\n",
       "          -2.1902e-02,  2.2216e-01,  7.2453e-01, -3.9384e-01, -6.8695e-01,\n",
       "           2.8032e-01,  4.8310e-01,  3.5229e-01,  1.4607e-01,  1.6044e-01,\n",
       "           5.2113e-01, -1.6247e-01, -2.6107e-01, -2.5940e-01, -3.1768e-01,\n",
       "           2.6463e-01, -1.4217e-01, -5.0724e-01],\n",
       "         [ 4.3940e-03,  1.4612e-01, -1.5904e-02, -1.7116e-01, -1.1600e-01,\n",
       "          -1.3732e-01, -1.8681e-01,  8.0933e-02, -2.1395e-01, -1.7314e-01,\n",
       "          -1.5328e-01, -5.5363e-01, -9.7402e-01, -4.3826e-01,  2.0092e-01,\n",
       "           3.9603e-01,  2.9224e-01, -3.8550e-01,  2.9054e-01,  1.9996e-01,\n",
       "           4.8162e-01,  7.2983e-02,  1.7249e-03,  6.2528e-02,  1.1290e+00,\n",
       "           2.1733e-01,  8.3328e-02, -7.6739e-01,  1.4029e-03, -1.1383e-01,\n",
       "          -1.7551e-01,  1.4545e-01,  5.7588e-01, -5.5937e-01, -2.7625e-01,\n",
       "           1.6997e-01,  1.7914e-01,  1.5213e-01,  3.2772e-01,  2.4598e-01,\n",
       "           3.9298e-01,  1.0137e-01, -2.2566e-01,  1.5020e-01, -1.8841e-01,\n",
       "          -1.9283e-01, -1.2045e-01,  2.1172e-01],\n",
       "         [ 3.7849e-01,  6.1656e-01,  6.3610e-02,  4.6794e-01,  7.4151e-04,\n",
       "           2.8132e-02, -3.0012e-02, -9.4167e-02, -5.8647e-02, -1.0703e-01,\n",
       "           1.9176e-01,  4.8984e-01, -4.6415e-01,  1.0052e-01, -8.2834e-02,\n",
       "           1.7194e-01,  4.2021e-02, -4.9411e-02, -6.2711e-01,  2.2309e-01,\n",
       "          -5.7681e-01,  6.5165e-02,  2.3620e-01, -2.9455e-01,  2.0242e-01,\n",
       "          -5.4368e-03,  7.0983e-01, -7.3004e-01, -4.9360e-02,  4.5317e-02,\n",
       "          -7.3849e-03,  2.3387e-01,  1.9129e-01, -1.5568e-01, -6.4572e-01,\n",
       "           1.6907e-01, -6.2340e-01, -1.5319e-01,  5.3067e-01,  4.7713e-01,\n",
       "           4.0198e-01,  9.6419e-02, -1.1607e-01,  6.0293e-01, -5.5834e-01,\n",
       "           2.7200e-01, -1.8066e-01,  5.9860e-01]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]]),\n",
       " tensor([[2.2500e-14, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
       "         [1.0000e+00, 7.8608e-02, 1.2245e-01, 1.3036e-01, 5.4131e-02, 7.9342e-02,\n",
       "          7.8864e-01, 1.0000e+00, 4.6470e-02],\n",
       "         [1.0000e+00, 3.5622e-02, 1.0000e+00, 1.0000e+00, 3.2528e-02, 1.0000e+00,\n",
       "          3.2991e-01, 1.3815e-01, 3.2219e-02],\n",
       "         [1.0000e+00, 6.2647e-02, 2.9325e-01, 3.9452e-01, 4.9795e-02, 1.8045e-01,\n",
       "          1.0000e+00, 9.4117e-01, 4.5819e-02],\n",
       "         [1.0000e+00, 3.8218e-01, 2.3829e-02, 2.9362e-02, 1.0000e+00, 2.6171e-02,\n",
       "          4.1701e-02, 4.6229e-02, 1.0000e+00],\n",
       "         [1.0000e+00, 3.0266e-02, 5.5910e-01, 1.0000e+00, 2.8942e-02, 1.0000e+00,\n",
       "          1.6710e-01, 8.6672e-02, 2.9421e-02],\n",
       "         [1.0000e+00, 1.0000e+00, 3.0455e-02, 3.7110e-02, 1.0000e+00, 3.0962e-02,\n",
       "          6.3504e-02, 8.0048e-02, 5.3151e-01],\n",
       "         [1.0000e+00, 2.9379e-02, 1.0000e+00, 1.0000e+00, 2.6657e-02, 1.0000e+00,\n",
       "          2.3136e-01, 1.1169e-01, 2.6275e-02],\n",
       "         [1.0000e+00, 1.0000e+00, 2.6015e-02, 3.1826e-02, 1.0000e+00, 2.7550e-02,\n",
       "          4.8953e-02, 5.7345e-02, 1.0000e+00]], dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[5, 6]])\n",
    "\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "np.concatenate((a, b.T), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjacencyTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        #src_t1 = self.lin(src_t1)\n",
    "        #src_t2 = self.lin(src_t2)\n",
    "        \n",
    "        #src_t1 = self.lin2(src_t1)\n",
    "        #src_t2 = self.lin2(src_t2)\n",
    "        \n",
    "        src1_emb = self.positional_encoding(src_t1)\n",
    "        src2_emb = self.positional_encoding(src_t2)\n",
    "        #print('trans_src',src1_emb,src1_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        out1 = self.encoder(src1_emb,src_key_padding_mask=src_padding_mask1)\n",
    "        out2 = self.encoder(src2_emb,src_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_dec1=self.decoder(out2, out1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        #out_dec2=self.decoder(out1, out2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_dec2=out1\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        out_dec2=torch.transpose(out_dec2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class AdjacencyNonlearn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(AdjacencyNonlearn, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        \n",
    "        #self.lin = nn.Sequential(\n",
    "        #    nn.Linear(input_dim, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "        \n",
    "        #self.lin2 = nn.Sequential(\n",
    "        #    nn.Linear(emb_size, emb_size),\n",
    "        #    nn.LeakyReLU())\n",
    "\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "      \n",
    "        \n",
    "        out_dec2=torch.transpose(src_t1,0,1) \n",
    "        out_dec1=torch.transpose(src_t2,0,1)\n",
    "        out_dec1=torch.transpose(out_dec1,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_dec2,out_dec1))\n",
    "        \n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 48 ###!!!!24 for n2v emb\n",
    "nhead= 6    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 5\n",
    "\n",
    "\n",
    "transformer = AdjacencyNonlearn(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7648, 0.7795, 0.7933, 0.8034],\n",
      "        [0.7931, 0.9024, 0.9992, 0.8748],\n",
      "        [0.7753, 0.9989, 0.8849, 0.9672],\n",
      "        [0.7946, 0.9605, 0.8529, 0.9984]]) tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "l 4.006432028377757\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e0f9c779eec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-c702201c3ecf>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'l'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_xyr.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_xyr.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9fb953fd0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2deZwdRdX3vyeZmcxkD9nISoIkSAgYIPIQdlQw7JhHeQk7KqgYkf0J8ooCPr7IIio7AgIqArJGjARBFkG2gCxJICRkIRuEbJCQPan3j+qyq3v63uk7c/d7vp/PfLq7um/f6rn31q9O1alzxBiDoiiKojjalboCiqIoSnmhwqAoiqJEUGFQFEVRIqgwKIqiKBFUGBRFUZQIdaWuQD7o1auXGTJkSKmroSiKUlG89tpry4wxvePlVSEMQ4YMYerUqaWuhqIoSkUhIvOTynUoSVEURYmgwqAoiqJEUGFQFEVRIqgwKIqiKBFUGBRFUZQIKgyKoihKBBUGRVEUJYIKQzY++ggeeqjUtVAURSkqVbHArWBsu63drlkDnTqVti6KoihFQi2GTPzlL+H+li2lq4eiKEqRUWHIxHxvpfjmzaWrh6IoSpFRYUiDCoOiKDWECkMmRML9TZtKVw9FUZQio8KQCV8Y1GJQFKWGUGFIg1oMiqLUECoMaVCLQVGUGkKFIRO+GKgwKIpSQ6QSBhEZKyIzRWS2iEzMcM2xIjJDRKaLyD2xc11FZJGIXJ/wukkiMs07HiUiL4nIGyIyVUT2zPWh8sLateG+DiUpilJDtLjyWUTaAzcABwMLgVdFZJIxZoZ3zTDgImAfY8xKEekTu83lwLMJ9x4HrIkVXwlcaoz5m4gcFhwfmP6R8sSqVeG+WgyKotQQaSyGPYHZxpg5xpiNwL3A0bFrTgduMMasBDDGLHUnRGQPoC/whP8CEekMnAv8LHYvA3QN9rsBi9M9Sp65+upwXy0GRVFqiDSxkgYAC7zjhcB/xa4ZDiAiLwDtgZ8aYx4XkXbANcBJwJdjr7k8OLc2Vn42MEVErsYK195JlRKRM4AzAAYPHpziMXLED4OhFoOiKDVEGotBEspM7LgOGIYd8hkP3CYi3YEzgcnGGF9YEJFRwA7GmIcT7v094BxjzCDgHOD2pEoZY241xow2xozu3bt3isdoA2oxKIpSQ6QRhoXAIO94IM2HdxYCjxpjNhlj5gIzsUIxBpggIvOAq4GTReSKoHyPoPx5YLiIPBPc6xTAxbr+M3Yoq/AYA7fdBqtXhxPP48bZ7ZtvFqUKiqIo5UAaYXgVGCYiQ0WkATgOmBS75hHgIAAR6YUdWppjjDnBGDPYGDMEOB+42xgz0RhzkzGmf1C+L/CeMebA4F6LgQOC/S8Bs1r9dLlw2WVw+ulw+OFhiO3ddoOmJliwIPtrFUVRqogW5xiMMZtFZAIwBTt/cIcxZrqIXAZMNcZMCs4dIiIzgC3ABcaY5a2s0+nAr0WkDlhPMI9QUFasgJ/+1O7/859heX09dOsGn31W8CooiqKUC6kS9RhjJgOTY2WXePsG62F0bpZ73AncmVA+DxjpHT8P7JGmXnnjzDOTyxsarPWgwqAoSg2hK58BPvkkuby+3grDmvhSC0VRlOpFhQGgffvk8vp66NxZLQZFUWoKFQaArVuTyxsa7ByDvwpaURSlylFhgMwL2IyBfv1gyZLi1kdRFKWEqDBAZmFYvBj697fb3/62uHVSFEUpESoMADvvnFy+cqUVBoAzCu81qyiKUg6oMAB07GgXsp12mj0+8ki72G3ixFAYIPNchKIoShWRah1D1bNxo51obmiwxz16wK232v1evcLr1q0LV0UriqJUKbVtMZx/Pnzuc1YY6uvtH0CHDuE1TU3hvq5nUBSlBqhtYdi6FZYuDS0GFzxv6NDwmsbGcF+FQVGUGqC2hcGFu1i/3loJH35oy3fYIbwmk8Xw5pv2dYqiKFWGCoMx1vuoqSnMu9C5c3iNbzF8+qndXnstjBoFv/xl8eqqKIpSJFQYAJYtswLghMFNQkPUYpg/325//3u7Xbo0PLdyJey0E7z1VuHqqyiKUgRUGMAKg28xuEloiFoMc+fa7erVdrvWy0r6xBPw7rtw+eWFq6+iKEoRUGGA7MLgeyitWxfdTptmh6Ig+toPPoCf/9xOaiuKolQYtS0Mbi7hk0+sMHTrZo87dgyvEYF582yD7wTBWQovvgh/+Yvd94Xh5pvh4ovhvvsK/giKoij5praFwV+s1tRk5w6uvRZ23TV63Xbb2UVvzgtp3bpwlfS8eXabZG28805Bqq0oilJIVBgcHTtC375w9tnWSoizdKm1BBYutALRs6ctd2LhJqLr68PQGStWFK7uiqIoBUKFIVeuvdZue/SwWycMP/6x3fpDTpkywymKopQxKgyOlSvTvcatXejZE+rqrDC8/np4vk+fUCxUGBRFyTdLlsD77xf0LWpbGLp0CfddLz8tQ4daV9YNG2y+BsemTaEwLF/e9joqiqL49O9vozNcfjk8/3xB3kKFwXH99dmvddFWHdtua11Z16+PhuNetCgUhgKruqIoNYZzjwe45BLYb7+CvE1tC4PvQTRsWPZrTz01etypk7UY4sJw++2h9bF8uV0joSiKkg8WLWpelutoRwpqWxhyob4epk8Pjzt2tMKwbl3zBD5+cD11WVUUJR9s3Qrf/W7z8gLMZaowAHTtmu66Pn3C/Y4dYZttrEvqX/8avW79erv2AWDmzMz3W7tWV0cr1U1DA1xzTalrUR08+2zztgbC4J55RIXhgw9gzpx013bvHu537GiF4sMP4Y47wnI377DjjnbfCcPLL9sPcPPm8NpOnWCvvdr+DIpSjmzZYp0xzj+/1DWpDvyAnj4udlseUWEYNChcrNYSdV4m1Pp6OwH95pvRazZssB9Up04wfLgdSpo+3QpAt27ReQ2Af/+7bfVXlHLFRQNQ8kNdhkzMpbIYRGSsiMwUkdkiMjHDNceKyAwRmS4i98TOdRWRRSLSzPVHRCaJyLRY2Q+C95suIlfm8kBF5YtfTC53YbyHDrUWyciRxa2XopQDGzaUugbVRZLQ7r57NE1AnsggQSEi0h64ATgYWAi8KiKTjDEzvGuGARcB+xhjVopIn9htLgeeTbj3OGBNrOwg4GhgV2PMhoR7lQ9+pjcfF621a1f417+anzcmOeyGolQTOn+WX/xhaLCuqs89V5C3SmMx7AnMNsbMMcZsBO7FNtw+pwM3GGNWAhhj/pPBRkT2APoCT/gvEJHOwLnAz2L3+h5whTFmQ/xeZYOLyjpgQFi2337w9NPhcWOjjb2U5K4a/8H4vsmKUi2oMOQX32KYOxcmTy7YW6URhgHAAu94YVDmMxwYLiIviMhLIjIWQETaAdcAFyTc9/Lg3NpY+XBgPxF5WUSeFZEM4zUlYsWKcKWzv/ahY0c74exobMzs7RTPFb1hgxWHtfF/haJUMCoM+cUJw2WXwZAh0RTEeSaNMCSNecS7uHXAMOBAYDxwm4h0B84EJhtjfGFBREYBOxhjHk64dx3QA9gLKyj3izQfdxGRM0RkqohM/fjjj1M8Rp7o0SNcMd2+PTz5pN2vq7MWgqOxMfMHF1+ksnatXd7eqVP6mE2KUu6oMOQXJwxjxxb8rVqcY8BaCIO844HA4oRrXjLGbALmishMrFCMwfb+zwQ6Aw0isgaYD+whIvOCOvQRkWeMMQcG93rIGGOAV0RkK9ALiLT+xphbgVsBRo8eXbqxmAMOgLPOgvPOg3btrIfT8uVWGDJFb91556g30qefwt132/1ly8LIrYpSyXz0UalrUF0k5XwpEGkshleBYSIyVEQagOOASbFrHgEOAhCRXtjhoDnGmBOMMYONMUOA84G7jTETjTE3GWP6B+X7Au8FouDu9aXgXsOBBqB840rU1cGvfw2DB9vjI46w26am5kNGPq++Gu5Pm9Z89bSiVDoHHFDqGlQX5SQMxpjNwARgCvAOcL8xZrqIXCYiRwWXTQGWi8gM4GngAmNMa0OL3gFsH7iw3gucElgPlcGWLXbb2BidLBozJnrdqlXh/ty5zXNHK0olE09SVUE/4bKliMKQZigJY8xkYHKs7BJv32A9jM7Nco87gTsTyucBI73jjcCJaepVlrief2MjnHiizex24YV2XuH440NL4cILw9d89FH4w7n/fvjpT4taZUXJOyfGfsKbNhXE376mcMKQaaFbHtGVz/nGCUOHDlYcfvITO6y0ww7ws7hnbsCHH4bCcOmluUVk1Z6YUo7E1+9kG1ZV0uHWMZTDUJKSI24oKUnVGxuTX/Phh9HjNN4cjz8O3/iGFZ0CLXJRlFZzzDF2e9ZZdqvC0Hbc/zBTO5JHCm+T1BqukU/yLOrQIfk1/lASpIuvfuih4f6rr8L++6evo6IUmp49rVfeF75gjwuQM6Dm+Owzu21NrvocUYsh35x0kt0mRU3NpPRLlkR/OLn+iLQ3ppQbGzaEw6lQm9/RadPSR25Og1sAWwSLQYUh35x+up1n8EN0O+IWw/XX20m6RYui8wq5CkPcA0RRSo0KA+yyC3zuc/m732ef2QgL7QrfbOtQUiHIFCDPV/otW+wH/N57za/LVRiWt9YzWFEKxIYN1gvJ5RCoRWFwbN2an8b8s8+KMowEajEUF9+bwH1RkkQg15hJajEo5YazGFy8sA8+KG19Sslbb+XnPmvXWouhCKgwFBNnMRx/fFiWlGQjjcWw997hvloMSrnhhOG//st2gt54o9Q1Kh1ttZaMseuh1GKoUnr2tD+Q228Py/xE3n/6k92mEQZ/hbQKg1JurFtnh5Hq6mzmwgIkrK8Y2pqwaOxYG6Bz7lwVhqrlC1+IzjX4H7SLt5Rm5fOKFfbLcuihKgxK+bF0KfTubfdrURh89/P4uqQtW+Doo+Hww8N1T5lYuhSeCFLZzJ+vwlAz3HRTuN+rl93OmpXdavjoI3j/fbvdfXcrEhqETyklxsA999hx8IcfhoULwzD03brBvHklrV7R8YePPvzQOqQ88og9/tvfYNIkm2inpSgHM2eG+8uX6xxDzeB6VRBdFOeH5Y7j53Po2dOKQq31yJTSM2tWGKbhscfghBPs+p1x42yD54ShSxd4/vna6rz4w0cnn2y31wcp7/1h4JaiHBx5ZPRYLYYaxF/7kK0n4ZL5XHttaGXocJJSTBYuhOHD4eKL7bHzvHn77fAaJwwuFP3q1cWrX6lJipLcv7/dtm8flrU0/xCPoKDCUIP47qzZehJONA4+2FoMfpmiFAM3xPHkk/DMM2Fv2McJg9vG3apXrLBDotVIkjAMCvKd+WsaWhKGXr2gT5/wWIeSapxswuCsg549Q2FQi0EpJu++a7edO8NBBzUPBAl2bgHCXm98MefOO9uowy49bjWRJAxOBPzfdjZh2LrV/p9HjgzL1GKoUebOtds0wrDNNjqUpJQGJwzZerDO+86tuYkLgBOTgw+Gd97Jb/1KTZIwuIWrvmNJNmFYtAjWrIE99wzLVBhqFJfMpCVh6NrVXqsWg1IKXEP++OOZr3GxwXr3hl13DcUkCRc5tFrIhzAsXWq3Q4aEZTqUVKM4YVizJvM1q1aFE9XdutnJLJ1jUIrJggXNy7bZJnrsr9epr7eeS5m+10UIDFdU2ioMf/4zTJxo97fbLiz3J64LSJV9GhXKK6/AX/5i910v67zz7Pa66+DWW6PXr1ljXQDB+kf37KkWg1Jcktyj499BXxhee81u//jH5PtVW76GbMJw991hWSZL6dhjw6G33XcPy4uUsVGFoRz44hdDl754XtyzzoLvfCc8njYNHnzQTvo5VBiUYrNqVfTYrbvZfvuwLCkxVaYe74MP5qde5YIThnPPtdt+/awwPPccTJ1qyxoa4J//zH6foUOja52OOy7/dU1AhaHcaCmf6y672K0Kg1IqNmywf87xAWDUKLudNi0s8y2Go4+220yL3K69Nr91LDVOGA491Pbyd93VCoP/Ox05MnnS3R9u69gxGsZ/wIDC1DeGCkO5kWmsNR5TRYVBKSbDh8OOO9r9jz+225/9zLqc+rj8CxC1ft0QklucCVE3zGrDCYP7H3TsaIXB/x3vuGM05IVjyZJwvwjZ2pJQYShnXLgBaJ6jwf8BNja2PYKjomRj1qxwHYJzqR461OYbj3dK3GI3Nw8GYc/XjamvWBG1LqoNJwxuBMAJw+LF4TU77mgD48XnV/w1IUWabI6jwlCOTJhgPTz83lXcm8M3LxsaWo65oij5Yv/97bZ/f9tBiXsjff/7dvjE7+2K2MbRCYMLpQFWYPr1K2ydi01cGDp3tiFBfvjD8JqddrL/p/j6Dl9oX3nFbm+8EZ56qnD1jaHCUI40NtrojGecEZZlE4b6+mQvCEXJN34j5rKzpaVTp1AY/I7M4YdXn1eSe0435Js03DtmjN3GhSEpg+P3vgdf+lJ+65gFFYZypEMHKwwuTC9kX9egFoPSVoyBq69uea7q8MPDfX+oKA2+MHz0UVje1FR9wuACBjph6N27+TzhoEHW6vrNb6KjA74wXHVVYeuZARWGcqSxsbn3xpo1UR9m349chUFpK08/DRdcYN2js+F/z3wHiDT4wuCHle/Qwc6R5eJAcdZZ8LnP5fb+xcR15Nz/KD7c5nBzDpdfHpb5wlCklc5xVBjKETcuueOO4RdrzZroj9JfJq9DSUpbcY1y3InBGDvBnERLrtVxunWz6x82b45OsLqx85ZEyee662DOHJgxI7c6FAuXhdHFNoqLaDwa7euvh/v+M5WzV5KIjBWRmSIyW0QmZrjmWBGZISLTReSe2LmuIrJIRJrF5hWRSSLSzD1BRM4XESMiveLnqh73g/v003A5/Jo1YVaoo4+OmphqMShtxQ19xIeHbr89GsTNcfDBub9H377WMvnHP6xF/OUvw8svh73rTz/N/Z5xd9lywVn0/uSzz/e/Hz1+9lm7vf9+uOUWu3/iiUVb0BanRWEQkfbADcChwAhgvIiMiF0zDLgI2McYszNwduw2lwPPJtx7HNBs8FxEBgEHAx+ke4wqo67ObpcsCRt8XxgOOSTqrlpXZy2GpEkrRUlDfEwc4KGH4PTTk69/6KHc38O5Xn71q3Z76qlWdCZMsMe+FVzJuE6bn3grU1TUuGD4Q2y//31ZDyXtCcw2xswxxmwE7gWOjl1zOnCDMWYlgDFmqTshInsAfYEn/BeISGfgXOBnCe95LXAhUJzAIOWGb6LPmmW33/xmaObHzUsXZ+mSSwpfN6U6SbIY/vu/M1/fmiEOvzMDYaN4xhn2XFIIjST8idpS8eabyXkk1q+HCy+0+y7MDUQb+K99Ldx3v9nhw5tfV0LSCMMAwA+luDAo8xkODBeRF0TkJREZCyAi7YBrgAsS7nt5cC7SzRWRo4BFxpg3s1VKRM4QkakiMvVjtxKzWsg0dusshvgPyEVWveaawtVJqW6cMKRdUOWs2lzwXawhKkJdu6YfSvJ/736AuWIyapQdTvvRj6Ll/hyNH8XAnwM89thw/4IL7LGIfS4nFIMH57/OOZBGGCShLN6TrwOGAQcC44HbRKQ7cCYw2RgTidErIqOAHYwxD8fKOwIXAy12fY0xtxpjRhtjRvf2g0xVA5mEwY3Fxntr/hiwTkIrrcEPcwGFieIZD/fiD6M0NlqPpUyxlHz83NGlmJx1cwAA/+//Rc/5wnDvveH+LrvYhEWvvNJ83sA5j/ziF2HZv/6Vv/q2gjTCsBAY5B0PBBYnXPOoMWaTMWYuMBMrFGOACSIyD7gaOFlErgjK9wjKn8daG88AnwOGAm8G5wYCr4vItq16ukrF741demm47/Ljxn8Md90V7mdb76AoSV48H3wAv/tdeLx6dbSB23//qDvl7Nmte+9sFkOHDnDPPXD88S3fxwlDp06hFV1Mzj8/3I8v8vP/b77INTXBCy/YSMpxnDD4DiR+nucSkEYYXgWGichQEWkAjgMmxa55BDgIIPAiGg7MMcacYIwZbIwZApwP3G2MmWiMuckY0z8o3xd4zxhzoDHmbWNMH2PMkODcQmB3Y0xCQtkqxlkMffpE5w2cCRofSvJ7XhozScnEgw9aL56jjoqWx3unc+eG8ZDA5nT2G7T4XEFa4sLgf2/dd/q++1q+jxOGPn1KszDO73zFrXv/95d2vsB5FfrWfq6uwHmmRWEwxmwGJgBTgHeA+40x00XksmA+gODcchGZATwNXGCM0XCfrcV9KVyI3VNPjZ5PMp9vv91u1W1VyYSL5PmXv0SHiuKN6/z5MMJzPOzQIfQkgtYLg38PiApDmiEkhxOG3r1bthguu6y5IOWTeBhsXxjiuVUy4SyGMhoGTjWDZIyZDEyOlV3i7Rush9G5We5xJ3BnQvk8IDH+bmA11B5uKMn9ALOlTHSkyRWt1Db+0M2GDeH3KP6diXv9xBu41o7rf+MbVhymTLHHvjDkMiTkhKFHj2i00iR+8hO73bSpML3w+P/GF4Ybb0x3j/p6G23297/PX73aiK58LkecZ4gThrgHiApDbXP11bDXXtFJ2DT4vvR+Ssn48GP8OM33Ly39+9ttQ0O0Uc1lDY577m22Sd/LLpSLa3xOz83hTJ5shTANblK+jH67KgzliFs12bOn3fohiiGaOcuhwlA7XHqpXTH80ku5vc7/bvgNcfw7E++9u2GeSy+1ItGWoRknKvGFXa7hTuMu64She/fchGHjxmiOk3zgC+x3vwsnnRS+X1rKaAjJocJQjrh1CU4YunYNA4aJqDDUOq5hfvrp9K/ZutWGbnYMHhwmynEWgltMGe8Fu/OXXNL2RsxZwfHQGy7yaKZgcz6rV1vrp6Gh5Ybe9cZXrbJzJfX1mWM/tQYnDMuXR91Yv/zl9Pcow9+sCkM5Mn487LMPTPTCUjkPh+7dkxcXqTDUDq5jEPehz0aSi+nzz9ut+864uFw//nH0uny6hGayGNwq4m1TeKavXm2FpaXgkWvXhtaObyGlcYlNy4oVVpycKznYjlzfvunvEbfACjlZnhIVhnKkVy/7o/VXPzphyDS+64Thr38tbN2U0uNPoqZ113TWgI/rTW/YYL8/7r7xvAH5dIF239+4Z9P++1uLZt685u8fJ60w/PKX4b4vDH7I+nxw9tnR+Z603kiOuPVUYldVUGGoHNwPKdOXzpVfcUVx6qOUDt8q9BPeZGPRouZl/qRntsYsn6ug3fc4aS5hxAjbwK5Ykf0ea9ZYi6O+3vbWM9Vv1apw3xeGtlpAbgLdufS+/HL0/rk27PG8EpPiy8SKjwpDpeAshkxfuhLFbVdKwKZN4ZBLWmFw+Q/82EK+xZApgN3xx8NFF7Wunkm472mSEDnRaKnhdhaDG1LNZGH41oTfcLd1nmTLFhv4729/s8ef/3zUcst1KMiPYHvCCc3Xe5QAFYZKwQlDpuBluebfVSqXjRttWkhIn6hm9Wrb8L72WljWrp1t0O6+O7mh/vrX4Y9/tAl28oUblhoxovk5JxotDY/5Q0mQuaH/5JPwuX7zm7B8/fq2WUGffmrff/BgGxV18eJonZ3zSFr8GFJ/+EPr65VHVBgqBWdev/tu8nlfGKotf65i+eADuPZaG/DOdRS++c10r3XDLz5vv20nSdessbk/4hx5ZNvqm4R7n6FDm5/LZDG89170eNUqK1ZphMGtTPYzpIHN/tYaNm+2vy83L/DeezbxkD9UVwUOICoMlcI//5n9vD+B1dIYrVKZfOUrcG4QXODZZnmvMmMMPPBA89g9v/xl9kVyhZgEdZbCmDHNzzmLwReG++6zKW6fCNK5GGOHxbbdNprQKolly5qHrHC0tvPkXHnjFvrUqa27n+O22+Dmm9t2jzyiwlApuCxXmfDHiHNJqq5UDh/EEhqedFLoYpqNBx6wjeT8+S1f+6tfhfuFEIbTTrO97P33b37OWQx+o+06RG8G6Vk++cSe79cvrN/nP9/8XuvW2eCAo0fDfvs1P99aTysXcDAusnGrJle+9S34znfado88osJQKaRJwnPeeXab6xhnGqZOhaVLw+MZM+xipGuvzf97KcnE8xn06tW8E7Ddds3TcX4YC06c7bs0bly4X4hsYiIwbFjyuSSLIb4OYeFCux04MLtwzZ5tX7vXXsnzcq0d7nFDRvEEQdOnJy88rVBUGCqFNJ4O559vr8s1VEJLrFhhwy7/n/8Tlv3tb3bZ/6OP5ve9lMz4wvDcc7Yh3bDBjnGL2Ebrgw/ssIRP3GPtxBMzv4cvBsVOM+nezw8z4RIIubUHbrJ98ODmcyY+rgc/bFhUQK67zm5bKwxuPsNZar6l4BwC0mbBK2NUGKqJbbe1Jra/CjMfvPCC3T7zTFjm0jDOmVMVk20VgT/Esu++dvhw06YwimemnrhrqHbZxW6zrVnwA+0VWxhcj9tP3enmD9yaBNc5GTQoe45oXxicxbDDDrDbbna/td9Z9zr3Pxw2LPTaGjTIClg55KRuIyoMlcThh8M552S/pn//lkMR50o8sQvYpC8ACxakjyKptB5jojkLRMKG0VkSmSZUXQ98chA5P77S1sdvbIstDC5Frz9k6b7Lv/tddL1Cv37ZLYZZs2xHyXdrbWoKG/RDDgm/w7kQFwawIg1WGLp2zf7/rRBUGCqJxx6LLvNPImncOd9s2WLHVB1lsFKz6vHH3d1QkWuc/vzn7K91njSuR96+Pfz3fydf6w9ZFlsYGhpsw+rmyIyJehz5nj91dWGguvjk8+uvWyEZMiS81m39Bv2mm3KvoxMGf3jKWQwjE9PKVCQqDNVGx46FXcfwyCPw738X7v5KMm4o5cYbrQcLZB9K8VmzxoqBf/2999rFbq63m0SxhcG9p/v+btpkBdFZrFdcYcXNRYlt396uEo67jt56q926+FCuEa+vjwpDpsWi2XDC4L92/HgrCkmWdYWiwlBtNDXllvQkV/7v/7XhAADuuqtw76NEcZOv3buHZbkIQ+fOUWugrs561vzzn7DTTuFnCnZyukuX0iSkb2qyDg1btoQTvf362e0jjzQP39HY2HxBnAtXf+eddusa8fr66GunTAlT4qbFxZXy/5dHHGEXC7oYSlWACkO10dSUf4vB92qZOze0GE4+2SaX//+zLZAAACAASURBVMpX8vt+iuXdd+Gss+zcgrMY/PAU2YTBD/mQtOrZZ8aMaC6BO++04/xx99hiMHeufe/rrw97534o7vXro9/HJGHYutU23kccYY99YYhHdf32t+08RtrFZZs25R49tQJRYag2OnbMr8VgjP3hnXyyPR4+PHq+f/+oe6GSP/bay7pXfvhh7haDHyaiJWGI07596YMyzp8fCoOzXPr3t8/lP3fXrs3DaG/eHB3qcUNJdXXhBLfPMcfY4amkCLRxWopEWyWoMFQb+bYYXG9sp53snxOBhx6y206dVBgKhWvwjEm2GOK9Xx/3ub31Ftx/P8ycWZg6FoqmpqgH0LhxoSD4otWjh3UP9S2kuDD4FkPSvIJz704TjluFQalIOna0X96Wkp2kxYlMU5PtdTofbdcL69SpeSpIJb9s3pxsMfhWwMKFoVhD2Mg99VTh61cI1q+PCkNTkx1mgjA8Blhh2Lgx2hnKZjGAXQjq42KLZYsb5di4sSwS6RQaFYZqw/Wq8pV1yzUwTU12QtL9iFyvSS2GwrN5c7LF4PvLDxgABx8cHrvhxJZyIpcbO+1ktxs2NBcGx3HHhftOKP1FZXFhcL8J9/+66qrk904rDGoxKBWH+xG0dmWnMXYhlDPN4xaDW2Tlek2dO6swFJpNm6zF0L59dGVy3E2zc+fQanANZb4sx2Lxxhs2Btf69eE8SVwY/KB4PXrYbTZhcOLR0rzJggUt108nn5WKxH1pW2sx3HWXXWF9xx32OC4M8fdxFkM+0z8qUZzF0K1b1E0yaYXtNtvYrbPsKs1iaGiwjX3SUJLDF0QnDKtW2U7L8uW28U4Shpbijb38sg2+F4+U+uijofCoxaBUJG21GFwkTrc4yBcGf8GTP8dgjCYHKiROGPz5BQgbxaSy99+HXXeNjsdXCh06WGH44hftsS8MjY3RRt8974oVdu6gVy/biPvXOEvB77wsWBANMQ52rmzYMJv/wbF0qfVa2mYbKzgqDEpF0laLIR762E3U+XFm/PdxQxs6nFQ43ORzPMVm0tCIsxj+8Ae76OqBB+zxvfcWto75pLExGsdo/fpQGOKhrYcOtUNsL7wQhoB/443oBPH++8MBB8DEiWHZwIHwwx9Gr0v6DvtDcQ8/rJPPSoXiGuzWWgxxYXjuObs1JioM/hwDqGdSvvED5mWyGMCu8vUXGLoetB8va9CgaMj0cifeI//qV0NhiCcm6tXLLoDzn3fJkqjFsMMONjJwUvRZ3wq+777m5/31IOvWqcXgIyJjRWSmiMwWkYkZrjlWRGaIyHQRuSd2rquILBKR6xNeN0lEpnnHV4nIuyLylog8LCIJvwYlI231SvITsrvQ2mB7Tv7CIn8oCdRiyDf+0NyaNbZHnDSPs2wZ/P3v4XHHjrbh8tO7xi2NcsfvpV93nf3eue9l0jqEhobo9z0+x5CNbGtBIHrf11+3cxAqDCAi7YEbgEOBEcB4ERkRu2YYcBGwjzFmZ+Ds2G0uB5olqRWRcUC8q/l3YKQxZlfgveC+SlraajG4MAi+7zxYV0gdSioevgXmhoP8fBiZELGWhS8MfkiJSsC55kLYcLtt0gRyQ0Pz73taYWgpUKB/39/8xlojKgwA7AnMNsbMMcZsBO4Fjo5dczpwgzFmJYAx5j8B1UVkD6Av8IT/AhHpDJwL/MwvN8Y8YYxxrhQvAQPTP47SZovBmc5bt4aN05/+FI3/D+EQkgpD/tm0Keqr77uopsEtcnSkyQtdTowfH+476yGbMHTo0Pz7nlYYJk60Q03xrHa3327fO6mDVcgglWVCGmEYAPgOvguDMp/hwHAReUFEXhKRsQAi0g64Brgg4b6XB+ey/Ze/Cfwt6YSInCEiU0Vk6sd+xqdaxzXYf/1r617vC4Nb8OPcIv2ekpvk1DmG/PP661HrwOXgcNFCWyI+Ke1STlYKo0eH++575YbDkhrqhobmHZO0wnD66dYD75BDouXf/rZ12U56Pz8XSZWSRhiSnH/jg511wDDgQGA8cFswN3AmMNkYE1k5IiKjgB2MMQ9nfFORi4HNwB+TzhtjbjXGjDbGjO6dFBirVvF/VK3B/RC2bAl/lK7xd8LQ2BgOOanFkH8yBbwbNSrd6+Pj5gMrzOg+8shwP55kyJ/3cjQ0NA+k19LcQZykIaWVK5OFoQa8ktLI6kLA73IMBOK5IxcCLxljNgFzRWQmVijGAPuJyJlAZ6BBRNYA84E9RGReUIc+IvKMMeZAABE5BTgC+LIxunIqJ9q3t2PMaQKCJeFbDC5DlrMY3FCS/6NTYcg/mVYrp414Gr9uQNzArwB+9CP4+c/h2GPtsYuw6rKy+XToAP/6V7QsV2FIGq6rq0seko2vOK9C0lgMrwLDRGSoiDQAxwHxXI6PAAcBiEgv7NDSHGPMCcaYwcaYIcD5wN3GmInGmJuMMf2D8n2B9zxRGAv8D3CUMab6B/MKQVKM+mw8/rgN8bxpk/0xQrRxcmPUzmLwzXQdSso/mVYrpxWGF1+MHvft27b6lIL//V/rheXSdg4YYIdH7767+bUuuJ5PrmHDk4ae2rdPthiuuCK3e1cgLQpDMBE8AZgCvAPcb4yZLiKXiYjLZTcFWC4iM4CngQuMMa1NPHw90AX4u4i8ISIpM2go/6GxMbfJ5+OPt+EAFi8OvVn8uDHx+QTfx14thvzjC8Npp4X7ufaCHYMHt60+5cJhhyWv5Zg3r3lZrv+rUaNgzBg455yw7Mkn7W/CZ9Kk6FBXlZJqhsYYMxmYHCu7xNs3WA+jc7Pc407gzoTyecBI73iHNHVSsuBCCqRh2rQwDoyfeN1lafvhD0NPEJcy0Redhgbb21JhyA8ffgg/+Ul47A8DtSZ5zuLF4edWS+QqDL162eGo++8Pyx57zP6B/UwuvTR5KKsK0ZXP1UguQ0m77BLuf/BB8/N+HltnOcStEQ29nT8OO8wO7YFtjPy4PWmF4RvfCPddvuRq5sknm5f50VZzIUlQunSxn8WcOdHfSxWjwlCNJPl1p2H+fLv1F0T5jZHrvfp+5mCHmKpljiFfeSxay+zZ4f5BB0VXLaf1hnHurbXCiBHNy1rrqZhJGERsXKYaQYWhGsl18tnhhMH/cfhrF/r0sdfEE6dXi8Xw4IP2fzdtWsvXFgq/8a+riwpDS2GjHbXmvp30vJdc0rwsDXvsAV/4QrSs1PmvS0DKVSBKRdHYmC4bVdwT2IXc9hug+PL/pInMahGGRx6x29dfh5Ejs19bCNavj4ayiCfmSYu/Qr0W8D2K/ud/rPXq3FtzpUcPG53V/w3MmdO2+lUgKgzVSIcO8MQTNupm0virI+4W6cZlswlDEtWS99kJZdqeeVt5+mlrIQwZAueea1fb+sQthlz485/DNJm1wKxZ9juYdhFgS6xYEc6p1SAqDNWI6zG2lAjeDykMsGiR3bbzRhjTCENTk43wWekhiYstDF/6kt2ecYZtyOPj220Rhq9/vW11qzR2yLMzY1ISpBpC5xiqkbSTlPHFOzNn2q0vDGmGJVzYZ7c4rlIppjC4YTuAf/zDbuOLt+rqktN3KkqBUWGoRtI2bE4YbrjBLu5x+C55aSwA935PPJG8CjVXjIGpU9t+n9a8LxRHGHw3Ut8Tyad9+5qIy1P21OBnoMJQjbRL+bE6Yaivj84R+GPdaYTBrUZ98UXYfvt0752N3/zG5vt1PeliUeyhpJZwk6r/+pfN4ayUhlqbzEeFoTpp3z7ddU4YGhrgssvC8g4dYO+97X6mgG4++Y5z6BLYF9sbpNziNTphGDMmP4Kr5MaBB9qtCoNSFaTt8brJ54YGOOaY0AujoQG++127n8YnPq0QpcXdL40o5RPXKxexC938lJn5JinmT5wa9J8vK556yubKnhSPGVr9qDBUI2kbarcIzg0XueB4HTrASSfB0qWw224t3+eJJ1q+Ji0PPgi33RatT7F47TW73bIFfvpTm7wlHs45X/ToYYMXJrFsmXUzroVwFuVMu3Zw772h9VxDqDBUI/5kWbbhEZei0C2ictc6oUi7gnb33eG++3KrYyZ8N0vfYnjjjczhqPPNypWhILzzTmHeY+PGzBZBjx5hLgxFKQEqDNWIvxI028Izt1o5kzDkgp8aMV9DQO4+b79tLZf6eli3Lj/3zsb558Nzz9n9pIxh+WDDBmuZff/79tjlHYD0zgOKUiD0G1iN+BaDH0o7jhMGl9bQDd20Rhi6d4drrrH7RxyR++uTcCLgu8C+8UZ+7h3HH7YqRiC9jRutMLj3/cEPCv+eipISFYZqxBcGfyFVnExDSa31wnCJfB5/3IpOW+cI3n7bbv04TIWakM4U66lQFsqGDVaA3f+oXTs7+f3004V5P0XJARWGasQfSkpjMeRjKAmiq3Q7d44mnMnG7NnJDfM999jtWi/Da6EmpDNZCYUQBmOaWwzt2lmXVOciqSglRIWhGjnllHA/mzC4BtfF6MmnMEDYsGfDGBg2zLrLJtG7dzSEeKEshqTcvgA/+1n+32vzZvvcDQ1hTJ4aSDCvVA4aRK8aGT7cNqDt20fDOMdxvWEnDB062LLWrkuIN25pfPXd5PiTT4buoj7LlkWP44H/8oW777hx8NBDhXkPhxOhDh3gvPNg4EA49tjCvqei5IBaDNVKu3a2B58tL4PriTtheO45m+DETUbnSjxMcZq8u6tWhfujRydf4/fmk9KP5gP3HuPGFW7tgsMNWzU02P/RD36gnkhKWaHfxmqmS5fs7pbr1tnGyTVKu+xiE563NlZQXBjSzAf4wuDj5zr2rZ7TT8+9Xi2xdGmYt6K+PhpQEPIfKsO3GBSlDFFhqGZWr4b77898ft26dL36tLRGGJYvTy4fPjz5GpfDIJ/stx+ceabdT5pfyffwlW8xKEoZosJQzaxebcfws3nc5FMYGhttCAFHGmFYvDi5/Prrw/2PPgr3C5FV6733wn3XWPvzI75XVD5wn4daDEqZosJQzZx3nt1mCtmcb2EAG3TMkWahWJLX1G23RXNL+w33Aw9kHn7KB04YVq6EW26x+/nOZ+1HtVWUMkSFoZo5+mi7ff755POFEAafeAO+YgXccYe1JDZuhIkTYd685q9zC+Uc8UQ2P/5xXqsZwW+s3SS8WgxKjaHCUM307Wu33/lO8vn16wsb2jnuKnvzzfCtb8FFF1kPqF/8Ijpk5Ih7Ra1fDz17hsf5Dqbnz2f43kFu4V+hhEEtBqVM0XUM1YwThkwU2mLwF6aBHZ4BuPLKMBlPmtdBtBHt1q3tdfP57DO7dmPLFhgwICx3ApXvoSQXVlwtBqVMSWUxiMhYEZkpIrNFZGKGa44VkRkiMl1E7omd6yoii0SkWfdQRCaJyDTveBsR+buIzAq2PXJ9KCXANaAitsGL52MutDBs3hxdqewvnJsypfn1P/6xtSLcKugvfCE85zei+Uq96dxQ1661Xknr18PQoeH5QlkMbs5kzz3ze19FyRMtCoOItAduAA4FRgDjRWRE7JphwEXAPsaYnYGzY7e5HHg24d7jgHhc6InAU8aYYcBTwbHSWg45xDaAixfbXMo+hRKGyZOj7+FoqUFv3x4uvDAMAvjGG2Gymk6dQpfSTz5pex3feMMOGz3zjLUIOnZs3oMvlMWwebP9XJzwKEqZkcZi2BOYbYyZY4zZCNwLHB275nTgBmPMSgBjzFJ3QkT2APoCkTRfItIZOBeIB6M5Grgr2L8LyBBER0mFH2k13qAWShgOPTScO/B72y3FOUpKTuMaz86d4YYbbKC5fORIePFFu73mGjsRntRIO2E45hj49a/b/p6OQltqitJG0gjDAGCBd7wwKPMZDgwXkRdE5CURGQsgIu2Aa4ALEu57eXAubqf3NcYsAQi2fZIqJSJniMhUEZn68ccfp3iMGsUfm4+7jxaygUry6MkUqXS//Wwvet99m59zvXjXcHfrlh+LwQWve+yx6P19/NSaZ8eN4DagwqCUOWmEIcn+j8cIqAOGAQcC44HbRKQ7cCYw2RjjCwsiMgrYwRjzcM41dhUw5lZjzGhjzOjeaVNQ1iK+xeBP6m7dar2GChXV0wnDK6/YjGirViVPKgPsvHPmwH1O2FzD3bVrfiyGCy+MHieFvejWDfok9kvaxrp1hfUGU5Q2ksYraSEwyDseCMSXqy4EXjLGbALmishMrFCMAfYTkTOBzkCDiKwB5gN7iMi8oA59ROQZY8yBwEci0s8Ys0RE+gFLUVqPbzH4DfO8eXZV9K67FuZ9t9vObt2Ct0WLMjf+2erg6u/WNnTrBvPnt71+CxZEjzMNcxXCc2j9erUYlLImjcXwKjBMRIaKSANwHDApds0jwEEAItILO7Q0xxhzgjFmsDFmCHA+cLcxZqIx5iZjTP+gfF/gvUAUCO7tEgqcAjza6qdTMud8dr1uf31APtlrr+hCtXffzTyU9LnPZb5PISyGJBEYNiz52riA5AMdSlLKnBaFwRizGZgATAHeAe43xkwXkctE5KjgsinAchGZATwNXGCMyRAdrUWuAA4WkVnAwcGx0lr8sNt+7KJ4yO1C4McbGjgwszBkW5cQF4Z8zDEkher42tfads+0GKNDSUrZk2qBmzFmMjA5VnaJt2+wHkbnZrnHncCdCeXzgJHe8XIgwT1FaRW+xeDnNXCNdCEbqKuugvHj7X6/fjBrVvJ12YTBzZHELQZjWr+eIS4Me+/duvu0hnXrrEDHs90pShmhITGqHX/18z/+AdOn2/1LL7XbQloMI0eG+598YhvFgw+2MZJ84rGRfNyksLumSxfrwZQUYyktcWHwJ+iz8e9/t/49HW4YLN+rtxUlj6gwVDu//a1dU3DIIfZ45EjrIfRssN6wkBZDD2/RuhOGpqbmMYKyeUa5XAjOYujf327dOoTWEBeGtDGLdt+99e8JNiS5S1+qOZ6VMkaFodrp0we+//1o3gM/wX0hhWHAACtA++xjhcF54zhPn/p6mDMnN2FwiXoyzVekId7AFyuY3fjxcMQRdl8tBqWMUWGoFaZNC/evuSbcTzuM0lr239/OL6xaFVoMThi6dInGJkrCCYMbSnIC0ZYwFfGor8UQhvg6iYEDC/+eitJKVBhqmW23jSbEKRTdu0eHktzEa0shMiD0pHKC4LaZ3HBbQzZhePzx/LyH/6x1dTa/tqKUKSoMtczvf28bqULjXEydMHz+87Y8jdupq58TBNeIX3xx2+v1jW/YbbbJ769+1Vo9jjRZ6ZLwPcJmz8682E9RygAVhlqmWL70IjZm0mef2fccNKjl1zjcUJcbisnkorpxY7oc0z5x0cmELxytXUPhC8q227buHopSJFQYaoUJE5qXFStRzNtvh/tNTbmttnYWgp+1beTI6D22bLHP4nJcZ8NfS9Grl922lNDojjvCtQ6tFQbfYtAEPUqZo8JQK1x3XTRPAhSvgdphh3Dfn2NIwy23wIknwgEHhGV77BHtxbtV3L/6Ffzud83vMWOGXcMBcNhhYfl558HPf96yoPTtG669yIcwKEqZo8JQS8SHTNJM/uaDq64K95ua7HDQkUcmN+Jxhg61cyG+iDU0RBtaf5jmm99sfo+ddw5zPfjeTAMG2PzTaQTSuZe+807L1ybh6nvgga17vaIUEc35XEvEJ1mLZTH4q6vd/qR4HMYcqK+HJUtg2TI7HJQpnHcSzv31jDNym3gfMsRuJ0yA99+3k9+5uPo6Yfjud9O/RlFKhFoMtYRrCIcOtcNKI0Zkv74Q5CMEh2uQXSKduKdQUm4Fh2ugc42z5Nx6P/3UhhP55S9ze72rY7EW0ylKG1BhqCWcN8yECTb9ZinIhzC4Vc+bN1trIS4Mf/hD5tc6YcgmHmlYsiS36996y241qZRSAagw1BJ9+tiFYeecU/z3diuc8+Eiu9yL6L733jBzZvR8tuGa1q5DiJOrxbFwod2OHp2f91eUAqLCUGt06tT6cNVtwXki5cNi8IXh3/+GY45Jvm7r1uaWQVsthWwYk3kthZsHUVdVpQJQYVCKg5sXyEfDGI91FMcYeOQRu7r4llui5fniww+bl40YAV/5SvL169dba6kUoqwoOaLCoBQHJwz+QrXW4iKsxnk0yAK7bl2Yke3GG8PzbU0J6jNnjt2+/DJcdpndf/ddePrp5Os1z7NSQagwKMXB5VTOxxzDlVfC7bc3L9955+armOfODfeXLQv3M/Xs07J2bXifn/yk5YVvkyfDypVte09FKRIqDEpxuPFG+NOf7KrltlJfD8OHNy/v16+58PhRWD/+2G5/+9swgF5rcZ5Rbmhs6tTwnFsr4fP++217P0UpIioMSnHo3BmOOy5/90sKRNexY3aLxLmM5iOInbMY3LoE3wLJZ0hwRSkBuvJZqUy23z7cv+OOcGI5mzB85zt229oJ8CVLwkV1a9fa5EdJ6xlWr46mNb3rrta9n6KUCBUGpTJp5xm7p50W7qdZQNZaYfAtjU8+yZxsZ7vtYP78cLX0qafabZror4pSBqgwKJXL1KnNo5amyfVQjLUEb73VPDuei7ekKGWOCoNSuSRNZKfJpdwWYXj9dViwAI4+Ovt1SesVipUYSVHaiE4+K9WFLwzTpiVf0xZh2G03OOooGwo8G/7qbIcKg1IhqDAo1YUThq5dw9zScfIxlHTiifDii9GyffYJ9085pflrVBiUCkGFQakunDA0NtqQGI7ttgv38zXH4AIDOh54IHo8fz6MHRseqzAoFYIKg1JduMnnk0+22zvusGk9p02DvfayZfkKTdGnD/zv/0aPfUaOhClTwmMVBqVCSCUMIjJWRGaKyGwRmZjhmmNFZIaITBeRe2LnuorIIhG53it7XETeDK6/WUTaB+WjROQlEXlDRKaKyJ5teUClxujRAxYvhiuusMennQYHHWQX2D31FDz7LGyzTX7eSwR+9KPwuF07G4/pyivtcXyhmwqDUiG0KAxBg30DcCgwAhgvIiNi1wwDLgL2McbsDJwdu83lwLOxsmONMV8ARgK9ARej4ErgUmPMKOCS4FhR0tOvX3QYydGxI+y/f/7fb9y4cL9LFzj//OTrVBiUCiGNxbAnMNsYM8cYsxG4F4j76p0O3GCMWQlgjFnqTojIHkBf4An/BcYYF+qyDmgAXExkA3QN9rsBi1M/jaKUgvvus6udHSLRlc8OzcWgVAhphGEAsMA7XhiU+QwHhovIC8Ew0FgAEWkHXANckHRjEZkCLAVWA27m7mzgKhFZAFyNtUSSXntGMNQ09WMXHE1RSkFdnR2q8kmyWOp02ZBSGaQRhqTMIvGMJ3XAMOBAYDxwm4h0B84EJhtjFpCAMearQD+gA+CC7H8POMcYMwg4B0iIrwzGmFuNMaONMaN7ax5dpdxwGdsgnOxOEgtFKUPSCMNCwI8zMJDmwzsLgUeNMZuMMXOBmVihGANMEJF52N7/ySJyhf9CY8x6YBLh8NQpwEPB/p+xQ1mKUln47rFuhbYm6lEqhDS27avAMBEZCiwCjgOOj13zCNZSuFNEemGHluYYY05wF4jIqcBoY8xEEekMdDHGLBGROuAw4J/BpYuBA4BnsFbErFY+m6KUjr/+1eaf6N0bjjzSuq2mieOkKGVAi8JgjNksIhOAKUB74A5jzHQRuQyYaoyZFJw7RERmAFuAC4wxCTEB/kMnYJKIdAju+Q/g5uDc6cCvA8FYD5zRymdTlNKx3XYw0fPsPumk0tVFUXJETD4TpJeI0aNHm6l+Bi1FURSlRUTkNWPM6Hi5rnxWFEVRIqgwKIqiKBFUGBRFUZQIKgyKoihKBBUGRVEUJYIKg6IoihJBhUFRFEWJUBXrGETkY2B+K1/eC1iWx+qUgmp4BqiO59BnKA/0GdKxnTGmWbC5qhCGtiAiU5MWeFQS1fAMUB3Poc9QHugztA0dSlIURVEiqDAoiqIoEVQY4NZSVyAPVMMzQHU8hz5DeaDP0AZqfo5BURRFiaIWg6IoihJBhUFRFEWJUNPCICJjRWSmiMwWkYktv6I0iMggEXlaRN4Rkeki8sOgfBsR+buIzAq2PYJyEZHfBM/1lojsXtonCBGR9iLybxF5LDgeKiIvB89wn4g0BOUdguPZwfkhpay3Q0S6i8gDIvJu8HmMqbTPQUTOCb5H00TkTyLSWAmfg4jcISJLRWSaV5bz/15ETgmunyUip5TBM1wVfJ/eEpGHRaS7d+6i4BlmishXvfLCtl3GmJr8w2aOex/YHmgA3gRGlLpeGeraD9g92O8CvAeMAK4EJgblE4FfBPuHAX8DBNgLeLnUz+A9y7nAPcBjwfH9wHHB/s3A94L9M4Gbg/3jgPtKXfegLncB3w72G4DulfQ5AAOAuUCT9/8/tRI+B2B/YHdgmleW0/8e2AaYE2x7BPs9SvwMhwB1wf4vvGcYEbRLHYChQXvVvhhtV0m/pKX8A8YAU7zji4CLSl2vlHV/FDgYmAn0C8r6ATOD/VuA8d71/7muxPUeCDyFzeX9WPCjXeb9KP7zmWDTxY4J9uuC66TE9e8aNKoSK6+YzyEQhgVBw1gXfA5frZTPARgSa1Rz+t9jc9Pf4pVHrivFM8TOfQ34Y7AfaZPcZ1GMtquWh5LcD8SxMCgrawJTfjfgZaCvMWYJQLDtE1xWrs/2K+BCYGtw3BNYZYzZHBz79fzPMwTnPwmuLyXbAx8DvwuGw24TkU5U0OdgjFkEXA18ACzB/l9fo7I+B59c//dl95nE+CbW0oESPkMtC4MklJW1766IdAYeBM42xnya7dKEspI+m4gcASw1xrzmFydcalKcKxV12GGAm4wxuwGfYYcvMlF2zxCMwR+NHZroD3QCDk24tJw/hzRkqnfZPo+IXAxsBv7oihIuK8oz1LIwLAQGeccDgcUlqkuLiEg9VhT+aIx5KCj+SET6Bef7AUuD8nJ8tn2Ao0RkHnAvdjjpV0B3EakLrvHr+Z9nCM53A1YUs8IJLAQWGmNeDo4ff3t+TwAAAaFJREFUwApFJX0OXwHmGmM+NsZsAh4C9qayPgefXP/35fiZEEyCHwGcYILxIUr4DLUsDK8CwwJvjAbsxNqkEtcpERER4HbgHWPML71TkwDnVXEKdu7BlZ8ceGbsBXzizO1SYYy5yBgz0BgzBPu//ocx5gTgaeDrwWXxZ3DP9vXg+pL27IwxHwILRGTHoOjLwAwq6HPADiHtJSIdg++Ve4aK+Rxi5Pq/nwIcIiI9AuvpkKCsZIjIWOB/gKOMMWu9U5OA4wLPsKHAMOAVitF2lWICqVz+sJ4L72Fn+C8udX2y1HNfrKn4FvBG8HcYdqz3KWBWsN0muF6AG4LnehsYXepniD3PgYReSdsHX/bZwJ+BDkF5Y3A8Ozi/fanrHdRrFDA1+CwewXq2VNTnAFwKvAtMA36P9Xop+88B+BN2XmQTttf8rdb877Hj+LODv9PK4BlmY+cM3G/7Zu/6i4NnmAkc6pUXtO3SkBiKoihKhFoeSlIURVESUGFQFEVRIqgwKIqiKBFUGBRFUZQIKgyKoihKBBUGRVEUJYIKg6IoihLh/wORob37k1U1+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss_AttTrack48.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "#test_error= np.loadtxt('./test_loss_AttTrack48.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=2000\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "#plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7568, 0.8246, 0.8182, 0.8218, 0.8209, 0.8239],\n",
      "        [0.8227, 0.8756, 0.8853, 0.9984, 0.8936, 0.9028],\n",
      "        [0.8273, 0.9983, 0.8994, 0.8774, 0.8847, 0.9985],\n",
      "        [0.8317, 0.9984, 0.8964, 0.8814, 0.8947, 0.9983],\n",
      "        [0.8311, 0.8930, 0.9988, 0.8929, 0.9988, 0.8680]])\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 7, 8, 9, 10, 11]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 12, 12, 12, 12, 12, 12]\n",
      "[5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 2431, 1753, 1817, 1781, 1791, 1760, 1773, 9327, 8494, 15, 8936, 9127, 1726, 17, 8351, 9120, 7661, 14, 1683, 15, 8858, 9387, 8313, 17, 1689, 8480, 12, 8586, 11, 8676, 0, 0, 0, 0, 0, 0]\n",
      "[6, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -6]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 7, 8, 9, 10, 11]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 6, 7, 8, 9, 10, 11, 12, 12, 12, 12, 12, 12]\n",
      "[5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 2431, 1753, 1817, 1781, 1791, 1760, 1773, 9327, 8494, 15, 8936, 9127, 1726, 17, 8351, 9120, 7661, 14, 1683, 15, 8858, 9387, 8313, 17, 1689, 8480, 12, 8586, 11, 8676, 0, 0, 0, 0, 0, 0]\n",
      "[6, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -6]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 8, 9, 10, 6, 7, 8, 9, 10, 6, 7, 8, 9, 10, 6, 7, 8, 9, 10, 11, 11, 11, 11, 11]\n",
      "[4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 2040, 1970, 1969, 1910, 2043, 2031, 20, 6591, 9693, 6503, 2256, 9087, 9597, 8663, 9585, 1933, 9709, 9818, 16, 9808, 2078, 4188, 39, 9800, 38, 0, 0, 0, 0, 0]\n",
      "[5, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, -5]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]\n",
      "[11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2431, 2609, 2096, 2454, 2224, 2483, 2211, 2357, 2577, 2637, 2219, 2171, 2462, 7592, 6988, 4646, 5838, 5096, 6492, 8680, 48, 165, 8711, 8052, 2451, 2841, 8442, 34, 8876, 7798, 8844, 7588, 3815, 4745, 8269, 9240, 2197, 8856, 8984, 8814, 78, 132, 43, 9022, 5685, 4097, 8741, 92, 2574, 71, 9344, 5750, 9253, 7604, 9077, 82, 7864, 6077, 2413, 9397, 2279, 9100, 8286, 8729, 34, 5694, 66, 9318, 4301, 6131, 9192, 87, 2011, 9224, 13, 8093, 8369, 9111, 9008, 9497, 6791, 8696, 9514, 9102, 2436, 7531, 8978, 7926, 5172, 63, 133, 8211, 2676, 114, 7715, 6317, 2279, 5062, 9601, 8790, 9401, 8079, 9131, 46, 8967, 8044, 31, 9456, 2352, 131, 9495, 7997, 9260, 7307, 8957, 53, 8471, 6747, 46, 9346, 2123, 9494, 9297, 9464, 2488, 8012, 2617, 9537, 8734, 8689, 9494, 28, 2643, 313, 8949, 3993, 8177, 656, 7436, 5470, 1613, 106, 5786, 8692, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -12]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 16, 16, 16, 16, 16, 16]\n",
      "[7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2040, 1937, 1877, 1864, 1915, 1914, 1862, 1904, 1859, 1158, 36, 9657, 92, 3713, 9708, 5850, 1838, 57, 4645, 9774, 4856, 61, 9792, 7391, 1872, 90, 5604, 9796, 206, 50, 9817, 4461, 1858, 5071, 102, 9734, 53, 2663, 9779, 65, 1868, 9779, 9717, 36, 9795, 9805, 13, 9818, 1815, 9757, 9676, 22, 9755, 9783, 38, 9783, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[8, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -8]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
      "[8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2431, 1723, 2038, 2067, 2076, 2023, 2181, 2144, 2174, 2132, 9164, 6856, 8403, 9195, 91, 92, 1227, 52, 2032, 8831, 6761, 1393, 32, 9564, 8832, 8873, 9300, 2101, 8898, 1569, 6827, 8710, 6344, 63, 89, 83, 2063, 8325, 1673, 6757, 8641, 7287, 90, 55, 676, 1926, 8295, 39, 42, 4783, 8899, 3204, 3884, 7488, 2497, 9520, 8317, 7826, 7992, 9533, 8915, 9217, 9310, 2514, 9435, 9050, 8351, 6224, 9743, 9505, 9527, 9650, 2039, 9368, 8372, 9041, 9446, 37, 4591, 6462, 53, 1747, 8, 8300, 8120, 8254, 9495, 9015, 8282, 9268, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -10]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]\n",
      "[13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2431, 2616, 2252, 2369, 2574, 2358, 2220, 2553, 2686, 2232, 2320, 2354, 2539, 2398, 2427, 9138, 9596, 9125, 8746, 8085, 8223, 9445, 6269, 9158, 5609, 401, 6425, 60, 2565, 5414, 8961, 6916, 260, 8537, 6205, 8329, 88, 9085, 6026, 305, 3907, 6216, 2272, 8991, 9412, 8154, 9094, 26, 9460, 9596, 7906, 77, 9327, 8961, 1281, 7062, 2277, 1213, 31, 3278, 6993, 9369, 9277, 8314, 8412, 9212, 9449, 9352, 8789, 9457, 2525, 7562, 9127, 6481, 7228, 4666, 8691, 9236, 162, 7782, 8291, 6334, 47, 2237, 2223, 8682, 9421, 9190, 7222, 9490, 29, 8055, 7549, 9602, 99, 3801, 8991, 8584, 2412, 86, 4148, 104, 840, 8957, 8857, 7686, 5611, 8951, 9121, 8763, 7428, 9101, 2272, 1013, 3880, 48, 6704, 8708, 9239, 8830, 7193, 8395, 9367, 9115, 7152, 9140, 2401, 8847, 9474, 8614, 8508, 5555, 8689, 9430, 4425, 8474, 7707, 5024, 196, 58, 2168, 8978, 9229, 7695, 9158, 62, 9533, 9605, 8481, 25, 9469, 9223, 5173, 8510, 2562, 8392, 9460, 8737, 6968, 8775, 5205, 9032, 328, 9269, 155, 83, 6467, 2386, 2478, 115, 7610, 2151, 110, 8914, 7754, 7030, 206, 9067, 8408, 7375, 6233, 8474, 2630, 9531, 9727, 9540, 9352, 9178, 8752, 9615, 8775, 9542, 6568, 6414, 8885, 5564, 2374, 8841, 9509, 9174, 7809, 9308, 94, 8925, 6539, 9526, 77, 121, 8531, 7268, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -15]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23]\n",
      "[10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2307, 2551, 2202, 2750, 2196, 2572, 3040, 2154, 2662, 2083, 2364, 2546, 59, 8295, 5807, 8578, 9568, 6986, 8663, 175, 8889, 6078, 2511, 9584, 8710, 9747, 9713, 55, 9639, 7844, 9610, 8028, 9760, 2930, 7160, 9367, 2239, 9587, 9682, 66, 9282, 2214, 9178, 9086, 2623, 186, 8804, 3233, 9339, 9559, 133, 8645, 91, 8612, 8593, 2068, 8742, 5477, 9378, 9524, 7188, 8974, 45, 8776, 36, 9540, 2417, 6342, 9130, 8018, 6607, 9729, 9073, 9399, 8109, 9496, 42, 2207, 8571, 8981, 9287, 39, 9691, 9507, 9417, 9145, 9537, 6493, 2763, 3686, 9284, 62, 9319, 9699, 3658, 9278, 177, 9319, 7586, 2071, 8518, 41, 9405, 9288, 8008, 9235, 78, 8886, 4734, 9381, 2031, 8890, 7393, 9420, 9602, 7594, 8898, 50, 8849, 27, 9568, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -11]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 7, 8, 9]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 10, 10, 10]\n",
      "[3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 2040, 2021, 1829, 1765, 1756, 9720, 9722, 28, 1772, 65, 22, 9732, 2008, 27, 2806, 9734, 2183, 9533, 9570, 6634, 0, 0, 0, 0]\n",
      "[5, 1, 1, 1, 1, 1, 0, 0, 0, 0, -5]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28]\n",
      "[14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2431, 2427, 2565, 2272, 2277, 2525, 2223, 2412, 2272, 2401, 2168, 2562, 2478, 2630, 2374, 2609, 243, 4169, 8967, 9459, 6997, 5052, 9029, 9259, 5132, 9266, 98, 8092, 4659, 115, 2096, 8900, 9006, 30, 9309, 7133, 9585, 9037, 8616, 7920, 26, 9168, 9107, 9444, 9478, 2454, 73, 6201, 6613, 9447, 1559, 8700, 8994, 9094, 50, 8361, 3376, 8370, 6137, 7548, 2224, 9185, 7546, 8268, 1984, 7102, 9231, 77, 40, 8764, 7701, 8872, 4200, 9571, 9242, 2483, 8214, 111, 8839, 7419, 5685, 7040, 1634, 6524, 7832, 8964, 5380, 73, 9201, 7137, 2211, 9176, 6044, 8847, 110, 7405, 8846, 58, 83, 8836, 8724, 8530, 113, 9544, 8949, 2357, 6566, 6601, 9376, 9455, 8496, 64, 9160, 9386, 8092, 9498, 1236, 8512, 7161, 55, 2577, 6407, 902, 4325, 8576, 46, 8732, 6639, 6705, 1220, 6380, 5702, 4540, 8894, 8231, 2637, 6147, 88, 8020, 8432, 174, 7311, 5739, 7299, 4570, 8560, 276, 174, 8760, 6167, 2219, 7863, 6183, 9429, 9331, 8620, 33, 8951, 9285, 8533, 9505, 4076, 8032, 8463, 59, 2171, 9413, 8230, 9111, 43, 8545, 9306, 98, 81, 9219, 8910, 9169, 5040, 9650, 9326, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -15]\n",
      "____________________________________\n",
      "There was an issue with the min cost flow input.\n",
      "Status: Status.UNBALANCED\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-de8242981d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mAd_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAd_real\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m '''\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "src1, src2, y,d = collate_fn(10,-100,train=False)\n",
    "        \n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "print(Ad[0])\n",
    "\n",
    "Ad_real = complete_postprocess(Ad,d)\n",
    "print(Ad_real[0])\n",
    "print(y[0])\n",
    "'''\n",
    "\n",
    "print(Ad[0])\n",
    "print(d[0])\n",
    "\n",
    "#Ad = torch.mul(Ad, d)\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "g=d[0].detach().numpy()\n",
    "\n",
    "print(np.multiply(f,g))\n",
    "print(y[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ad = postprocess_2(Ad)\n",
    "print(Ad[0])\n",
    "Ad=postprocess_MinCostAss(Ad)\n",
    "print(Ad[0])\n",
    "#torch.manual_seed(344)\n",
    "\n",
    "#Ad = torch.rand(1,3,4)\n",
    "#print(Ad[0])\n",
    "\n",
    "f=Ad[0].detach().numpy()\n",
    "\n",
    "l=np.ones(len(f))*2\n",
    "l=l.astype(int)\n",
    "f2=np.repeat(Ad[0].detach().numpy(), l, axis=0)\n",
    "\n",
    "#print('f2',f2)\n",
    "\n",
    "row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "\n",
    "#print(row_ind,col_ind)\n",
    "\n",
    "z=np.zeros(f.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "\n",
    "print(z)\n",
    "        \n",
    "f2[0::2, :] = z[:] \n",
    "        \n",
    "#print('f2',f2) \n",
    "\n",
    "\n",
    "row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "for i,j in zip(row_ind_f, col_ind_f):\n",
    "        z3[i,j]=1\n",
    "\n",
    "\n",
    "print(z3)\n",
    "\n",
    "f_add = z3[0::2, :] + z3[1::2, :]\n",
    "\n",
    "print('f_add',f_add)\n",
    "        \n",
    "z2 = np.zeros(f.shape)\n",
    "zero_col=np.where(~z.any(axis=0))[0]\n",
    "ind=torch.argmax(Ad[0][:,zero_col], dim=0)\n",
    "        \n",
    "print(Ad[0][:,zero_col])        \n",
    "print(np.where(~z.any(axis=0))[0])\n",
    "print(ind)\n",
    "print(z)\n",
    "\n",
    "for k,l in zip(ind,zero_col):\n",
    "    z2[k,l]=1\n",
    "    \n",
    "print(z+z2)    \n",
    "\n",
    "pp_A=postprocess(Ad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(pp_A[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon\n",
    "run=14\n",
    "src1, src2, y = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "\n",
    "print(src1.size())\n",
    "src1= src1.to(DEVICE)\n",
    "src2= src2.to(DEVICE)\n",
    "    \n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "#transformer.load_state_dict(torch.load('AttTrack.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()    \n",
    "    \n",
    "\n",
    "Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "Ad = postprocess_2(Ad)\n",
    "pp_A=postprocess_MinCostAss(Ad)\n",
    "#pp_A=postprocess_linAss(Ad)\n",
    "\n",
    "print('y',y[0])\n",
    "print('Ad',Ad[0])\n",
    "print('pp',pp_A[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "make_reconstructed_edgelist(pp_A,run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x=np.arange(1,51,dtype=int)\n",
    "y=np.arange(1,7,dtype=int)\n",
    "\n",
    "l=[]\n",
    "for i in range(5):\n",
    "    z=np.random.choice(x, replace=False)\n",
    "    l.append(z)\n",
    "print(l)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)\n",
    "s=np.random.random_integers(12)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('/home/mo/Desktop/IWR/CellTracking/Fluo-C2DL-Huh7/02_GT/TRA/man_track001.tif')\n",
    "im.show()\n",
    "\n",
    "print(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_drop=0.05\n",
    "learning_rate=0.0001 #0.001 for cnn\n",
    "epochs = 2000\n",
    "emb_size=6   #!!!!!!!!!!!!!!!!!!!!\n",
    "seq_length=104\n",
    "d_m=12*20\n",
    "nhead= 3\n",
    "num_encoder_layers=4\n",
    "\n",
    "model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "#model=MiniLin(ch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler=optim.lr_scheduler.MultiStepLR(optimizer,milestones=[250,750,1000,1500,2000,2500], gamma=0.5)\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "#loss_function = myL_loss(100,100)\n",
    "\n",
    "\n",
    "model, loss_over_time, test_error = train_easy(model, optimizer, loss_function, epochs, scheduler,verbose=True,eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=0\n",
    "\n",
    "\n",
    "\n",
    "a = torch.ones(5, 6)*2\n",
    "b = torch.ones(2, 6)\n",
    "c = torch.ones(4, 6)\n",
    "c2 = torch.ones(4, 6)/2\n",
    "\n",
    "print(c)\n",
    "print(c2)\n",
    "\n",
    "\n",
    "#torch.matmul(d, e) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = pad_sequence([a, c])\n",
    "e = pad_sequence([b, c2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d.size(),e.size())\n",
    "#print('d',d[:,1,:],d[:,1,:].size())\n",
    "\n",
    "mask1=create_mask(d,PAD_IDX)\n",
    "mask2=create_mask(e,PAD_IDX)\n",
    "\n",
    "\n",
    "d=torch.transpose(d,0,1)\n",
    "e=torch.transpose(e,0,1)\n",
    "e=torch.transpose(e,1,2)\n",
    "\n",
    "#print('d2',d,d.size(),d[1,:,:])\n",
    "#print('e2',e,e.size(),e[1,:,:])\n",
    "\n",
    "\n",
    "#d=torch.reshape(d, (d.size(1), d.size(0), d.size(2)))\n",
    "#e=torch.reshape(e, (e.size(1), e.size(2), e.size(0)))\n",
    "\n",
    "\n",
    "#print(d,d.size())\n",
    "#print('e',e,e.size(),e[0,:,:])\n",
    "\n",
    "\n",
    "\n",
    "z=torch.bmm(d,e)\n",
    "\n",
    "#print(z[0],z[1])\n",
    "print(mask1[1],mask2[1])\n",
    "\n",
    "#model = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#out=model(d,e,mask1,mask2)\n",
    "#print(out.size())\n",
    "\n",
    "\n",
    "mA=makeAdja()\n",
    "Ad=mA.forward(z,mask1,mask2)\n",
    "\n",
    "print(Ad.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "\n",
    "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
    "# pip install -U torchdata\n",
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        print('PE',token_embedding.size(),self.pos_embedding[:token_embedding.size(0), :].size())\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src,src.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        print('trans_src',src_emb,src_emb.size())\n",
    "        print('trans_src_padd',src_padding_mask,src_padding_mask.size())\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        print('outs',outs.size())\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    print('src_size',src.size())\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        #print('src_sample',src_sample)\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        #print('emb',src_batch[-1])\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        \n",
    "        \n",
    "        #print('trainsrc',src,src.size())\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        #print('trainsrc_padd',src_padding_mask,src_padding_mask.size())\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "\n",
    "\n",
    "# function to generate output sequence using greedy algorithm\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
