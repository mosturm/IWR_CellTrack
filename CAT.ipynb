{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12,path0='.'):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph(path0=path0)\n",
    "        elif recon: \n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run_r=run,t_r=j,path0=path0)\n",
    "            #print('recon',run)\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False,path0=path0)\n",
    "        #print('src_sample',src_sample)\n",
    "        #print(A)\n",
    "        if torch.sum(A) != 0:\n",
    "            src1_batch.append(E1)\n",
    "            #print('emb',src_batch[-1])\n",
    "            src2_batch.append(E2)\n",
    "            y_batch.append(A)\n",
    "            d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src1_batch[3])\n",
    "    #print('src2_batch',src2_batch[3])\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:],src1_batch[:,0,:].size())\n",
    "    #print('src2',src2_batch[:,0,:],src2_batch[:,0,:].size())\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run_r=None,easy=False,recon=False,t_r=None,path0='.'):\n",
    "    \n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        run=select_random_run(path0)\n",
    "        try:\n",
    "            E=np.loadtxt(path0+'/'+str(run)+'/'+'embed.txt')\n",
    "        except:\n",
    "            run = f\"{int(run):02}\"\n",
    "            E=np.loadtxt(path0+'/'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt(path0+'/'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt(path0+'/'+str(run)+'_GT'+'/TRA/'+'A.txt')\n",
    "        D=np.loadtxt(path0+'/'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(find_max_t(path0+'/'+str(run))) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        A=modify_matrix(A)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "    elif recon: \n",
    "        run=run_r\n",
    "        #print('recon_run',run)\n",
    "        E=np.loadtxt(path0+'/'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt(path0+'/'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt(path0+'/'+str(run)+'_GT'+'/TRA/'+'A.txt')\n",
    "        D=np.loadtxt(path0+'/'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        #print(run,t,id1,id2)\n",
    "        if t < find_max_t(path0+'/'+str(run)):\n",
    "            id1 = id[tt==t].astype(int)\n",
    "            if t==0:\n",
    "                id1=id1[1:]\n",
    "            id2 = id[tt==(t+1)].astype(int)\n",
    "        else:\n",
    "            return torch.zeros(2), torch.zeros(2), torch.zeros(2), torch.zeros(2)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        A=modify_matrix(A)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        run=select_random_run(path0)\n",
    "        try:\n",
    "            E=np.loadtxt(path0+'/'+str(run)+'/'+'embed.txt')\n",
    "        except:\n",
    "            run = f\"{int(run):02}\"\n",
    "            E=np.loadtxt(path0+'/'+str(run)+'/'+'embed.txt')\n",
    "            \n",
    "            \n",
    "        id,tt = np.loadtxt(path0+'/'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt(path0+'/'+str(run)+'_GT'+'/TRA/'+'A.txt')\n",
    "        D=np.loadtxt(path0+'/'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(find_max_t(path0+'/'+str(run))) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        A=modify_matrix(A)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn,train_path):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100,path0=train_path)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    #print('src1',src1.size(),src1)\n",
    "    \n",
    "    #print('src1_mask',src_padding_mask1.size(),src_padding_mask1)\n",
    "    #print('src1_0',src1[:,0,:].size(),src1[:,0,:])\n",
    "    #print('src1_0_mask',src_padding_mask1.size(),src_padding_mask1[:,0,:])\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        \n",
    "        \n",
    "    #Ad = complete_postprocess(Ad,d,0.01) #!!!!!!!!!\n",
    "    print('Ad',Ad[0],y[0])\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    #Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            #if self.trans:\n",
    "            #    Ad[i]=convert_tensor(Ad[i])[0]\n",
    "                \n",
    "            \n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            #if i==0:\n",
    "             #   print('loss',Ad[i], y[i],s)\n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn,test_path):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False,path0=test_path)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "      \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "def find_max_t(run_folder_path):\n",
    "    \"\"\"\n",
    "    Returns the highest file number (x) in the given run folder for both PNG and TIF files.\n",
    "    The files are expected to be in formats like x.png or txxx.tif.\n",
    "    \"\"\"\n",
    "    png_files = [f for f in os.listdir(run_folder_path) if f.endswith('.png')]\n",
    "    tif_files = [f for f in os.listdir(run_folder_path) if f.endswith('.tif')]\n",
    "    \n",
    "    if not png_files and not tif_files:  # If both lists are empty\n",
    "        return -1  # No PNG or TIF files in the folder\n",
    "\n",
    "    # Extract the numbers from PNG filenames, convert to int\n",
    "    png_numbers = [int(f.split('.')[0]) for f in png_files]\n",
    "    \n",
    "    # Extract the numbers from TIF filenames (considering the \"t\" prefix), convert to int\n",
    "    tif_numbers = [int(f[1:].split('.')[0]) for f in tif_files]\n",
    "\n",
    "    # Combine both lists and sort\n",
    "    all_numbers = sorted(png_numbers + tif_numbers)\n",
    "\n",
    "    return all_numbers[-1] \n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        #print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    #print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    #print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        #print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def get_run_folders(path):\n",
    "    \"\"\"Returns a list of run numbers from folders ending with '_GT'.\"\"\"\n",
    "    run_numbers = []\n",
    "    \n",
    "    for folder in os.listdir(path):\n",
    "        if folder.endswith(\"_GT\"):\n",
    "            run_number = int(folder.split('_')[0])\n",
    "            run_numbers.append(run_number)\n",
    "\n",
    "    return run_numbers\n",
    "\n",
    "def select_random_run(path):\n",
    "    \"\"\"Selects and returns a random run number.\"\"\"\n",
    "    run_numbers = get_run_folders(path)\n",
    "    if not run_numbers:\n",
    "        print(\"No valid runs found!\")\n",
    "        return None\n",
    "    \n",
    "    return random.choice(run_numbers)\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    Ad_n = []\n",
    "    #Ad_n=copy.deepcopy(Ad)\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        \n",
    "        A_t,ill_flag=treshold(Ad[h],t=0.5)\n",
    "        \n",
    "        #print('ill_flag',ill_flag)\n",
    "        #print(Ad[h],A_t)\n",
    "        if ill_flag==True:\n",
    "            #Ad[h]=np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy())!!!!!!!!!!!\n",
    "            Ad[h]=Ad[h].detach().numpy()\n",
    "            A_t = postprocess_MinCostAss(np.array([Ad[h]]),a)[0]#!!!!!!!!\n",
    "        \n",
    "        \n",
    "        if isinstance(A_t, torch.Tensor):\n",
    "            A_t = A_t.detach().numpy()\n",
    "        #print(Ad[h],A_t)\n",
    "        Ad_n.append(A_t)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad_n\n",
    "\n",
    "def treshold(matrix, t):\n",
    "    z=np.where(matrix >= t, 1, 0)\n",
    "    \n",
    "    ill_flag=False\n",
    "\n",
    "      \n",
    "    if (~z.any(axis=0)).any() or any(np.sum(z[:,1:], axis=0)>1):\n",
    "        ill_flag=True\n",
    "          \n",
    "    return z,ill_flag\n",
    "\n",
    "def err_perc(a,b):\n",
    "    w=0\n",
    "    s=0\n",
    "    for i in range(len(a)):\n",
    "        m=a[i]-b[i].detach().numpy()\n",
    "        w=w+0.5*np.sum(np.abs(m))\n",
    "        s=s+np.size(m)\n",
    "    \n",
    "    \n",
    "    print('w,s',w,s)\n",
    "    \n",
    "    return w*100/s\n",
    "\n",
    "\n",
    "def modify_matrix(matrix):\n",
    "    # Check for rows with all zeros and set the first element to 1\n",
    "    row_sums = np.sum(matrix, axis=1)\n",
    "    matrix[row_sums == 0, 0] = 1\n",
    "\n",
    "    # Check for columns with all zeros and set the first element to 1\n",
    "    col_sums = np.sum(matrix, axis=0)\n",
    "    matrix[0, col_sums == 0] = 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = False, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05,\n",
    "                 use_transformer: bool = True):\n",
    "        super(CAT, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder_1_1 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_1_2 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_2_1 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_2_2 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        self.use_transformer=use_transformer\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        \n",
    "        half_dim = int(src_t1.size(-1) // 2)\n",
    "\n",
    "        pos_1 = src_t1[:, :, :half_dim]\n",
    "        vis_1 = src_t1[:, :, half_dim:]\n",
    "        \n",
    "        pos_2 = src_t2[:, :, :half_dim]\n",
    "        vis_2 = src_t2[:, :, half_dim:]\n",
    "        \n",
    "        if not self.use_transformer:  # <-- Add this condition\n",
    "            src_t1=pos_1\n",
    "            src_t2=pos_2\n",
    "            src_t1 = F.normalize(src_t1, p=2, dim=-1)\n",
    "            src_t2 = F.normalize(src_t2, p=2, dim=-1)\n",
    "            src_t1_t = torch.transpose(src_t1, 0, 1)\n",
    "            src_t2_t = torch.transpose(src_t2, 0, 1)\n",
    "            src_t2_t = torch.transpose(src_t2_t, 1, 2)\n",
    "            #embedding_dim = src_t1.shape[-1]  # Assuming the last dimension is the embedding dimension\n",
    "            #scaling_factor = embedding_dim**0.5\n",
    "            z=torch.bmm(src_t1_t, src_t2_t)\n",
    "            z = (z + 1) / 2\n",
    "            #z = A / torch.max(A)\n",
    "        \n",
    "            Ad = self.Ad.forward(z, src_padding_mask1, src_padding_mask2)\n",
    "            return Ad\n",
    "        \n",
    "        #src1_emb = src_t1\n",
    "        #src2_emb = src_t2\n",
    "        #print('pos_1,vis_1',pos_1[:,0,:],vis_1[:,0,:],pos_1.size(),vis_1.size())\n",
    "        #print('src2',src2_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size()) !!!!!\n",
    "        povi_1 = self.decoder_1_1(vis_1, vis_1,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask1)\n",
    "        povi_2 = self.decoder_1_2(vis_2, vis_2,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask2)\n",
    "        \n",
    "        out_1 = self.decoder_2_1(povi_1, povi_2,tgt_key_padding_mask=src_padding_mask1,memory_key_padding_mask=src_padding_mask2)\n",
    "        out_2 = self.decoder_2_2(povi_2, povi_1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        \n",
    "        #out_1_normalized = F.normalize(out_1, p=2, dim=-1)\n",
    "        #out_2_normalized = F.normalize(out_2, p=2, dim=-1)\n",
    "\n",
    "\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        \n",
    "        out_1=torch.transpose(out_1,0,1)\n",
    "        out_2=torch.transpose(out_2,0,1)\n",
    "        out_2=torch.transpose(out_2,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_1,out_2)) #here \n",
    "        #z = torch.bmm(out_1, out_2)\n",
    "        #z = (z + 1) / 2\n",
    "        #print('z',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m Loss(pen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,tra_to_tens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#!!!!!!!\u001b[39;00m\n\u001b[1;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(transformer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.98\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstop\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 64 ###!!!!24 for n2v emb\n",
    "nhead= 4    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 2\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "transformer = CAT(num_encoder_layers, emb_size, nhead, use_transformer=True)\n",
    "# CAT(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "#transformer.load_state_dict(torch.load('CAT1_vis.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=False) #!!!!!!!\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1500\n",
    "train_path='/home/mo/Desktop/IWR/Cell_GT_Proj/CTC_datasets/Fluo-C2DL-Huh7'\n",
    "test_path='/media/mo/Label/HUH_01_t'\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn,train_path)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn,test_path)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./losses_final/'+'train_loss_real_on_real.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./losses_final/'+'test_loss_real_on_real.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "    print('train_loss***',train_loss)\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "torch.save(transformer.state_dict(), 'CAT_vis_real.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "loss_over_time = np.loadtxt('./train_loss_new_emb_full.txt', skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error = np.loadtxt('./test_loss_new_emb_full.txt', skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "dist_error = np.loadtxt('./train_loss_distance_only.txt', skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "# Compute the running mean for the first two datasets\n",
    "N = 500\n",
    "loss_over_time_mean = np.convolve(loss_over_time, np.ones(N)/N, mode='valid')\n",
    "test_error_mean = np.convolve(test_error, np.ones(N)/N, mode='valid')\n",
    "\n",
    "# Compute the mean and standard deviation for the dist_error\n",
    "dist_error_mean = np.mean(dist_error)\n",
    "dist_error_std = np.std(dist_error)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6)) # Set the figure size\n",
    "\n",
    "\n",
    "# Plot the dist_error as a straight line with mean value\n",
    "plt.axhline(dist_error_mean, color='green', linestyle='-', label='Distance')\n",
    "# Shade the region between [mean - std, mean + std] to show uncertainty\n",
    "plt.fill_between(range(len(test_error_mean)), dist_error_mean - dist_error_std, dist_error_mean + dist_error_std, color='green', alpha=0.2)\n",
    "\n",
    "\n",
    "plt.plot(test_error_mean, label='Vis')\n",
    "plt.plot(loss_over_time_mean, c='red', label='Pos + Vis')\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Test Loss over Epochs')\n",
    "plt.xlabel('Epochs')  # Add x-axis label\n",
    "plt.ylabel('Loss')    # Add y-axis label\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)  # Add a grid for better readability\n",
    "plt.tight_layout()   # Ensure all elements fit well\n",
    "plt.savefig('test_loss_over_epochs.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack2_new.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recon########################\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tracks(adj_matrices):\n",
    "    tracks = []\n",
    "    label_counter = 1\n",
    "\n",
    "    for t, adj_matrix in enumerate(adj_matrices):\n",
    "        if t==7:\n",
    "            print('t=7ADA',adj_matrix,len(adj_matrix))\n",
    "        if t==8:\n",
    "            print('t=8ADA',adj_matrix,len(adj_matrix))\n",
    "        for i, row in enumerate(adj_matrix[1:], 1):  # Skip the background node's row\n",
    "            linked_cells = [j for j, x in enumerate(row[1:], 1) if x == 1]  # Skip the background node's column\n",
    "            # Detect new cell appearance\n",
    "            if adj_matrix[0][i] == 1 and i != 0:\n",
    "                tracks.append({'L': label_counter, 'B': t, 'E': t + 1, 'P': 0})\n",
    "                label_counter += 1\n",
    "                continue\n",
    "\n",
    "            # Detect cells that disappeared\n",
    "            if adj_matrix[i][0] == 1 and i != 0:\n",
    "                existing_track = find_track_by_cell_label_and_end_time(tracks, i, t)\n",
    "                if existing_track:\n",
    "                    existing_track['E'] = t  # This cell's track ends at current time\n",
    "                continue\n",
    "\n",
    "            # For continuation of cells\n",
    "            if len(linked_cells) == 1:\n",
    "                existing_track = find_track_by_cell_label_and_end_time(tracks, i, t)\n",
    "                if existing_track:\n",
    "                    existing_track['E'] = t + 1\n",
    "                else:\n",
    "                    tracks.append({'L': label_counter, 'B': t, 'E': t + 1, 'P': 0})\n",
    "                    label_counter += 1\n",
    "\n",
    "            # For cell division\n",
    "            elif len(linked_cells) > 1:\n",
    "                parent_track = find_track_by_cell_label_and_end_time(tracks, i, t)\n",
    "                if parent_track:\n",
    "                    parent_track['E'] = t  # The parent cell's track ends at current time\n",
    "                \n",
    "                \n",
    "                                # Assuming there's only one new row added after the split\n",
    "                daughter1 = linked_cells[0]\n",
    "                daughter2 = i + 1 if i + 1 != daughter1 else i + 2\n",
    "\n",
    "                for cell in [daughter1, daughter2]:\n",
    "                    tracks.append({'L': label_counter, 'B': t + 1, 'E': t + 1, 'P': parent_track['L'] if parent_track else 0})\n",
    "                    label_counter += 1\n",
    "                #for cell in linked_cells:\n",
    "                 #   tracks.append({'L': label_counter, 'B': t + 1, 'E': t + 1, 'P': parent_track['L'] if parent_track else 0})\n",
    "                 #   label_counter += 1\n",
    "\n",
    "    return tracks\n",
    "\n",
    "def find_track_by_cell_label_and_end_time(tracks, cell_label, end_time):\n",
    "    \n",
    "    if cell_label==13:\n",
    "        print('13',cell_label,end_time)\n",
    "        print(tracks)\n",
    "    elif cell_label==12:\n",
    "        print('12',cell_label,end_time)\n",
    "        print(tracks)\n",
    "    elif cell_label==11:\n",
    "        print('11',cell_label,end_time)\n",
    "        print(tracks)\n",
    "    if end_time==7:\n",
    "        print('t=7',tracks)\n",
    "    if end_time==8:\n",
    "        print('t=8',tracks)\n",
    "    for track in tracks:\n",
    "        if track['L'] == cell_label and track['E'] == end_time:\n",
    "            return track\n",
    "    return None\n",
    "def write_tracks_to_file(tracks, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for track in tracks:\n",
    "            f.write(f\"{track['L']} {track['B']} {track['E']} {track['P']}\\n\")\n",
    "\n",
    "def adjacency_to_edgelist(matrices):\n",
    "    edges = []\n",
    "    \n",
    "    # Starting node based on the size of the first matrix\n",
    "    current_start_node = matrices[0].shape[1]-1\n",
    "    \n",
    "    # Target nodes from the previous matrix\n",
    "    previous_targets = list(range(1,matrices[0].shape[0]))\n",
    "    #print(current_start_node,previous_targets)\n",
    "    for matrix in matrices:\n",
    "        # Number of nodes in the current timeframe\n",
    "        num_nodes_t0 = matrix.shape[1]\n",
    "        print('M',matrix)\n",
    "        new_targets = []\n",
    "        \n",
    "        for row in range(matrix.shape[0]):\n",
    "            for col in range(num_nodes_t0):\n",
    "                if matrix[row, col] == 1 and not (row == 0 or col == 0):\n",
    "                    # Use previous_targets for source nodes\n",
    "                   \n",
    "                    source_node = sorted(previous_targets)[row-1]\n",
    "                    target_node = col + current_start_node\n",
    "                    edges.append((source_node, target_node))\n",
    "                    print('row',row,col,source_node,target_node,edges)\n",
    "                    new_targets.append(target_node)\n",
    "                    \n",
    "        # Update the previous targets\n",
    "        previous_targets = new_targets\n",
    "                    \n",
    "        # Update the start node for the next matrix\n",
    "        current_start_node += matrix.shape[1] - 1  # subtracting 1 for the background node\n",
    "\n",
    "    return edges\n",
    "\n",
    "def find_track(node, graph):\n",
    "    \"\"\"Returns the entire track of a node.\"\"\"\n",
    "    track = [node]\n",
    "    while node in graph:\n",
    "        node = graph[node]\n",
    "        track.append(node)\n",
    "    return track\n",
    "\n",
    "def get_track(node, graph):\n",
    "    \"\"\"Returns the track of a node.\"\"\"\n",
    "    track = [node]\n",
    "    while node in graph: \n",
    "        if len(graph[node]) == 1:\n",
    "            next_node = graph[node][0]\n",
    "            del graph[node]  # Remove the utilized edge\n",
    "            node = next_node\n",
    "            track.append(node)\n",
    "        else:\n",
    "            print('gnode',graph[node][0])\n",
    "            del graph[node] \n",
    "            break\n",
    "    return track\n",
    "\n",
    "\n",
    "def read_timetable(filename):\n",
    "    timetable = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        next(file)  # Skip the first line\n",
    "        for line in file:\n",
    "            # Split each line by the tab character to extract node and time\n",
    "            node, time = line.strip().split('\\t')\n",
    "            timetable[int(node) - 1] = int(time)  # Shift the node index up by 1\n",
    "    return timetable\n",
    "def reverse_graph(graph):\n",
    "    reversed_graph = {}\n",
    "    for key, values in graph.items():\n",
    "        for value in values:\n",
    "            reversed_graph[value] = reversed_graph.get(value, [])\n",
    "            reversed_graph[value].append(key)\n",
    "    return reversed_graph\n",
    "\n",
    "# Function to find root node\n",
    "def find_root(reversed_graph, node):\n",
    "    while node in reversed_graph:\n",
    "        node = reversed_graph[node][0]  # Follow the first parent up the tree\n",
    "    return node\n",
    "\n",
    "# Function to find the initial parent cell\n",
    "def find_initial_parent_cell(node, tracks,graph):\n",
    "    reversed_graph = reverse_graph(graph)\n",
    "    root = find_root(reversed_graph, node)\n",
    "    \n",
    "    # Determine which track the root belongs to\n",
    "    for i, track in enumerate(tracks):\n",
    "        if root in track:\n",
    "            return i + 1  # Return track index (1-based)\n",
    "    return None  # If not found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#recon+testerror\n",
    "transformer.load_state_dict(torch.load('CAT1_vis_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "test_path='/media/mo/Label/HeLa_track_test'\n",
    "for r in range(1):\n",
    "    r=71\n",
    "    src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=r,path0=test_path)\n",
    "\n",
    "    #print(src1.size())\n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    #print('y',y[25])\n",
    "    #print('Ad',len(Ad))\n",
    "    \n",
    "    \n",
    "    #val_loss = evaluate(transformer,loss_fn)\n",
    "    #print('L',val_loss)\n",
    "    a=0.1\n",
    "    #print(Ad)\n",
    "    pp_A = complete_postprocess(Ad,d,a)\n",
    "    \n",
    "    #err_p=err_perc(pp_A,y)\n",
    "    #print('err',r,err_p)\n",
    "\n",
    "#print(src1.size())\n",
    "\n",
    "    print('y',y[7])\n",
    "    print('Ad',Ad[7])\n",
    "    print('pp',pp_A[7])\n",
    "    print('d',d[7])\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(pp_A[i])\n",
    "\n",
    "    filename = \"/media/mo/Label/HeLa_track_test/71/timetable.txt\"\n",
    "    timetable = read_timetable(filename)\n",
    "    \n",
    "    edgelist=adjacency_to_edgelist(pp_A)\n",
    "    print(edgelist)\n",
    "    \n",
    "    start_nodes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "    # Convert edgelist into a dictionary for easier lookup\n",
    "    graph = {}\n",
    "    for start, end in edgelist:\n",
    "        graph.setdefault(start, []).append(end)\n",
    "    all_tracks = []\n",
    "    graph_orig = deepcopy(graph)\n",
    "    print('graph**',graph)\n",
    "    # Discover tracks from initial nodes\n",
    "    for node in start_nodes:\n",
    "        all_tracks.append(get_track(node, graph))\n",
    "\n",
    "    # Discover tracks for daughter cells until the graph is exhausted\n",
    "    remaining_nodes = list(graph.keys())\n",
    "    print('rn0',remaining_nodes)\n",
    "    print('allTra0',all_tracks)\n",
    "    while remaining_nodes:\n",
    "        node = remaining_nodes[0]\n",
    "        print('node',node,graph)\n",
    "        all_tracks.append(get_track(node, graph))\n",
    "        remaining_nodes = list(graph.keys())\n",
    "        print('allTra1',all_tracks)\n",
    "        print('rn1',remaining_nodes)\n",
    "        #print(stop)\n",
    "\n",
    "    # Determine the parent for each node (0 if no parent)\n",
    "    parent_dict = {node: 0 for node in start_nodes}\n",
    "    for track in all_tracks:\n",
    "        print('track',track)\n",
    "        if track[0] not in start_nodes:\n",
    "            parent_dict[track[0]] = find_initial_parent_cell(track[0],all_tracks,graph_orig)\n",
    "    cell_id_counter = max(start_nodes)\n",
    "    # Writing the data to man_track.txt\n",
    "    with open('man_track.txt', 'w') as file:\n",
    "        for track in all_tracks:\n",
    "            \n",
    "            B = timetable[track[0]]\n",
    "            E = timetable[track[-1]]\n",
    "            P = parent_dict[track[0]]\n",
    "            \n",
    "            if track[0] in start_nodes:\n",
    "                L = track[0]\n",
    "            else:\n",
    "                cell_id_counter += 1\n",
    "                L = cell_id_counter\n",
    "\n",
    "            if E != B:\n",
    "                file.write(f\"{L} {B} {E} {P}\\n\")\n",
    "\n",
    "                # Handle splits\n",
    "                if track[-1] in graph:\n",
    "                    for child in graph[track[-1]]:\n",
    "                        file.write(f\"{child} {E + 1} {timetable.get(child, E)} {L}\\n\")\n",
    "\n",
    "    print(\"File 'man_track.txt' written successfully!\")\n",
    "\n",
    "    #make_reconstructed_edgelist(Ad,run=r)\n",
    "    #tracks = generate_tracks(pp_A)\n",
    "    \n",
    "    #for track in tracks:\n",
    "    #    print(f\"{track['L']} {track['B']} {track['E']} {track['P']}\")\n",
    "    #write_tracks_to_file(tracks, 'man_track.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tracks(adj_matrices):\n",
    "    tracks = []\n",
    "    label_counter = 1\n",
    "\n",
    "    for t, adj_matrix in enumerate(adj_matrices):\n",
    "        for i, row in enumerate(adj_matrix[1:], 1):  # Skip the background node's row\n",
    "            linked_cells = [j for j, x in enumerate(row[1:], 1) if x == 1]  # Skip the background node's column\n",
    "            \n",
    "            # Detect new cell appearance\n",
    "            if adj_matrix[0][i] == 1 and i != 0:\n",
    "                tracks.append({'L': label_counter, 'B': t, 'E': t + 1, 'P': 0})\n",
    "                label_counter += 1\n",
    "                continue\n",
    "\n",
    "            # Detect cells that disappeared\n",
    "            if adj_matrix[i][0] == 1 and i != 0:\n",
    "                existing_track = find_track_by_cell_label_and_end_time(tracks, i, t)\n",
    "                if existing_track:\n",
    "                    existing_track['E'] = t  # This cell's track ends at current time\n",
    "                continue\n",
    "\n",
    "            # For continuation of cells\n",
    "            if len(linked_cells) == 1:\n",
    "                existing_track = find_track_by_cell_label_and_end_time(tracks, i, t)\n",
    "                if existing_track:\n",
    "                    existing_track['E'] = t + 1\n",
    "\n",
    "            # For cell division\n",
    "            elif len(linked_cells) > 1:\n",
    "                parent_track = find_track_by_cell_label_and_end_time(tracks, i, t)\n",
    "                if parent_track:\n",
    "                    parent_track['E'] = t  # The parent cell's track ends at current time\n",
    "\n",
    "                # Assuming there's only one new row added after the split\n",
    "                daughter1 = linked_cells[0]\n",
    "                daughter2 = i + 1 if i + 1 != daughter1 else i + 2\n",
    "\n",
    "                for cell in [daughter1, daughter2]:\n",
    "                    tracks.append({'L': label_counter, 'B': t + 1, 'E': t + 1, 'P': parent_track['L'] if parent_track else 0})\n",
    "                    label_counter += 1\n",
    "\n",
    "    return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=95\n",
    "t= 8\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:]#[1:]\n",
    "src_t2=src_t2[:,t,:]#[1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:]#[1:]\n",
    "b=b[:,t,:]#[1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine',n_neighbors=4)\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
