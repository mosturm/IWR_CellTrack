{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import copy\n",
    "from typing import Optional, List\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ortools.graph.python import min_cost_flow\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import copy\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        #print('PE',self.pos_embedding[:token_embedding.size(0), :])\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "    \n",
    "def collate_fn(batch_len,PAD_IDX,train=True,recon=False,run=12,path0='.'):\n",
    "    #print('batch',len(batch),batch)\n",
    "    src1_batch, src2_batch, y_batch,d_batch = [], [], [], []\n",
    "    for j in range(batch_len):\n",
    "        \n",
    "        if train:\n",
    "            E1,E2,A,D=loadgraph(path=path0)\n",
    "        elif recon:\n",
    "            E1,E2,A,D=loadgraph(recon=True, train=False,run=run,t_r=j,path=path0)\n",
    "            #print('recon')\n",
    "        else:\n",
    "            E1,E2,A,D=loadgraph(train=False,path=path0)\n",
    "        #print('src_sample',src_sample)\n",
    "        src1_batch.append(E1)\n",
    "        #print('emb',src_batch[-1])\n",
    "        src2_batch.append(E2)\n",
    "        y_batch.append(A)\n",
    "        d_batch.append(D)\n",
    "        \n",
    "        \n",
    "    #print('src_batch',src1_batch[3])\n",
    "    #print('src2_batch',src2_batch[3])\n",
    "    #print('src_batch s',len(src_batch))\n",
    "    src1_batch = pad_sequence(src1_batch, padding_value=PAD_IDX)\n",
    "    #print('src_batch',src_batch)\n",
    "    #print('src_batch s',src_batch.size())\n",
    "    src2_batch = pad_sequence(src2_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    \n",
    "    #print('src1',src1_batch[:,0,:],src1_batch[:,0,:].size())\n",
    "    #print('src2',src2_batch[:,0,:],src2_batch[:,0,:].size())\n",
    "    #print('y',y_batch)\n",
    "    ##\n",
    "    return src1_batch, src2_batch,y_batch,d_batch\n",
    "\n",
    "\n",
    "def loadgraph(train=True,run=None,easy=False,recon=False,t_r=None,path='.'):\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    if train:\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,3) #!!!!!!!!!!##100 total data size\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        #print('E',E.shape)\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        #print(bg_a)\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        #print(D)\n",
    "        #print(np.dot(E1,E2.T))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #print('eval')\n",
    "        if run==None:\n",
    "            run=np.random.randint(1,3) #!!!!!!!!\n",
    "        else: run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        bg=A[0]\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        t = np.random.randint(30) #!!!!!!!!how many t??\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "       \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "        \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "        \n",
    "        \n",
    "    if recon: \n",
    "        run=run\n",
    "        E=np.loadtxt('./'+str(run)+'/'+'embed.txt')\n",
    "        id,tt = np.loadtxt('./'+str(run)+'/'+'timetable.txt', delimiter='\\t', usecols=(0,1), unpack=True)\n",
    "        A=np.loadtxt('./'+str(run)+'_GT'+'/'+'A.txt')\n",
    "        D=np.loadtxt('./'+str(run)+'/'+'D.txt')\n",
    "        for i in range(len(A)):\n",
    "            for j in range(len(A)):\n",
    "                if i>j:\n",
    "                    A[i,j]=0\n",
    "        #A=A+np.eye(len(A), dtype=int)\n",
    "        \n",
    "        \n",
    "        #print(id)\n",
    "        t = t_r\n",
    "        id1 = id[tt==t].astype(int)\n",
    "        if t==0:\n",
    "            id1=id1[1:]\n",
    "        id2 = id[tt==(t+1)].astype(int)\n",
    "        \n",
    "        #print(run,t,id1,id2)\n",
    "        \n",
    "        E1 = E[id1-1]\n",
    "        E2 = E[id2-1]\n",
    "       \n",
    "        E_bg = E[0]\n",
    "        \n",
    "        E1=np.concatenate((np.array([E_bg]), E1), axis=0)\n",
    "        E2=np.concatenate((np.array([E_bg]), E2), axis=0)\n",
    "        \n",
    "     \n",
    "        \n",
    "        A=A[id1-1]\n",
    "        \n",
    "        A=A[:,id2-1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        D=D[id1-1]\n",
    "        D=D[:,id2-1]\n",
    "        \n",
    "        \n",
    "        #print(A)\n",
    "        \n",
    "        \n",
    "        #print(bg[id1-1])\n",
    "        #print(bg[id2-1])\n",
    "        \n",
    "        \n",
    "        A=np.concatenate((np.array([bg[id2-1]]), A), axis=0)\n",
    "        \n",
    "        bg_a=np.append(1,bg[id1-1])\n",
    "        A=np.concatenate((np.array([bg_a]).T, A), axis=1)\n",
    "       \n",
    "        #print(E1,E2)\n",
    "        \n",
    "        bg_b = np.append(0,np.zeros(len(bg[id1-1])))\n",
    "    \n",
    "    \n",
    "    \n",
    "        D=np.concatenate((np.array([np.zeros(len(bg[id2-1]))]), D), axis=0)\n",
    "        D=np.concatenate((np.array([bg_b]).T, D), axis=1)\n",
    "    \n",
    "    \n",
    "    if easy:\n",
    "        n1=np.random.randint(3,6)\n",
    "        n2=n1+np.random.randint(2)\n",
    "        E1=np.ones((n1,6))\n",
    "        E2=np.ones((n2,6))*3\n",
    "        A=np.ones((n1,n2))\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    D=D.astype(np.float32)\n",
    "    \n",
    "    vd = np.vectorize(d_mask_function,otypes=[float])\n",
    "    \n",
    "    D = vd(D,0.15,-2.0)\n",
    "    \n",
    "    \n",
    "    E1=E1.astype(np.float32)\n",
    "    E2=E2.astype(np.float32)\n",
    "    A=A.astype(np.float32)\n",
    "    #A=A.astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    E1=convert_tensor(E1) \n",
    "    E2=convert_tensor(E2) \n",
    "    A=convert_tensor(A)\n",
    "    D=convert_tensor(D)\n",
    "    \n",
    "    #print(E1[0].size(),E1[0])\n",
    "    #print(E2[0].size(),E2[0])\n",
    "    #print(A,A.size())\n",
    "    #print('E',E.size())\n",
    "    \n",
    "    return E1[0],E2[0],A[0],D[0]\n",
    "\n",
    "def create_mask(src,PAD_IDX):\n",
    "    \n",
    "    src= src[:,:,0]\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    #print('src_padding_mask',src_padding_mask,src_padding_mask.size())\n",
    "    return src_padding_mask\n",
    "\n",
    "\n",
    "def train_easy(model, optimizer, loss_function, epochs,scheduler,verbose=True,eval=True):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_over_time = []\n",
    "    test_error = []\n",
    "    perf=[]\n",
    "    t0 = time.time()\n",
    "    i=0\n",
    "    while i < epochs:\n",
    "        print(i)\n",
    "        \n",
    "        #u = np.random.random_integers(4998) #4998 for 3_GT\n",
    "        src1, src2, y = collate_fn(10,-100)\n",
    "        \n",
    "        #print('src_batch',src1)\n",
    "        #print('src_batch s',src1.size())\n",
    "        \n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        '''#trysimplesttrans'''\n",
    "        \n",
    "        #output=model(tgt,tgt)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output1,output2 = model(src1,src2,src_padding_mask1,src_padding_mask2)  \n",
    "        #output = model(src)   #!!!!!!!\n",
    "        #imshow(src1)\n",
    "        #imshow(tgt1)\n",
    "        \n",
    "        #print('out1',output1,output1.size())\n",
    "        #print('out2',output2,output2.size())\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        #print('train_sizes',src.size(),output[:,:n_nodes,:n_nodes].size(),y.size())\n",
    "        \n",
    "        \n",
    "        epoch_loss = loss_function(output1, src1)\n",
    "        epoch_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 5 == 0 and i>0:\n",
    "            t1 = time.time()\n",
    "            epochs_per_sec = 10/(t1 - t0) \n",
    "            if verbose:\n",
    "                print(f\"Epoch: {i} loss {epoch_loss.item()} @ {epochs_per_sec} epochs per second\")\n",
    "            loss_over_time.append(epoch_loss.item())\n",
    "            t0 = t1\n",
    "            np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "            perf.append(epochs_per_sec)\n",
    "        try:\n",
    "            print(c)\n",
    "            d=len(loss_over_time)\n",
    "            if np.sqrt((np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))**2) < np.std(loss_over_time[d-10:-1])/50:\n",
    "                print('loss not reducing')\n",
    "                print(np.mean(loss_over_time[d-10:-1])-np.mean(loss_over_time[d-20:d-10]))\n",
    "                print(np.std(loss_over_time[d-10:-1])/10)\n",
    "                print(d)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        i=i+1\n",
    "        \n",
    "        '''\n",
    "        if i % 5 == 0 and i>0:\n",
    "        \n",
    "    \n",
    "        \n",
    "            if eval:\n",
    "                u = np.random.random_integers(490)\n",
    "                src_t, tgt_t, y_t = loadgraph(easy=True)\n",
    "                \n",
    "                n_nodes=0\n",
    "                for h in range(len(src_t[0])):\n",
    "                    if torch.sum(src_t[0][h])!=0:\n",
    "                        n_nodes=n_nodes+1\n",
    "                \n",
    "                max_len=len(src_t[0])\n",
    "                \n",
    "                output_t = model(src_t,tgt_t,n_nodes)\n",
    "\n",
    "                test_loss = loss_function(output_t[:,:n_nodes,:n_nodes], y_t)\n",
    "\n",
    "                test_error.append(test_loss.item())\n",
    "                \n",
    "                np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "            \n",
    "        \n",
    "        i=i+1\n",
    "            \n",
    "    print('Mean Performance', np.mean(perf))\n",
    "    return model, loss_over_time, test_error\n",
    "    '''\n",
    "        \n",
    "        \n",
    "class makeAdja:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,z:Tensor,\n",
    "                mask1: Tensor,\n",
    "                mask2: Tensor):\n",
    "        Ad = []\n",
    "        for i in range(z.size(0)):\n",
    "            n=len([i for i, e in enumerate(mask1[i]) if e != True])\n",
    "            m=len([i for i, e in enumerate(mask2[i]) if e != True])\n",
    "            Ad.append(z[i,0:n,0:m])\n",
    "        \n",
    "        \n",
    "        return Ad\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    #print('src1',src1.size(),src1)\n",
    "    \n",
    "    #print('src1_mask',src_padding_mask1.size(),src_padding_mask1)\n",
    "    #print('src1_0',src1[:,0,:].size(),src1[:,0,:])\n",
    "    #print('src1_0_mask',src_padding_mask1.size(),src_padding_mask1[:,0,:])\n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    #print(Ad[0],y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def train_epoch_post_process(model, optimizer,loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    src1, src2, y,d = collate_fn(31,-100)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    Ad = complete_postprocess(Ad,d,0.01)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    print(Ad[0])\n",
    "    print(y[0])\n",
    "    #print('l',loss)\n",
    "    #print('l',loss.item() / len(src1))\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    losses += loss.item()\n",
    "    \n",
    "    \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "\n",
    "class Loss():\n",
    "    def __init__(self,pen,tra_to_tens=False):\n",
    "        self.pen=pen\n",
    "        self.trans=tra_to_tens\n",
    "        \n",
    "    def loss (self,Ad,y):\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(Ad)):\n",
    "            l = nn.CrossEntropyLoss()\n",
    "            if self.trans:\n",
    "                Ad[i]=convert_tensor(Ad[i])[0]\n",
    "            #print(Ad[i], y[i])\n",
    "            \n",
    "            s = l(Ad[i], y[i])\n",
    "            \n",
    "            loss=loss+s\n",
    "                \n",
    "        if self.trans:\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model,loss_fn):\n",
    "    #model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    src1, src2, y,d = collate_fn(31,-100,train=False)\n",
    "        \n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    try:\n",
    "        Ad,out1,out2,out_dec1,src1_t1,src2_t2 = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    except:    \n",
    "        Ad = model(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "    \n",
    "   \n",
    "    loss = loss_fn.loss(Ad,y)\n",
    "    \n",
    "    losses += loss.item()\n",
    "    \n",
    "        \n",
    "\n",
    "    return losses / len(src1)\n",
    "\n",
    "\n",
    "def postprocess(A):\n",
    "    pp_A=[]\n",
    "    for i in range(len(A)):\n",
    "        ind=torch.argmax(A[i], dim=0)\n",
    "        B=np.zeros(A[i].shape)\n",
    "        for j in range(len(ind)):\n",
    "            B[ind[j],j]=1\n",
    "        pp_A.append(B)\n",
    "    return pp_A\n",
    "\n",
    "def square(m):\n",
    "    return m.shape[0] == m.shape[1]\n",
    "\n",
    "\n",
    "def postprocess_2(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2)  \n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_3(Ad):\n",
    "    pp_A=[]\n",
    "    \n",
    "    row_ind, col_ind = linear_sum_assignment(1-Ad[0])\n",
    "    \n",
    "    print(1-Ad[0])\n",
    "    print(row_ind, col_ind)\n",
    "    \n",
    "    z=np.zeros(Ad[0].shape)\n",
    "\n",
    "\n",
    "    for i,j in zip(row_ind, col_ind):\n",
    "        z[i,j]=1\n",
    "    \n",
    "    \n",
    "    print(z)\n",
    "    '''\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h])\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "            \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            zero_col=np.where(~z.any(axis=0))[0]\n",
    "            c_A=Ad[h].detach().numpy()\n",
    "            z[:,zero_col] = c_A[:,zero_col]\n",
    "            #print(z)\n",
    "            pp_A.append(z)\n",
    "        \n",
    "            \n",
    "       # else:\n",
    "        #    z2 = np.zeros(Ad[h].shape)\n",
    "        #    zero_col=np.where(~z.any(axis=0))[0]\n",
    "            \n",
    "         #   for k,l in zip(ind,zero_col):\n",
    "         #       z2[k,l]=1\n",
    "         #   pp_A.append(z+z2) \n",
    "    '''\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_linAss(Ad):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        row_ind, col_ind = linear_sum_assignment(1-Ad[h].detach().numpy())\n",
    "\n",
    "        z=np.zeros(Ad[h].shape)\n",
    "\n",
    "\n",
    "        for i,j in zip(row_ind, col_ind):\n",
    "            z[i,j]=1\n",
    "    \n",
    "        \n",
    "        if square(z):\n",
    "            pp_A.append(z)\n",
    "        else:\n",
    "            f=Ad[h].detach().numpy()\n",
    "            l=np.ones(len(f))*2\n",
    "            l=l.astype(int)\n",
    "            \n",
    "            \n",
    "            f2=np.repeat(f, l, axis=0)\n",
    "            row_ind, col_ind = linear_sum_assignment(1-f)\n",
    "            z=np.zeros(f.shape)\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "\n",
    "            f2[0::2, :] = z[:] \n",
    "\n",
    "            row_ind_f, col_ind_f = linear_sum_assignment(1-f2)\n",
    "\n",
    "\n",
    "            z3=np.zeros(f2.shape)\n",
    "\n",
    "\n",
    "            for i,j in zip(row_ind_f, col_ind_f):\n",
    "                z3[i,j]=1\n",
    "\n",
    "            f_add = z3[0::2, :] + z3[1::2, :]\n",
    "            \n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess_MinCostAss(Ad,a):\n",
    "    pp_A=[]\n",
    "    for h in range(len(Ad)):\n",
    "        smcf = min_cost_flow.SimpleMinCostFlow()\n",
    "        c_A = Ad[h]\n",
    "        \n",
    "        #left_n=c_A.size(0)\n",
    "        #right_n=c_A.size(1)\n",
    "        \n",
    "        left_n=c_A.shape[0]\n",
    "        right_n=c_A.shape[1]\n",
    "        \n",
    "        \n",
    "        st=np.zeros(left_n)\n",
    "        con= np.ones(right_n) \n",
    "        for v in range(left_n-1):\n",
    "            con= np.append(con, np.ones(right_n)*(v+2))\n",
    "        #print('con',con) \n",
    "        si = np.arange(left_n+1,left_n+right_n+1)\n",
    "        start_nodes = np.concatenate((st,np.array(con),si))\n",
    "        start_nodes = np.append(start_nodes,0)\n",
    "        start_nodes = [int(x) for x in start_nodes ]\n",
    "        #print(start_nodes)\n",
    "        \n",
    "        st_e = np.arange(1,left_n+1)\n",
    "        con_e = si\n",
    "        for j in range(left_n-1):\n",
    "            con_e = np.append(con_e,si)\n",
    "            \n",
    "        si_e = np.ones(right_n)*left_n+right_n+1\n",
    "        \n",
    "        end_nodes = np.concatenate((st_e,np.array(con_e),si_e))\n",
    "        end_nodes = np.append(end_nodes,si_e[-1])\n",
    "        end_nodes = [int(x) for x in end_nodes ]\n",
    "        #print(end_nodes)\n",
    "        \n",
    "        \n",
    "        tasks = np.max([right_n,left_n])\n",
    "        \n",
    "        cap_0 = np.ones(left_n)\n",
    "        cap_0[0]=right_n-1\n",
    "        \n",
    "        cap_left=np.ones(right_n)\n",
    "        cap_left[0]=right_n\n",
    "        \n",
    "        capacities = np.concatenate((cap_0,np.ones(len(con_e)),cap_left))\n",
    "        capacities = np.append(capacities,tasks)\n",
    "        capacities = [int(x) for x in capacities]\n",
    "        #print(capacities)\n",
    "        \n",
    "        '''\n",
    "        c_A[0]=c_A[0]/c_A[0,0]\n",
    "        c_A[0]=c_A[0]/(1.01*np.max(c_A[0]))\n",
    "        c_A[:,0]=c_A[:,0]/c_A[0,0]\n",
    "        c_A[:,0]=c_A[:,0]/(1.01*np.max(c_A[:,0]))\n",
    "        '''\n",
    "        \n",
    "        #print(c_A)\n",
    "        c= c_A.flatten()                          \n",
    "        #c=torch.flatten(c_A)\n",
    "        #c=c.detach().numpy()  \n",
    "                                    \n",
    "                                    \n",
    "        c=(1-c)*10**4\n",
    "        \n",
    "        #print(c)\n",
    "                                    \n",
    "        costs = np.concatenate((np.zeros(left_n),c,np.zeros(right_n)))\n",
    "        costs = np.append(costs,a*np.mean(c))                            \n",
    "        costs = [int(x) for x in costs]\n",
    "                                    \n",
    "        #print(costs)\n",
    "        \n",
    "        source = 0\n",
    "        sink = left_n+right_n+1\n",
    "        \n",
    "        supplies= tasks \n",
    "        \n",
    "        supplies=np.append(supplies,np.ones(left_n))\n",
    "        supplies=np.append(supplies,np.zeros(right_n))\n",
    "        \n",
    "        #supplies=np.append(supplies,np.zeros(left_n+right_n))\n",
    "        \n",
    "        supplies=np.append(supplies,-(tasks+left_n))\n",
    "        \n",
    "        supplies = [int(x) for x in supplies]\n",
    "        #print(supplies)\n",
    "        #print('____________________________________')\n",
    "        # Add each arc.\n",
    "        for i in range(len(start_nodes)):\n",
    "            #print(start_nodes[i], end_nodes[i],capacities[i], costs[i])\n",
    "            smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "        # Add node supplies.\n",
    "        for i in range(len(supplies)):\n",
    "            smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "        # Find the minimum cost flow between node 0 and node 10.\n",
    "        status = smcf.solve()\n",
    "\n",
    "        if status == smcf.OPTIMAL:\n",
    "            #print('Total cost = ', smcf.optimal_cost())\n",
    "            #print()\n",
    "            row_ind=[]\n",
    "            col_ind=[]\n",
    "            for arc in range(smcf.num_arcs()):\n",
    "                # Can ignore arcs leading out of source or into sink.\n",
    "                if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                    # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                    # give an assignment of worker to task.\n",
    "                    if smcf.flow(arc) > 0:\n",
    "                        #p#rint('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                        #      (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                        row_ind.append(smcf.tail(arc)-1)\n",
    "                        col_ind.append(smcf.head(arc)-left_n-1)\n",
    "            z=np.zeros((left_n,right_n))\n",
    "            \n",
    "            for i,j in zip(row_ind, col_ind):\n",
    "                z[i,j]=1\n",
    "             \n",
    "            \n",
    "            #print('z_orig',z)\n",
    "            s=np.sum(z,axis=1)\n",
    "            for e in range(len(s)):\n",
    "                if s[e]>1 and e!=0:\n",
    "                    z[e,0]=0\n",
    "            #print('z_bg_cor',z)      \n",
    "            if (~z.any(axis=0)).any():\n",
    "                z_col_ind=np.where(~z.any(axis=0))[0]\n",
    "                z[:,z_col_ind]=c_A[:,z_col_ind]\n",
    "                #print('---------z_0_col',z)\n",
    "                z=postprocess_MinCostAss(np.array([z]),2*a)[0]\n",
    "                #print('z_0_col_after',z)\n",
    "\n",
    "                    \n",
    "            pp_A.append(z)\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "        else:\n",
    "            print('There was an issue with the min cost flow input.')\n",
    "            print(f'Status: {status}')\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "    return pp_A\n",
    "\n",
    "      \n",
    "'''\n",
    "\n",
    "    start_nodes = np.zeros(c_A.size(0)) + [\n",
    "        1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3\n",
    "    ] + [4, 5, 6, 7]\n",
    "    end_nodes = [1, 2, 3] + [4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7] + [8,8,8,8]\n",
    "    capacities = [2, 2, 2] + [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n",
    "    ] + [2, 2, 2, 2]\n",
    "    costs = (\n",
    "        [0, 0, 0] +\n",
    "        c +\n",
    "        [0, 0, 0 ,0])\n",
    "\n",
    "    source = 0\n",
    "    sink = 8\n",
    "    tasks = 4\n",
    "    supplies = [tasks, 0, 0, 0, 0, 0, 0, 0, -tasks]\n",
    "\n",
    "    # Add each arc.\n",
    "    for i in range(len(start_nodes)):\n",
    "        smcf.add_arc_with_capacity_and_unit_cost(start_nodes[i], end_nodes[i],\n",
    "                                                 capacities[i], costs[i])\n",
    "    # Add node supplies.\n",
    "    for i in range(len(supplies)):\n",
    "        smcf.set_node_supply(i, supplies[i])\n",
    "\n",
    "    # Find the minimum cost flow between node 0 and node 10.\n",
    "    status = smcf.solve()\n",
    "\n",
    "    if status == smcf.OPTIMAL:\n",
    "        print('Total cost = ', smcf.optimal_cost())\n",
    "        print()\n",
    "        for arc in range(smcf.num_arcs()):\n",
    "            # Can ignore arcs leading out of source or into sink.\n",
    "            if smcf.tail(arc) != source and smcf.head(arc) != sink:\n",
    "\n",
    "                # Arcs in the solution have a flow value of 1. Their start and end nodes\n",
    "                # give an assignment of worker to task.\n",
    "                if smcf.flow(arc) > 0:\n",
    "                    print('Worker %d assigned to task %d.  Cost = %d Flow = %d' %\n",
    "                          (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                    \n",
    "                #else:\n",
    "                    #print('Worker %d assigned to task %d.  Cost = %d  Flow = %d' %\n",
    "                      #    (smcf.tail(arc), smcf.head(arc), smcf.unit_cost(arc),smcf.flow(arc)))\n",
    "                \n",
    "    else:\n",
    "        print('There was an issue with the min cost flow input.')\n",
    "        print(f'Status: {status}')\n",
    "            pp_A.append(f_add)\n",
    "\n",
    "        \n",
    "    return pp_A\n",
    "\n",
    "'''\n",
    "\n",
    "def make_reconstructed_edgelist(A,run):\n",
    "    \n",
    "    e_start=[2,3,4]\n",
    "    e1=[]\n",
    "    e2=[]\n",
    "    \n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        M=A[i]\n",
    "        print('M0',M)\n",
    "        X=M[0][1:]\n",
    "        M=M[1:,1:]\n",
    "        #print('M1',M)\n",
    "        \n",
    "        \n",
    "        for z in range(len(M)):\n",
    "            for j in range(len(M[0])):\n",
    "                e_mid=np.arange(e_start[-1]+1,e_start[-1]+len(M[0])+1)\n",
    "                if M[z,j]!=0:\n",
    "                    #print(z,e_start)\n",
    "                    e1.append(int(e_start[z]))\n",
    "                    #print('e',e_mid)\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                if z==0 and X[j]!=0:\n",
    "                    e1.append(int(1))\n",
    "                    e2.append(int(e_mid[j]))\n",
    "                    \n",
    "        \n",
    "        e_start=e_mid\n",
    "        #print('mid',e_mid)\n",
    "    \n",
    "    \n",
    "    np.savetxt('./'+str(run)+'_GT'+'/'+'reconstruct.edgelist', np.c_[e1,e2], fmt='%i',delimiter='\\t')\n",
    "    return 0\n",
    "\n",
    "def d_mask_function(x,r_core,alpha):\n",
    "    if x < r_core:\n",
    "        return 1\n",
    "    else:\n",
    "        return (x/r_core)**alpha\n",
    "    \n",
    "    \n",
    "def complete_postprocess(Ad,d,a):\n",
    "    \n",
    "    Ad_n = []\n",
    "    #Ad_n=copy.deepcopy(Ad)\n",
    "    \n",
    "    for h in range(len(Ad)):\n",
    "        \n",
    "        A_t,ill_flag=treshold(Ad[h],t=0.5)\n",
    "        \n",
    "        #print('ill_flag',ill_flag)\n",
    "        #print(Ad[h],A_t)\n",
    "        if ill_flag==True:\n",
    "            Ad[h]=np.multiply(Ad[h].detach().numpy(),d[h].detach().numpy())\n",
    "            A_t = postprocess_MinCostAss(np.array([Ad[h]]),a)[0]\n",
    "            \n",
    "        #print(Ad[h],A_t)\n",
    "        Ad_n.append(A_t)\n",
    "    #Ad=postprocess_MinCostAss(Ad)\n",
    "\n",
    "\n",
    "\n",
    "    return Ad_n\n",
    "\n",
    "def treshold(matrix, t):\n",
    "    z=np.where(matrix >= t, 1, 0)\n",
    "    \n",
    "    ill_flag=False\n",
    "\n",
    "      \n",
    "    if (~z.any(axis=0)).any() or any(np.sum(z[:,1:], axis=0)>1):\n",
    "        ill_flag=True\n",
    "          \n",
    "    return z,ill_flag\n",
    "\n",
    "def err_perc(a,b):\n",
    "    w=0\n",
    "    s=0\n",
    "    for i in range(len(a)):\n",
    "        m=a[i]-b[i].detach().numpy()\n",
    "        w=w+0.5*np.sum(np.abs(m))\n",
    "        s=s+np.size(m)\n",
    "    \n",
    "    \n",
    "    print('w,s',w,s)\n",
    "    \n",
    "    return w*100/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.87 0.05 0.08 0.77 0.11]\n",
      " [0.05 0.12 0.19 0.11 0.14 0.93]\n",
      " [0.07 0.12 0.45 0.89 0.23 0.05]\n",
      " [0.04 0.1  0.97 0.65 0.34 0.02]]\n",
      "[[1.   1.   1.   1.   1.   1.  ]\n",
      " [1.   0.75 0.07 0.1  0.08 0.8 ]\n",
      " [1.   0.69 0.07 0.88 0.34 0.02]\n",
      " [1.   0.1  0.9  0.05 0.84 0.02]]\n",
      "[[9.900e-01 8.700e-01 5.000e-02 8.000e-02 7.700e-01 1.100e-01]\n",
      " [5.000e-02 9.000e-02 1.330e-02 1.100e-02 1.120e-02 7.440e-01]\n",
      " [7.000e-02 8.280e-02 3.150e-02 7.832e-01 7.820e-02 1.000e-03]\n",
      " [4.000e-02 1.000e-02 8.730e-01 3.250e-02 2.856e-01 4.000e-04]]\n",
      "[[0.87 0.05 0.08 0.77 0.11]\n",
      " [0.12 0.19 0.11 0.14 0.93]\n",
      " [0.12 0.45 0.89 0.23 0.05]\n",
      " [0.1  0.97 0.65 0.34 0.02]]\n",
      "True\n",
      "0.2222222222222222\n",
      "tensor(0.8848)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.4513,  0.1221, -0.0285,  ...,  0.0232,  0.0256,  0.0393],\n",
       "         [-0.5104,  0.1248, -0.0601,  ...,  0.0736,  0.0396, -0.0459],\n",
       "         ...,\n",
       "         [-0.6634,  0.2230, -0.1667,  ...,  0.0521, -0.0640, -0.0107],\n",
       "         [-0.4996,  0.2267, -0.2140,  ...,  0.0140, -0.0024,  0.0838],\n",
       "         [-0.6322,  0.2320, -0.1698,  ..., -0.0068,  0.0372,  0.0009]]),\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-5.5557e-01,  2.5337e-01, -2.3358e-01,  ...,  3.3732e-03,\n",
       "          -3.7825e-02,  4.3635e-02],\n",
       "         [-4.0343e-01,  1.7492e-01, -1.1902e-01,  ..., -6.7206e-04,\n",
       "          -3.9526e-03,  1.7096e-02],\n",
       "         ...,\n",
       "         [-5.6323e-01,  2.0225e-01, -1.1524e-01,  ...,  4.7995e-03,\n",
       "           5.9752e-02, -5.0799e-02],\n",
       "         [-6.4361e-01,  2.8356e-01, -2.3510e-01,  ..., -7.7097e-02,\n",
       "           1.3224e-01,  3.0098e-04],\n",
       "         [-6.1588e-01,  2.2746e-01, -1.7741e-01,  ...,  1.0176e-01,\n",
       "          -9.0410e-02, -8.7811e-03]]),\n",
       " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 0.3376, 1.0000, 0.2039, 0.1133, 0.1924, 1.0000, 1.0000, 0.1004,\n",
       "          0.3035, 0.2087, 0.0646, 0.1303, 0.6867, 0.6972, 0.2086],\n",
       "         [1.0000, 0.1476, 0.2100, 0.5090, 0.0986, 1.0000, 0.1835, 0.1898, 0.2169,\n",
       "          0.5451, 1.0000, 0.0904, 0.2257, 0.3655, 0.6353, 0.1865],\n",
       "         [1.0000, 0.1572, 0.7700, 1.0000, 0.0795, 0.4891, 0.2804, 0.9851, 0.2756,\n",
       "          0.3036, 0.3561, 0.0586, 0.1231, 1.0000, 0.7148, 0.1438],\n",
       "         [1.0000, 0.0575, 0.1270, 1.0000, 0.0395, 0.2265, 0.0772, 0.1700, 1.0000,\n",
       "          0.0992, 0.1421, 0.0355, 0.0605, 0.3151, 0.1342, 0.0589],\n",
       "         [1.0000, 0.1822, 1.0000, 0.4034, 0.0804, 0.2345, 0.4001, 1.0000, 0.1640,\n",
       "          0.2377, 0.2128, 0.0533, 0.1048, 1.0000, 0.5448, 0.1404],\n",
       "         [1.0000, 0.0778, 0.2136, 1.0000, 0.0490, 0.3163, 0.1125, 0.3091, 1.0000,\n",
       "          0.1387, 0.1944, 0.0415, 0.0754, 0.7704, 0.2124, 0.0771],\n",
       "         [1.0000, 0.5888, 0.1009, 0.0551, 1.0000, 0.1071, 0.2299, 0.0709, 0.0371,\n",
       "          0.2976, 0.1624, 0.5878, 0.6613, 0.0724, 0.1806, 1.0000],\n",
       "         [1.0000, 1.0000, 0.3751, 0.0898, 0.3440, 0.1349, 1.0000, 0.1950, 0.0536,\n",
       "          0.3947, 0.1852, 0.1178, 0.2366, 0.1615, 0.4341, 0.6712],\n",
       "         [1.0000, 0.2529, 0.2336, 0.2552, 0.1630, 1.0000, 0.2822, 0.1778, 0.1260,\n",
       "          1.0000, 1.0000, 0.1394, 0.4937, 0.2801, 1.0000, 0.3809],\n",
       "         [1.0000, 0.6994, 0.3821, 0.1811, 0.2672, 0.5411, 0.8085, 0.2274, 0.0921,\n",
       "          1.0000, 1.0000, 0.1559, 0.6085, 0.2873, 1.0000, 0.9656],\n",
       "         [1.0000, 0.2503, 0.0754, 0.0527, 1.0000, 0.1144, 0.1380, 0.0567, 0.0369,\n",
       "          0.2606, 0.1758, 1.0000, 1.0000, 0.0621, 0.1458, 0.5705],\n",
       "         [1.0000, 1.0000, 0.1790, 0.0831, 1.0000, 0.1786, 0.5422, 0.1144, 0.0513,\n",
       "          0.8398, 0.3022, 0.3136, 1.0000, 0.1189, 0.4201, 1.0000],\n",
       "         [1.0000, 0.6433, 0.8381, 0.2201, 0.1946, 0.4329, 1.0000, 0.3951, 0.1034,\n",
       "          1.0000, 0.6327, 0.1098, 0.3107, 0.4695, 1.0000, 0.5295],\n",
       "         [1.0000, 0.5521, 0.1269, 0.0795, 1.0000, 0.2016, 0.2709, 0.0891, 0.0509,\n",
       "          0.8013, 0.3688, 0.6315, 1.0000, 0.1002, 0.3211, 1.0000],\n",
       "         [1.0000, 1.0000, 0.7810, 0.1392, 0.2475, 0.2205, 1.0000, 0.3264, 0.0745,\n",
       "          0.7426, 0.3082, 0.1104, 0.2693, 0.2877, 1.0000, 0.6423]],\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.99, 0.87,0.05,0.08,0.77,0.11], [0.05, 0.12,0.19,0.11,0.14,0.93],[0.07, 0.12,0.45,0.89,0.23,0.05],[0.04, 0.1,0.97,0.65,0.34,0.02]])\n",
    "print(a)\n",
    "\n",
    "b = np.array([[1, 1,1,1,1,1], [1, 0.75,0.07,0.1,0.08,0.8],[1, 0.69,0.07,0.88,0.34,0.02],[1, 0.1,0.9,0.05,0.84,0.02]])\n",
    "print(b)\n",
    "\n",
    "print(np.multiply(a,b))\n",
    "#print(threshold_matrix(a, 0.2))\n",
    "print(a[:,1:])\n",
    "print(any(np.sum(a[:,1:], axis=0)>1))\n",
    "np.concatenate((a, b), axis=0)\n",
    "\n",
    "\n",
    "#np.concatenate((a, b.T), axis=1)\n",
    "c = np.array([[1, 0,0], [0, 1,0],[0, 0,1]])\n",
    "d = np.array([[1, 0,0], [0, 0,1],[0, 0,1]])\n",
    "\n",
    "\n",
    "m=c-d\n",
    "print(np.sum(np.abs(m))/np.size(m))\n",
    "\n",
    "\n",
    "c = torch.from_numpy(c).float()\n",
    "d = torch.from_numpy(d).float()\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = cross_entropy_loss(c, d)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "loadgraph(run=1)\n",
    "\n",
    "#print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCAT\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      3\u001b[0m                  num_encoder_layers: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      4\u001b[0m                  emb_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                  dim_feedforward: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m,):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28msuper\u001b[39m(CAT, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 out = False, \n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.05,\n",
    "                 use_transformer: bool = True):\n",
    "        super(CAT, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=nhead,dim_feedforward=dim_feedforward)\n",
    "        \n",
    "        \n",
    "        self.decoder_1_1 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_1_2 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_2_1 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_2_2 = nn.TransformerDecoder(decoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.out=out \n",
    "        \n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.Ad = makeAdja()\n",
    "        self.use_transformer=use_transformer\n",
    "    def forward(self,\n",
    "                src_t1: Tensor,\n",
    "                src_t2: Tensor,\n",
    "                src_padding_mask1: Tensor,\n",
    "                src_padding_mask2: Tensor):\n",
    "        \n",
    "        #print('trans_src_before_pos',src_t1,src_t1.size())\n",
    "        #print('trans_src_toke',self.src_tok_emb(src),self.src_tok_emb(src).size())\n",
    "        \n",
    "        if not self.use_transformer:  # <-- Add this condition\n",
    "            src_t1_t = torch.transpose(src_t1, 0, 1)\n",
    "            src_t2_t = torch.transpose(src_t2, 0, 1)\n",
    "            src_t2_t = torch.transpose(src_t2_t, 1, 2)\n",
    "            z = self.sig(torch.bmm(src_t1_t, src_t2_t))\n",
    "            Ad = self.Ad.forward(z, src_padding_mask1, src_padding_mask2)\n",
    "            return Ad\n",
    "        \n",
    "        src1_emb = src_t1\n",
    "        src2_emb = src_t2\n",
    "        #print('src1',src1_emb.size())\n",
    "        #print('src2',src2_emb.size())\n",
    "        #print('trans_src_padd',src_padding_mask1,src_padding_mask1.size())\n",
    "        povi_1 = self.decoder_1_1(pos_1, vis_1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        povi_2 = self.decoder_1_2(pos_2, vis_2,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "        out_1 = self.decoder_2_1(povi_1, povi_2,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        out_2 = self.decoder_2_2(povi_2, povi_1,tgt_key_padding_mask=src_padding_mask2,memory_key_padding_mask=src_padding_mask1)\n",
    "        \n",
    "\n",
    "        #out1=torch.transpose(out1,0,1)\n",
    "        #out2=torch.transpose(out2,0,1)\n",
    "        #out2=torch.transpose(out2,1,2)\n",
    "        \n",
    "        #z=self.sig(torch.bmm(out1,out2))\n",
    "        \n",
    "        \n",
    "        out_1=torch.transpose(out_1,0,1)\n",
    "        out_2=torch.transpose(out_2,0,1)\n",
    "        out_2=torch.transpose(out_2,1,2)\n",
    "        \n",
    "        z=self.sig(torch.bmm(out_1,out_2))\n",
    "        #print('z',z.size())\n",
    "        \n",
    "        Ad=self.Ad.forward(z,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "        \n",
    "        if self.out:\n",
    "            return Ad,out1,out2,out_dec1,src_t1,src_t2\n",
    "        else:\n",
    "            return Ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m Loss(pen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(transformer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.98\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstop\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim=3\n",
    "\n",
    "emb_size= 64 ###!!!!24 for n2v emb\n",
    "nhead= 4    ####!!!! 6 for n2v emb\n",
    "num_encoder_layers = 2\n",
    "\n",
    "\n",
    "transformer = CAT(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.172, Val loss: 4.138, Epoch time = 1.739s\n",
      "Epoch: 2, Train loss: 5.095, Val loss: 4.317, Epoch time = 1.505s\n",
      "Epoch: 3, Train loss: 4.109, Val loss: 4.226, Epoch time = 1.464s\n",
      "Epoch: 4, Train loss: 5.454, Val loss: 4.396, Epoch time = 1.562s\n",
      "Epoch: 5, Train loss: 4.346, Val loss: 4.863, Epoch time = 1.787s\n",
      "Epoch: 6, Train loss: 3.715, Val loss: 4.145, Epoch time = 1.723s\n",
      "Epoch: 7, Train loss: 4.355, Val loss: 4.299, Epoch time = 1.554s\n",
      "Epoch: 8, Train loss: 4.323, Val loss: 4.740, Epoch time = 1.623s\n",
      "Epoch: 9, Train loss: 4.438, Val loss: 4.823, Epoch time = 1.752s\n",
      "Epoch: 10, Train loss: 4.091, Val loss: 4.093, Epoch time = 1.754s\n",
      "Epoch: 11, Train loss: 4.233, Val loss: 4.116, Epoch time = 1.769s\n",
      "Epoch: 12, Train loss: 4.173, Val loss: 3.924, Epoch time = 1.732s\n",
      "Epoch: 13, Train loss: 4.093, Val loss: 4.080, Epoch time = 1.668s\n",
      "Epoch: 14, Train loss: 3.837, Val loss: 4.259, Epoch time = 1.670s\n",
      "Epoch: 15, Train loss: 4.357, Val loss: 4.511, Epoch time = 1.695s\n",
      "Epoch: 16, Train loss: 4.066, Val loss: 4.550, Epoch time = 1.729s\n",
      "Epoch: 17, Train loss: 4.236, Val loss: 4.989, Epoch time = 1.887s\n",
      "Epoch: 18, Train loss: 4.003, Val loss: 4.701, Epoch time = 1.908s\n",
      "Epoch: 19, Train loss: 4.040, Val loss: 4.235, Epoch time = 1.954s\n",
      "Epoch: 20, Train loss: 4.633, Val loss: 4.281, Epoch time = 2.095s\n",
      "Epoch: 21, Train loss: 4.766, Val loss: 4.473, Epoch time = 2.072s\n",
      "Epoch: 22, Train loss: 3.818, Val loss: 4.029, Epoch time = 1.979s\n",
      "Epoch: 23, Train loss: 3.950, Val loss: 4.250, Epoch time = 2.046s\n",
      "Epoch: 24, Train loss: 4.577, Val loss: 4.640, Epoch time = 2.160s\n",
      "Epoch: 25, Train loss: 4.084, Val loss: 4.838, Epoch time = 2.263s\n",
      "Epoch: 26, Train loss: 4.069, Val loss: 4.495, Epoch time = 2.089s\n",
      "Epoch: 27, Train loss: 4.570, Val loss: 4.538, Epoch time = 2.087s\n",
      "Epoch: 28, Train loss: 4.782, Val loss: 3.641, Epoch time = 2.124s\n",
      "Epoch: 29, Train loss: 3.968, Val loss: 4.348, Epoch time = 2.035s\n",
      "Epoch: 30, Train loss: 3.842, Val loss: 5.134, Epoch time = 2.275s\n",
      "Epoch: 31, Train loss: 5.228, Val loss: 3.875, Epoch time = 1.863s\n",
      "Epoch: 32, Train loss: 4.327, Val loss: 3.833, Epoch time = 1.870s\n",
      "Epoch: 33, Train loss: 4.131, Val loss: 4.894, Epoch time = 1.827s\n",
      "Epoch: 34, Train loss: 4.130, Val loss: 3.689, Epoch time = 1.804s\n",
      "Epoch: 35, Train loss: 4.352, Val loss: 3.907, Epoch time = 1.875s\n",
      "Epoch: 36, Train loss: 4.109, Val loss: 4.594, Epoch time = 1.870s\n",
      "Epoch: 37, Train loss: 4.124, Val loss: 3.766, Epoch time = 1.763s\n",
      "Epoch: 38, Train loss: 3.708, Val loss: 4.153, Epoch time = 1.904s\n",
      "Epoch: 39, Train loss: 3.899, Val loss: 4.031, Epoch time = 1.794s\n",
      "Epoch: 40, Train loss: 3.840, Val loss: 3.853, Epoch time = 1.682s\n",
      "Epoch: 41, Train loss: 4.404, Val loss: 4.045, Epoch time = 1.688s\n",
      "Epoch: 42, Train loss: 3.685, Val loss: 3.836, Epoch time = 1.910s\n",
      "Epoch: 43, Train loss: 4.731, Val loss: 3.842, Epoch time = 1.661s\n",
      "Epoch: 44, Train loss: 4.151, Val loss: 4.557, Epoch time = 1.753s\n",
      "Epoch: 45, Train loss: 4.645, Val loss: 3.846, Epoch time = 1.936s\n",
      "Epoch: 46, Train loss: 3.729, Val loss: 4.042, Epoch time = 1.819s\n",
      "Epoch: 47, Train loss: 3.769, Val loss: 3.840, Epoch time = 1.880s\n",
      "Epoch: 48, Train loss: 3.890, Val loss: 4.735, Epoch time = 1.836s\n",
      "Epoch: 49, Train loss: 4.309, Val loss: 4.171, Epoch time = 1.790s\n",
      "Epoch: 50, Train loss: 5.031, Val loss: 3.635, Epoch time = 1.738s\n",
      "Epoch: 51, Train loss: 3.689, Val loss: 3.384, Epoch time = 1.854s\n",
      "Epoch: 52, Train loss: 4.226, Val loss: 4.251, Epoch time = 1.686s\n",
      "Epoch: 53, Train loss: 4.174, Val loss: 4.326, Epoch time = 1.766s\n",
      "Epoch: 54, Train loss: 3.700, Val loss: 4.181, Epoch time = 1.639s\n",
      "Epoch: 55, Train loss: 3.703, Val loss: 3.924, Epoch time = 1.615s\n",
      "Epoch: 56, Train loss: 4.029, Val loss: 4.212, Epoch time = 1.645s\n",
      "Epoch: 57, Train loss: 3.855, Val loss: 4.079, Epoch time = 1.619s\n",
      "Epoch: 58, Train loss: 3.810, Val loss: 3.666, Epoch time = 1.638s\n",
      "Epoch: 59, Train loss: 3.867, Val loss: 4.566, Epoch time = 1.887s\n",
      "Epoch: 60, Train loss: 3.711, Val loss: 3.910, Epoch time = 1.697s\n",
      "Epoch: 61, Train loss: 3.674, Val loss: 3.847, Epoch time = 1.728s\n",
      "Epoch: 62, Train loss: 3.835, Val loss: 3.646, Epoch time = 1.670s\n",
      "Epoch: 63, Train loss: 4.737, Val loss: 3.648, Epoch time = 1.637s\n",
      "Epoch: 64, Train loss: 3.842, Val loss: 4.389, Epoch time = 1.656s\n",
      "Epoch: 65, Train loss: 3.716, Val loss: 3.862, Epoch time = 1.675s\n",
      "Epoch: 66, Train loss: 3.656, Val loss: 4.299, Epoch time = 1.691s\n",
      "Epoch: 67, Train loss: 3.419, Val loss: 3.896, Epoch time = 1.724s\n",
      "Epoch: 68, Train loss: 4.326, Val loss: 3.396, Epoch time = 1.717s\n",
      "Epoch: 69, Train loss: 3.845, Val loss: 3.188, Epoch time = 1.904s\n",
      "Epoch: 70, Train loss: 4.226, Val loss: 3.688, Epoch time = 1.896s\n",
      "Epoch: 71, Train loss: 3.670, Val loss: 3.568, Epoch time = 1.840s\n",
      "Epoch: 72, Train loss: 4.339, Val loss: 3.588, Epoch time = 1.904s\n",
      "Epoch: 73, Train loss: 4.421, Val loss: 3.881, Epoch time = 1.706s\n",
      "Epoch: 74, Train loss: 3.813, Val loss: 4.860, Epoch time = 2.039s\n",
      "Epoch: 75, Train loss: 5.083, Val loss: 3.820, Epoch time = 2.386s\n",
      "Epoch: 76, Train loss: 3.589, Val loss: 3.798, Epoch time = 1.862s\n",
      "Epoch: 77, Train loss: 4.515, Val loss: 3.557, Epoch time = 1.872s\n",
      "Epoch: 78, Train loss: 4.160, Val loss: 3.597, Epoch time = 1.866s\n",
      "Epoch: 79, Train loss: 3.661, Val loss: 3.543, Epoch time = 1.761s\n",
      "Epoch: 80, Train loss: 3.722, Val loss: 3.883, Epoch time = 1.790s\n",
      "Epoch: 81, Train loss: 3.628, Val loss: 3.785, Epoch time = 1.696s\n",
      "Epoch: 82, Train loss: 3.652, Val loss: 3.703, Epoch time = 1.713s\n",
      "Epoch: 83, Train loss: 3.499, Val loss: 3.453, Epoch time = 1.655s\n",
      "Epoch: 84, Train loss: 4.695, Val loss: 4.117, Epoch time = 1.632s\n",
      "Epoch: 85, Train loss: 3.958, Val loss: 3.424, Epoch time = 1.688s\n",
      "Epoch: 86, Train loss: 3.401, Val loss: 3.710, Epoch time = 1.630s\n",
      "Epoch: 87, Train loss: 3.870, Val loss: 3.616, Epoch time = 1.618s\n",
      "Epoch: 88, Train loss: 3.526, Val loss: 3.989, Epoch time = 1.782s\n",
      "Epoch: 89, Train loss: 3.590, Val loss: 4.081, Epoch time = 1.710s\n",
      "Epoch: 90, Train loss: 3.553, Val loss: 3.406, Epoch time = 1.662s\n",
      "Epoch: 91, Train loss: 3.538, Val loss: 4.876, Epoch time = 1.626s\n",
      "Epoch: 92, Train loss: 3.582, Val loss: 3.572, Epoch time = 1.843s\n",
      "Epoch: 93, Train loss: 3.382, Val loss: 3.659, Epoch time = 1.738s\n",
      "Epoch: 94, Train loss: 3.764, Val loss: 3.799, Epoch time = 1.866s\n",
      "Epoch: 95, Train loss: 4.432, Val loss: 3.544, Epoch time = 1.872s\n",
      "Epoch: 96, Train loss: 4.514, Val loss: 4.599, Epoch time = 1.849s\n",
      "Epoch: 97, Train loss: 3.449, Val loss: 4.665, Epoch time = 1.714s\n",
      "Epoch: 98, Train loss: 3.735, Val loss: 3.852, Epoch time = 1.770s\n",
      "Epoch: 99, Train loss: 3.483, Val loss: 4.288, Epoch time = 1.758s\n",
      "Epoch: 100, Train loss: 4.417, Val loss: 3.959, Epoch time = 1.816s\n",
      "Epoch: 101, Train loss: 4.121, Val loss: 3.486, Epoch time = 1.784s\n",
      "Epoch: 102, Train loss: 3.736, Val loss: 3.383, Epoch time = 1.620s\n",
      "Epoch: 103, Train loss: 3.430, Val loss: 3.569, Epoch time = 1.638s\n",
      "Epoch: 104, Train loss: 4.281, Val loss: 4.382, Epoch time = 1.635s\n",
      "Epoch: 105, Train loss: 3.536, Val loss: 4.049, Epoch time = 1.654s\n",
      "Epoch: 106, Train loss: 4.048, Val loss: 3.599, Epoch time = 1.798s\n",
      "Epoch: 107, Train loss: 3.712, Val loss: 3.634, Epoch time = 1.683s\n",
      "Epoch: 108, Train loss: 3.893, Val loss: 3.718, Epoch time = 1.808s\n",
      "Epoch: 109, Train loss: 3.906, Val loss: 3.404, Epoch time = 1.752s\n",
      "Epoch: 110, Train loss: 3.535, Val loss: 3.650, Epoch time = 1.648s\n",
      "Epoch: 111, Train loss: 4.066, Val loss: 4.538, Epoch time = 1.727s\n",
      "Epoch: 112, Train loss: 3.404, Val loss: 4.418, Epoch time = 1.819s\n",
      "Epoch: 113, Train loss: 3.876, Val loss: 4.319, Epoch time = 1.819s\n",
      "Epoch: 114, Train loss: 3.583, Val loss: 3.744, Epoch time = 1.730s\n",
      "Epoch: 115, Train loss: 4.107, Val loss: 4.477, Epoch time = 1.706s\n",
      "Epoch: 116, Train loss: 4.784, Val loss: 4.427, Epoch time = 1.919s\n",
      "Epoch: 117, Train loss: 3.497, Val loss: 4.184, Epoch time = 1.813s\n",
      "Epoch: 118, Train loss: 3.777, Val loss: 3.738, Epoch time = 1.935s\n",
      "Epoch: 119, Train loss: 3.445, Val loss: 3.678, Epoch time = 1.864s\n",
      "Epoch: 120, Train loss: 3.975, Val loss: 3.788, Epoch time = 1.656s\n",
      "Epoch: 121, Train loss: 3.894, Val loss: 3.564, Epoch time = 1.608s\n",
      "Epoch: 122, Train loss: 3.990, Val loss: 3.660, Epoch time = 1.744s\n",
      "Epoch: 123, Train loss: 3.501, Val loss: 3.974, Epoch time = 1.777s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Train loss: 3.884, Val loss: 4.183, Epoch time = 1.655s\n",
      "Epoch: 125, Train loss: 4.299, Val loss: 4.316, Epoch time = 1.620s\n",
      "Epoch: 126, Train loss: 4.396, Val loss: 4.571, Epoch time = 1.677s\n",
      "Epoch: 127, Train loss: 3.554, Val loss: 3.828, Epoch time = 1.628s\n",
      "Epoch: 128, Train loss: 3.831, Val loss: 4.330, Epoch time = 1.692s\n",
      "Epoch: 129, Train loss: 3.615, Val loss: 3.380, Epoch time = 1.737s\n",
      "Epoch: 130, Train loss: 3.594, Val loss: 4.384, Epoch time = 1.798s\n",
      "Epoch: 131, Train loss: 4.471, Val loss: 3.594, Epoch time = 1.670s\n",
      "Epoch: 132, Train loss: 3.543, Val loss: 3.418, Epoch time = 1.641s\n",
      "Epoch: 133, Train loss: 3.425, Val loss: 3.367, Epoch time = 1.658s\n",
      "Epoch: 134, Train loss: 3.871, Val loss: 3.613, Epoch time = 1.794s\n",
      "Epoch: 135, Train loss: 3.257, Val loss: 4.045, Epoch time = 1.912s\n",
      "Epoch: 136, Train loss: 4.534, Val loss: 3.597, Epoch time = 1.762s\n",
      "Epoch: 137, Train loss: 3.651, Val loss: 3.288, Epoch time = 1.689s\n",
      "Epoch: 138, Train loss: 4.180, Val loss: 4.123, Epoch time = 1.765s\n",
      "Epoch: 139, Train loss: 4.279, Val loss: 3.498, Epoch time = 1.689s\n",
      "Epoch: 140, Train loss: 3.952, Val loss: 3.611, Epoch time = 1.749s\n",
      "Epoch: 141, Train loss: 4.348, Val loss: 3.625, Epoch time = 1.772s\n",
      "Epoch: 142, Train loss: 3.654, Val loss: 4.055, Epoch time = 2.151s\n",
      "Epoch: 143, Train loss: 3.813, Val loss: 3.377, Epoch time = 1.879s\n",
      "Epoch: 144, Train loss: 4.715, Val loss: 3.469, Epoch time = 1.728s\n",
      "Epoch: 145, Train loss: 3.719, Val loss: 3.779, Epoch time = 1.677s\n",
      "Epoch: 146, Train loss: 3.945, Val loss: 4.174, Epoch time = 1.657s\n",
      "Epoch: 147, Train loss: 3.606, Val loss: 3.563, Epoch time = 1.684s\n",
      "Epoch: 148, Train loss: 3.814, Val loss: 4.169, Epoch time = 1.695s\n",
      "Epoch: 149, Train loss: 3.665, Val loss: 4.033, Epoch time = 1.801s\n",
      "Epoch: 150, Train loss: 3.434, Val loss: 4.038, Epoch time = 1.963s\n",
      "Epoch: 151, Train loss: 3.808, Val loss: 4.161, Epoch time = 1.949s\n",
      "Epoch: 152, Train loss: 3.391, Val loss: 4.421, Epoch time = 1.681s\n",
      "Epoch: 153, Train loss: 3.520, Val loss: 2.922, Epoch time = 1.706s\n",
      "Epoch: 154, Train loss: 3.930, Val loss: 3.725, Epoch time = 1.762s\n",
      "Epoch: 155, Train loss: 4.199, Val loss: 3.651, Epoch time = 1.637s\n",
      "Epoch: 156, Train loss: 3.498, Val loss: 3.697, Epoch time = 1.745s\n",
      "Epoch: 157, Train loss: 3.423, Val loss: 3.526, Epoch time = 1.758s\n",
      "Epoch: 158, Train loss: 3.954, Val loss: 3.429, Epoch time = 1.698s\n",
      "Epoch: 159, Train loss: 3.380, Val loss: 3.790, Epoch time = 1.857s\n",
      "Epoch: 160, Train loss: 4.495, Val loss: 4.191, Epoch time = 1.744s\n",
      "Epoch: 161, Train loss: 3.713, Val loss: 3.358, Epoch time = 1.701s\n",
      "Epoch: 162, Train loss: 3.802, Val loss: 3.508, Epoch time = 1.666s\n",
      "Epoch: 163, Train loss: 3.473, Val loss: 3.853, Epoch time = 1.816s\n",
      "Epoch: 164, Train loss: 3.938, Val loss: 3.787, Epoch time = 1.661s\n",
      "Epoch: 165, Train loss: 4.138, Val loss: 4.257, Epoch time = 1.697s\n",
      "Epoch: 166, Train loss: 4.334, Val loss: 4.520, Epoch time = 1.931s\n",
      "Epoch: 167, Train loss: 3.187, Val loss: 3.481, Epoch time = 1.755s\n",
      "Epoch: 168, Train loss: 3.463, Val loss: 4.121, Epoch time = 1.823s\n",
      "Epoch: 169, Train loss: 3.333, Val loss: 4.058, Epoch time = 1.683s\n",
      "Epoch: 170, Train loss: 3.807, Val loss: 3.305, Epoch time = 1.685s\n",
      "Epoch: 171, Train loss: 3.285, Val loss: 3.871, Epoch time = 1.774s\n",
      "Epoch: 172, Train loss: 4.265, Val loss: 3.960, Epoch time = 1.664s\n",
      "Epoch: 173, Train loss: 3.619, Val loss: 3.087, Epoch time = 1.677s\n",
      "Epoch: 174, Train loss: 3.738, Val loss: 3.244, Epoch time = 1.703s\n",
      "Epoch: 175, Train loss: 3.440, Val loss: 3.614, Epoch time = 1.749s\n",
      "Epoch: 176, Train loss: 3.725, Val loss: 4.322, Epoch time = 1.897s\n",
      "Epoch: 177, Train loss: 3.494, Val loss: 3.080, Epoch time = 1.759s\n",
      "Epoch: 178, Train loss: 3.949, Val loss: 3.412, Epoch time = 1.751s\n",
      "Epoch: 179, Train loss: 3.701, Val loss: 3.370, Epoch time = 1.708s\n",
      "Epoch: 180, Train loss: 3.855, Val loss: 4.120, Epoch time = 1.629s\n",
      "Epoch: 181, Train loss: 3.647, Val loss: 3.024, Epoch time = 1.695s\n",
      "Epoch: 182, Train loss: 3.635, Val loss: 3.535, Epoch time = 1.728s\n",
      "Epoch: 183, Train loss: 3.310, Val loss: 3.818, Epoch time = 1.793s\n",
      "Epoch: 184, Train loss: 3.671, Val loss: 3.676, Epoch time = 1.802s\n",
      "Epoch: 185, Train loss: 3.309, Val loss: 3.714, Epoch time = 1.820s\n",
      "Epoch: 186, Train loss: 3.395, Val loss: 4.019, Epoch time = 1.757s\n",
      "Epoch: 187, Train loss: 3.249, Val loss: 4.159, Epoch time = 1.851s\n",
      "Epoch: 188, Train loss: 3.639, Val loss: 3.717, Epoch time = 1.867s\n",
      "Epoch: 189, Train loss: 3.487, Val loss: 3.589, Epoch time = 1.940s\n",
      "Epoch: 190, Train loss: 3.265, Val loss: 3.453, Epoch time = 1.846s\n",
      "Epoch: 191, Train loss: 3.370, Val loss: 3.932, Epoch time = 1.879s\n",
      "Epoch: 192, Train loss: 3.100, Val loss: 4.198, Epoch time = 1.866s\n",
      "Epoch: 193, Train loss: 3.301, Val loss: 3.977, Epoch time = 1.813s\n",
      "Epoch: 194, Train loss: 3.411, Val loss: 3.526, Epoch time = 1.817s\n",
      "Epoch: 195, Train loss: 3.453, Val loss: 4.273, Epoch time = 1.764s\n",
      "Epoch: 196, Train loss: 4.097, Val loss: 4.044, Epoch time = 1.758s\n",
      "Epoch: 197, Train loss: 3.952, Val loss: 3.460, Epoch time = 1.758s\n",
      "Epoch: 198, Train loss: 3.100, Val loss: 3.262, Epoch time = 1.846s\n",
      "Epoch: 199, Train loss: 4.459, Val loss: 3.355, Epoch time = 1.642s\n",
      "Epoch: 200, Train loss: 3.739, Val loss: 3.449, Epoch time = 1.648s\n",
      "Epoch: 201, Train loss: 3.430, Val loss: 3.462, Epoch time = 1.661s\n",
      "Epoch: 202, Train loss: 3.493, Val loss: 3.453, Epoch time = 1.710s\n",
      "Epoch: 203, Train loss: 3.679, Val loss: 3.655, Epoch time = 1.689s\n",
      "Epoch: 204, Train loss: 4.548, Val loss: 3.613, Epoch time = 1.677s\n",
      "Epoch: 205, Train loss: 3.494, Val loss: 3.603, Epoch time = 1.815s\n",
      "Epoch: 206, Train loss: 3.495, Val loss: 3.037, Epoch time = 1.734s\n",
      "Epoch: 207, Train loss: 3.680, Val loss: 3.980, Epoch time = 1.695s\n",
      "Epoch: 208, Train loss: 3.228, Val loss: 3.438, Epoch time = 1.691s\n",
      "Epoch: 209, Train loss: 3.425, Val loss: 3.436, Epoch time = 1.687s\n",
      "Epoch: 210, Train loss: 3.352, Val loss: 3.906, Epoch time = 1.639s\n",
      "Epoch: 211, Train loss: 3.348, Val loss: 4.701, Epoch time = 1.843s\n",
      "Epoch: 212, Train loss: 3.382, Val loss: 3.812, Epoch time = 1.686s\n",
      "Epoch: 213, Train loss: 3.053, Val loss: 3.159, Epoch time = 1.794s\n",
      "Epoch: 214, Train loss: 3.262, Val loss: 3.271, Epoch time = 1.786s\n",
      "Epoch: 215, Train loss: 3.296, Val loss: 4.027, Epoch time = 1.806s\n",
      "Epoch: 216, Train loss: 3.364, Val loss: 3.282, Epoch time = 1.774s\n",
      "Epoch: 217, Train loss: 3.177, Val loss: 3.160, Epoch time = 1.821s\n",
      "Epoch: 218, Train loss: 3.836, Val loss: 3.570, Epoch time = 1.792s\n",
      "Epoch: 219, Train loss: 2.899, Val loss: 3.384, Epoch time = 1.793s\n",
      "Epoch: 220, Train loss: 4.041, Val loss: 3.598, Epoch time = 1.735s\n",
      "Epoch: 221, Train loss: 3.635, Val loss: 3.450, Epoch time = 1.772s\n",
      "Epoch: 222, Train loss: 3.687, Val loss: 3.618, Epoch time = 1.599s\n",
      "Epoch: 223, Train loss: 3.380, Val loss: 3.200, Epoch time = 1.641s\n",
      "Epoch: 224, Train loss: 3.386, Val loss: 3.583, Epoch time = 1.624s\n",
      "Epoch: 225, Train loss: 3.228, Val loss: 3.324, Epoch time = 1.694s\n",
      "Epoch: 226, Train loss: 3.400, Val loss: 4.240, Epoch time = 1.744s\n",
      "Epoch: 227, Train loss: 3.398, Val loss: 3.252, Epoch time = 1.685s\n",
      "Epoch: 228, Train loss: 3.637, Val loss: 3.501, Epoch time = 1.724s\n",
      "Epoch: 229, Train loss: 3.312, Val loss: 4.281, Epoch time = 1.678s\n",
      "Epoch: 230, Train loss: 3.180, Val loss: 3.348, Epoch time = 1.669s\n",
      "Epoch: 231, Train loss: 3.264, Val loss: 4.002, Epoch time = 1.670s\n",
      "Epoch: 232, Train loss: 3.442, Val loss: 4.009, Epoch time = 1.764s\n",
      "Epoch: 233, Train loss: 3.657, Val loss: 3.238, Epoch time = 1.776s\n",
      "Epoch: 234, Train loss: 3.376, Val loss: 3.934, Epoch time = 1.730s\n",
      "Epoch: 235, Train loss: 3.421, Val loss: 3.196, Epoch time = 1.662s\n",
      "Epoch: 236, Train loss: 3.458, Val loss: 3.477, Epoch time = 1.642s\n",
      "Epoch: 237, Train loss: 4.155, Val loss: 3.184, Epoch time = 1.787s\n",
      "Epoch: 238, Train loss: 3.672, Val loss: 3.553, Epoch time = 1.709s\n",
      "Epoch: 239, Train loss: 4.291, Val loss: 3.508, Epoch time = 1.697s\n",
      "Epoch: 240, Train loss: 3.652, Val loss: 3.532, Epoch time = 1.746s\n",
      "Epoch: 241, Train loss: 3.865, Val loss: 3.298, Epoch time = 1.860s\n",
      "Epoch: 242, Train loss: 3.476, Val loss: 3.930, Epoch time = 1.741s\n",
      "Epoch: 243, Train loss: 3.627, Val loss: 3.411, Epoch time = 1.865s\n",
      "Epoch: 244, Train loss: 3.423, Val loss: 3.610, Epoch time = 1.781s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245, Train loss: 3.583, Val loss: 3.615, Epoch time = 1.709s\n",
      "Epoch: 246, Train loss: 3.546, Val loss: 4.374, Epoch time = 1.807s\n",
      "Epoch: 247, Train loss: 3.097, Val loss: 3.466, Epoch time = 1.781s\n",
      "Epoch: 248, Train loss: 3.467, Val loss: 3.476, Epoch time = 1.808s\n",
      "Epoch: 249, Train loss: 3.354, Val loss: 2.981, Epoch time = 1.686s\n",
      "Epoch: 250, Train loss: 3.892, Val loss: 3.553, Epoch time = 1.744s\n",
      "Epoch: 251, Train loss: 3.275, Val loss: 4.143, Epoch time = 1.847s\n",
      "Epoch: 252, Train loss: 3.655, Val loss: 4.102, Epoch time = 1.866s\n",
      "Epoch: 253, Train loss: 3.645, Val loss: 3.988, Epoch time = 1.822s\n",
      "Epoch: 254, Train loss: 3.322, Val loss: 4.117, Epoch time = 1.847s\n",
      "Epoch: 255, Train loss: 3.117, Val loss: 3.459, Epoch time = 1.781s\n",
      "Epoch: 256, Train loss: 3.177, Val loss: 4.136, Epoch time = 1.773s\n",
      "Epoch: 257, Train loss: 3.228, Val loss: 3.449, Epoch time = 1.798s\n",
      "Epoch: 258, Train loss: 3.867, Val loss: 3.122, Epoch time = 1.676s\n",
      "Epoch: 259, Train loss: 3.937, Val loss: 3.570, Epoch time = 1.654s\n",
      "Epoch: 260, Train loss: 3.747, Val loss: 3.406, Epoch time = 1.758s\n",
      "Epoch: 261, Train loss: 4.233, Val loss: 3.135, Epoch time = 1.823s\n",
      "Epoch: 262, Train loss: 3.469, Val loss: 3.234, Epoch time = 1.840s\n",
      "Epoch: 263, Train loss: 3.569, Val loss: 3.342, Epoch time = 1.827s\n",
      "Epoch: 264, Train loss: 3.797, Val loss: 3.550, Epoch time = 1.770s\n",
      "Epoch: 265, Train loss: 3.539, Val loss: 3.470, Epoch time = 1.880s\n",
      "Epoch: 266, Train loss: 3.538, Val loss: 3.070, Epoch time = 1.744s\n",
      "Epoch: 267, Train loss: 3.636, Val loss: 3.502, Epoch time = 1.730s\n",
      "Epoch: 268, Train loss: 3.342, Val loss: 3.709, Epoch time = 1.799s\n",
      "Epoch: 269, Train loss: 3.821, Val loss: 3.372, Epoch time = 1.816s\n",
      "Epoch: 270, Train loss: 3.548, Val loss: 3.451, Epoch time = 1.758s\n",
      "Epoch: 271, Train loss: 3.838, Val loss: 3.728, Epoch time = 1.698s\n",
      "Epoch: 272, Train loss: 3.486, Val loss: 3.450, Epoch time = 1.781s\n",
      "Epoch: 273, Train loss: 3.418, Val loss: 3.718, Epoch time = 1.696s\n",
      "Epoch: 274, Train loss: 3.456, Val loss: 3.087, Epoch time = 1.684s\n",
      "Epoch: 275, Train loss: 3.659, Val loss: 4.231, Epoch time = 1.852s\n",
      "Epoch: 276, Train loss: 4.256, Val loss: 3.163, Epoch time = 1.821s\n",
      "Epoch: 277, Train loss: 3.400, Val loss: 3.075, Epoch time = 1.827s\n",
      "Epoch: 278, Train loss: 3.532, Val loss: 3.609, Epoch time = 1.784s\n",
      "Epoch: 279, Train loss: 3.098, Val loss: 3.831, Epoch time = 1.906s\n",
      "Epoch: 280, Train loss: 3.405, Val loss: 3.735, Epoch time = 1.755s\n",
      "Epoch: 281, Train loss: 4.057, Val loss: 3.580, Epoch time = 1.812s\n",
      "Epoch: 282, Train loss: 3.580, Val loss: 3.514, Epoch time = 1.815s\n",
      "Epoch: 283, Train loss: 3.142, Val loss: 4.364, Epoch time = 1.872s\n",
      "Epoch: 284, Train loss: 4.029, Val loss: 3.804, Epoch time = 1.823s\n",
      "Epoch: 285, Train loss: 3.219, Val loss: 3.447, Epoch time = 1.834s\n",
      "Epoch: 286, Train loss: 3.441, Val loss: 4.228, Epoch time = 1.923s\n",
      "Epoch: 287, Train loss: 3.752, Val loss: 3.732, Epoch time = 1.787s\n",
      "Epoch: 288, Train loss: 3.900, Val loss: 3.465, Epoch time = 1.764s\n",
      "Epoch: 289, Train loss: 3.315, Val loss: 4.638, Epoch time = 1.906s\n",
      "Epoch: 290, Train loss: 3.256, Val loss: 3.006, Epoch time = 2.040s\n",
      "Epoch: 291, Train loss: 3.223, Val loss: 4.402, Epoch time = 1.935s\n",
      "Epoch: 292, Train loss: 3.360, Val loss: 3.372, Epoch time = 1.762s\n",
      "Epoch: 293, Train loss: 3.331, Val loss: 3.461, Epoch time = 1.758s\n",
      "Epoch: 294, Train loss: 3.114, Val loss: 3.237, Epoch time = 1.802s\n",
      "Epoch: 295, Train loss: 3.286, Val loss: 3.187, Epoch time = 1.784s\n",
      "Epoch: 296, Train loss: 3.700, Val loss: 3.452, Epoch time = 1.822s\n",
      "Epoch: 297, Train loss: 3.132, Val loss: 3.469, Epoch time = 1.818s\n",
      "Epoch: 298, Train loss: 3.602, Val loss: 3.476, Epoch time = 1.850s\n",
      "Epoch: 299, Train loss: 3.393, Val loss: 3.249, Epoch time = 1.748s\n",
      "Epoch: 300, Train loss: 2.882, Val loss: 3.535, Epoch time = 1.718s\n",
      "Epoch: 301, Train loss: 3.328, Val loss: 3.411, Epoch time = 1.822s\n",
      "Epoch: 302, Train loss: 3.325, Val loss: 3.521, Epoch time = 1.830s\n",
      "Epoch: 303, Train loss: 3.417, Val loss: 3.344, Epoch time = 1.721s\n",
      "Epoch: 304, Train loss: 3.068, Val loss: 3.312, Epoch time = 1.705s\n",
      "Epoch: 305, Train loss: 4.096, Val loss: 3.518, Epoch time = 1.709s\n",
      "Epoch: 306, Train loss: 3.556, Val loss: 4.096, Epoch time = 1.727s\n",
      "Epoch: 307, Train loss: 4.239, Val loss: 3.154, Epoch time = 1.703s\n",
      "Epoch: 308, Train loss: 3.327, Val loss: 4.085, Epoch time = 1.831s\n",
      "Epoch: 309, Train loss: 3.226, Val loss: 3.157, Epoch time = 1.761s\n",
      "Epoch: 310, Train loss: 3.838, Val loss: 3.276, Epoch time = 1.709s\n",
      "Epoch: 311, Train loss: 4.068, Val loss: 3.462, Epoch time = 1.770s\n",
      "Epoch: 312, Train loss: 3.404, Val loss: 3.648, Epoch time = 1.713s\n",
      "Epoch: 313, Train loss: 3.722, Val loss: 3.875, Epoch time = 1.731s\n",
      "Epoch: 314, Train loss: 3.559, Val loss: 3.459, Epoch time = 1.688s\n",
      "Epoch: 315, Train loss: 3.909, Val loss: 3.545, Epoch time = 1.859s\n",
      "Epoch: 316, Train loss: 3.134, Val loss: 3.240, Epoch time = 1.832s\n",
      "Epoch: 317, Train loss: 3.079, Val loss: 3.442, Epoch time = 1.825s\n",
      "Epoch: 318, Train loss: 2.968, Val loss: 4.411, Epoch time = 1.805s\n",
      "Epoch: 319, Train loss: 3.756, Val loss: 3.163, Epoch time = 1.873s\n",
      "Epoch: 320, Train loss: 3.201, Val loss: 3.478, Epoch time = 1.789s\n",
      "Epoch: 321, Train loss: 4.009, Val loss: 3.522, Epoch time = 2.047s\n",
      "Epoch: 322, Train loss: 2.804, Val loss: 3.222, Epoch time = 1.890s\n",
      "Epoch: 323, Train loss: 3.553, Val loss: 3.442, Epoch time = 1.874s\n",
      "Epoch: 324, Train loss: 3.974, Val loss: 3.018, Epoch time = 1.892s\n",
      "Epoch: 325, Train loss: 3.071, Val loss: 3.581, Epoch time = 1.687s\n",
      "Epoch: 326, Train loss: 3.574, Val loss: 4.313, Epoch time = 1.741s\n",
      "Epoch: 327, Train loss: 3.223, Val loss: 3.431, Epoch time = 1.755s\n",
      "Epoch: 328, Train loss: 3.626, Val loss: 3.599, Epoch time = 1.760s\n",
      "Epoch: 329, Train loss: 3.988, Val loss: 3.802, Epoch time = 1.892s\n",
      "Epoch: 330, Train loss: 3.381, Val loss: 3.384, Epoch time = 1.690s\n",
      "Epoch: 331, Train loss: 3.180, Val loss: 3.102, Epoch time = 1.782s\n",
      "Epoch: 332, Train loss: 3.233, Val loss: 3.131, Epoch time = 1.756s\n",
      "Epoch: 333, Train loss: 4.071, Val loss: 3.274, Epoch time = 1.726s\n",
      "Epoch: 334, Train loss: 3.058, Val loss: 3.414, Epoch time = 1.784s\n",
      "Epoch: 335, Train loss: 3.908, Val loss: 3.060, Epoch time = 1.801s\n",
      "Epoch: 336, Train loss: 3.564, Val loss: 3.140, Epoch time = 1.760s\n",
      "Epoch: 337, Train loss: 3.521, Val loss: 3.367, Epoch time = 1.859s\n",
      "Epoch: 338, Train loss: 3.490, Val loss: 3.219, Epoch time = 1.899s\n",
      "Epoch: 339, Train loss: 3.405, Val loss: 3.330, Epoch time = 1.913s\n",
      "Epoch: 340, Train loss: 3.634, Val loss: 3.600, Epoch time = 1.897s\n",
      "Epoch: 341, Train loss: 3.603, Val loss: 3.334, Epoch time = 1.898s\n",
      "Epoch: 342, Train loss: 3.264, Val loss: 3.139, Epoch time = 1.831s\n",
      "Epoch: 343, Train loss: 4.229, Val loss: 3.263, Epoch time = 1.738s\n",
      "Epoch: 344, Train loss: 3.252, Val loss: 3.324, Epoch time = 1.744s\n",
      "Epoch: 345, Train loss: 3.848, Val loss: 3.492, Epoch time = 1.713s\n",
      "Epoch: 346, Train loss: 3.212, Val loss: 3.420, Epoch time = 1.713s\n",
      "Epoch: 347, Train loss: 3.384, Val loss: 3.466, Epoch time = 1.749s\n",
      "Epoch: 348, Train loss: 3.117, Val loss: 3.865, Epoch time = 1.726s\n",
      "Epoch: 349, Train loss: 3.960, Val loss: 3.168, Epoch time = 1.771s\n",
      "Epoch: 350, Train loss: 4.086, Val loss: 3.220, Epoch time = 1.698s\n",
      "Epoch: 351, Train loss: 3.853, Val loss: 3.193, Epoch time = 1.674s\n",
      "Epoch: 352, Train loss: 3.145, Val loss: 3.339, Epoch time = 1.786s\n",
      "Epoch: 353, Train loss: 3.638, Val loss: 3.387, Epoch time = 1.761s\n",
      "Epoch: 354, Train loss: 3.947, Val loss: 2.829, Epoch time = 1.696s\n",
      "Epoch: 355, Train loss: 3.890, Val loss: 4.001, Epoch time = 1.950s\n",
      "Epoch: 356, Train loss: 3.933, Val loss: 3.459, Epoch time = 1.992s\n",
      "Epoch: 357, Train loss: 3.264, Val loss: 3.564, Epoch time = 1.803s\n",
      "Epoch: 358, Train loss: 3.929, Val loss: 3.290, Epoch time = 1.677s\n",
      "Epoch: 359, Train loss: 3.195, Val loss: 3.208, Epoch time = 1.734s\n",
      "Epoch: 360, Train loss: 3.097, Val loss: 3.261, Epoch time = 1.925s\n",
      "Epoch: 361, Train loss: 3.040, Val loss: 3.406, Epoch time = 1.873s\n",
      "Epoch: 362, Train loss: 3.358, Val loss: 4.087, Epoch time = 1.924s\n",
      "Epoch: 363, Train loss: 3.386, Val loss: 3.505, Epoch time = 1.889s\n",
      "Epoch: 364, Train loss: 3.842, Val loss: 3.396, Epoch time = 1.656s\n",
      "Epoch: 365, Train loss: 3.262, Val loss: 2.818, Epoch time = 1.726s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 366, Train loss: 3.936, Val loss: 3.414, Epoch time = 1.722s\n",
      "Epoch: 367, Train loss: 3.163, Val loss: 3.332, Epoch time = 1.827s\n",
      "Epoch: 368, Train loss: 3.903, Val loss: 3.637, Epoch time = 1.852s\n",
      "Epoch: 369, Train loss: 3.689, Val loss: 3.319, Epoch time = 1.758s\n",
      "Epoch: 370, Train loss: 3.107, Val loss: 3.087, Epoch time = 1.859s\n",
      "Epoch: 371, Train loss: 3.303, Val loss: 3.474, Epoch time = 1.721s\n",
      "Epoch: 372, Train loss: 3.814, Val loss: 3.121, Epoch time = 1.761s\n",
      "Epoch: 373, Train loss: 3.680, Val loss: 3.186, Epoch time = 1.913s\n",
      "Epoch: 374, Train loss: 3.056, Val loss: 3.354, Epoch time = 1.848s\n",
      "Epoch: 375, Train loss: 3.821, Val loss: 3.763, Epoch time = 1.909s\n",
      "Epoch: 376, Train loss: 3.801, Val loss: 3.622, Epoch time = 1.701s\n",
      "Epoch: 377, Train loss: 3.294, Val loss: 3.395, Epoch time = 1.929s\n",
      "Epoch: 378, Train loss: 3.418, Val loss: 3.938, Epoch time = 1.796s\n",
      "Epoch: 379, Train loss: 3.399, Val loss: 3.211, Epoch time = 1.737s\n",
      "Epoch: 380, Train loss: 3.730, Val loss: 3.466, Epoch time = 1.936s\n",
      "Epoch: 381, Train loss: 3.625, Val loss: 2.941, Epoch time = 1.903s\n",
      "Epoch: 382, Train loss: 3.384, Val loss: 3.174, Epoch time = 1.862s\n",
      "Epoch: 383, Train loss: 3.668, Val loss: 3.788, Epoch time = 1.883s\n",
      "Epoch: 384, Train loss: 3.019, Val loss: 3.268, Epoch time = 1.895s\n",
      "Epoch: 385, Train loss: 3.118, Val loss: 3.588, Epoch time = 1.868s\n",
      "Epoch: 386, Train loss: 3.199, Val loss: 3.473, Epoch time = 1.877s\n",
      "Epoch: 387, Train loss: 2.996, Val loss: 2.965, Epoch time = 1.757s\n",
      "Epoch: 388, Train loss: 3.703, Val loss: 3.219, Epoch time = 1.718s\n",
      "Epoch: 389, Train loss: 3.325, Val loss: 3.442, Epoch time = 1.857s\n",
      "Epoch: 390, Train loss: 3.767, Val loss: 3.590, Epoch time = 1.727s\n",
      "Epoch: 391, Train loss: 3.707, Val loss: 3.284, Epoch time = 1.767s\n",
      "Epoch: 392, Train loss: 3.104, Val loss: 3.399, Epoch time = 1.945s\n",
      "Epoch: 393, Train loss: 3.671, Val loss: 3.015, Epoch time = 2.100s\n",
      "Epoch: 394, Train loss: 3.396, Val loss: 3.009, Epoch time = 1.974s\n",
      "Epoch: 395, Train loss: 3.143, Val loss: 3.642, Epoch time = 1.866s\n",
      "Epoch: 396, Train loss: 3.270, Val loss: 3.418, Epoch time = 1.834s\n",
      "Epoch: 397, Train loss: 3.317, Val loss: 3.493, Epoch time = 1.723s\n",
      "Epoch: 398, Train loss: 3.027, Val loss: 3.448, Epoch time = 1.778s\n",
      "Epoch: 399, Train loss: 2.977, Val loss: 3.068, Epoch time = 1.861s\n",
      "Epoch: 400, Train loss: 3.580, Val loss: 3.059, Epoch time = 1.762s\n",
      "Epoch: 401, Train loss: 3.041, Val loss: 3.185, Epoch time = 1.792s\n",
      "Epoch: 402, Train loss: 3.464, Val loss: 4.032, Epoch time = 1.877s\n",
      "Epoch: 403, Train loss: 3.385, Val loss: 3.054, Epoch time = 1.864s\n",
      "Epoch: 404, Train loss: 3.297, Val loss: 3.360, Epoch time = 1.898s\n",
      "Epoch: 405, Train loss: 3.201, Val loss: 3.320, Epoch time = 2.076s\n",
      "Epoch: 406, Train loss: 3.316, Val loss: 3.211, Epoch time = 1.957s\n",
      "Epoch: 407, Train loss: 3.672, Val loss: 3.032, Epoch time = 1.715s\n",
      "Epoch: 408, Train loss: 3.438, Val loss: 3.547, Epoch time = 1.733s\n",
      "Epoch: 409, Train loss: 2.972, Val loss: 3.805, Epoch time = 1.769s\n",
      "Epoch: 410, Train loss: 3.297, Val loss: 3.595, Epoch time = 1.768s\n",
      "Epoch: 411, Train loss: 3.462, Val loss: 3.302, Epoch time = 1.715s\n",
      "Epoch: 412, Train loss: 3.721, Val loss: 3.748, Epoch time = 1.755s\n",
      "Epoch: 413, Train loss: 3.165, Val loss: 3.289, Epoch time = 1.719s\n",
      "Epoch: 414, Train loss: 3.862, Val loss: 3.752, Epoch time = 1.832s\n",
      "Epoch: 415, Train loss: 3.209, Val loss: 3.168, Epoch time = 1.735s\n",
      "Epoch: 416, Train loss: 3.969, Val loss: 3.041, Epoch time = 1.764s\n",
      "Epoch: 417, Train loss: 3.374, Val loss: 3.030, Epoch time = 1.787s\n",
      "Epoch: 418, Train loss: 3.458, Val loss: 3.283, Epoch time = 1.790s\n",
      "Epoch: 419, Train loss: 2.948, Val loss: 4.124, Epoch time = 1.836s\n",
      "Epoch: 420, Train loss: 3.053, Val loss: 3.599, Epoch time = 1.939s\n",
      "Epoch: 421, Train loss: 3.255, Val loss: 3.096, Epoch time = 1.811s\n",
      "Epoch: 422, Train loss: 3.578, Val loss: 3.281, Epoch time = 1.820s\n",
      "Epoch: 423, Train loss: 3.264, Val loss: 2.939, Epoch time = 1.990s\n",
      "Epoch: 424, Train loss: 3.268, Val loss: 3.590, Epoch time = 1.991s\n",
      "Epoch: 425, Train loss: 3.216, Val loss: 3.804, Epoch time = 1.779s\n",
      "Epoch: 426, Train loss: 3.204, Val loss: 3.526, Epoch time = 1.982s\n",
      "Epoch: 427, Train loss: 3.019, Val loss: 2.966, Epoch time = 1.865s\n",
      "Epoch: 428, Train loss: 3.053, Val loss: 3.014, Epoch time = 2.213s\n",
      "Epoch: 429, Train loss: 3.310, Val loss: 3.528, Epoch time = 2.009s\n",
      "Epoch: 430, Train loss: 3.950, Val loss: 3.637, Epoch time = 1.955s\n",
      "Epoch: 431, Train loss: 4.096, Val loss: 3.136, Epoch time = 1.860s\n",
      "Epoch: 432, Train loss: 3.814, Val loss: 3.048, Epoch time = 1.890s\n",
      "Epoch: 433, Train loss: 3.269, Val loss: 3.076, Epoch time = 1.890s\n",
      "Epoch: 434, Train loss: 3.682, Val loss: 3.168, Epoch time = 1.852s\n",
      "Epoch: 435, Train loss: 3.521, Val loss: 3.353, Epoch time = 1.697s\n",
      "Epoch: 436, Train loss: 3.725, Val loss: 3.364, Epoch time = 1.710s\n",
      "Epoch: 437, Train loss: 3.131, Val loss: 3.166, Epoch time = 1.921s\n",
      "Epoch: 438, Train loss: 3.434, Val loss: 3.951, Epoch time = 1.852s\n",
      "Epoch: 439, Train loss: 3.394, Val loss: 3.231, Epoch time = 1.817s\n",
      "Epoch: 440, Train loss: 2.990, Val loss: 3.082, Epoch time = 1.845s\n",
      "Epoch: 441, Train loss: 3.688, Val loss: 3.591, Epoch time = 1.937s\n",
      "Epoch: 442, Train loss: 3.868, Val loss: 3.547, Epoch time = 1.689s\n",
      "Epoch: 443, Train loss: 3.882, Val loss: 3.461, Epoch time = 1.793s\n",
      "Epoch: 444, Train loss: 3.333, Val loss: 3.052, Epoch time = 1.933s\n",
      "Epoch: 445, Train loss: 2.821, Val loss: 3.200, Epoch time = 1.918s\n",
      "Epoch: 446, Train loss: 3.479, Val loss: 3.271, Epoch time = 1.962s\n",
      "Epoch: 447, Train loss: 3.387, Val loss: 3.006, Epoch time = 1.773s\n",
      "Epoch: 448, Train loss: 3.474, Val loss: 3.539, Epoch time = 1.941s\n",
      "Epoch: 449, Train loss: 3.035, Val loss: 3.184, Epoch time = 1.792s\n",
      "Epoch: 450, Train loss: 3.454, Val loss: 3.107, Epoch time = 1.740s\n",
      "Epoch: 451, Train loss: 3.630, Val loss: 3.012, Epoch time = 1.789s\n",
      "Epoch: 452, Train loss: 3.415, Val loss: 2.911, Epoch time = 1.835s\n",
      "Epoch: 453, Train loss: 3.475, Val loss: 3.719, Epoch time = 1.762s\n",
      "Epoch: 454, Train loss: 3.460, Val loss: 3.129, Epoch time = 1.774s\n",
      "Epoch: 455, Train loss: 3.563, Val loss: 3.179, Epoch time = 1.713s\n",
      "Epoch: 456, Train loss: 3.412, Val loss: 2.877, Epoch time = 1.928s\n",
      "Epoch: 457, Train loss: 3.559, Val loss: 3.449, Epoch time = 1.849s\n",
      "Epoch: 458, Train loss: 2.969, Val loss: 3.424, Epoch time = 2.054s\n",
      "Epoch: 459, Train loss: 3.256, Val loss: 3.338, Epoch time = 1.830s\n",
      "Epoch: 460, Train loss: 3.530, Val loss: 3.044, Epoch time = 1.842s\n",
      "Epoch: 461, Train loss: 3.147, Val loss: 3.534, Epoch time = 2.014s\n",
      "Epoch: 462, Train loss: 3.216, Val loss: 3.355, Epoch time = 1.888s\n",
      "Epoch: 463, Train loss: 3.143, Val loss: 3.169, Epoch time = 1.892s\n",
      "Epoch: 464, Train loss: 3.106, Val loss: 3.365, Epoch time = 1.867s\n",
      "Epoch: 465, Train loss: 3.342, Val loss: 3.284, Epoch time = 1.805s\n",
      "Epoch: 466, Train loss: 3.165, Val loss: 3.375, Epoch time = 1.833s\n",
      "Epoch: 467, Train loss: 3.605, Val loss: 3.312, Epoch time = 2.032s\n",
      "Epoch: 468, Train loss: 3.490, Val loss: 3.077, Epoch time = 1.951s\n",
      "Epoch: 469, Train loss: 3.328, Val loss: 3.232, Epoch time = 1.884s\n",
      "Epoch: 470, Train loss: 3.334, Val loss: 3.044, Epoch time = 1.741s\n",
      "Epoch: 471, Train loss: 2.985, Val loss: 3.285, Epoch time = 1.829s\n",
      "Epoch: 472, Train loss: 3.488, Val loss: 3.939, Epoch time = 1.765s\n",
      "Epoch: 473, Train loss: 3.128, Val loss: 2.985, Epoch time = 1.806s\n",
      "Epoch: 474, Train loss: 3.494, Val loss: 3.980, Epoch time = 1.767s\n",
      "Epoch: 475, Train loss: 3.484, Val loss: 3.190, Epoch time = 1.919s\n",
      "Epoch: 476, Train loss: 3.008, Val loss: 3.868, Epoch time = 1.952s\n",
      "Epoch: 477, Train loss: 3.338, Val loss: 3.362, Epoch time = 1.970s\n",
      "Epoch: 478, Train loss: 4.370, Val loss: 3.399, Epoch time = 1.893s\n",
      "Epoch: 479, Train loss: 3.370, Val loss: 3.473, Epoch time = 1.753s\n",
      "Epoch: 480, Train loss: 3.473, Val loss: 2.909, Epoch time = 1.950s\n",
      "Epoch: 481, Train loss: 2.919, Val loss: 3.605, Epoch time = 1.792s\n",
      "Epoch: 482, Train loss: 3.080, Val loss: 3.031, Epoch time = 1.796s\n",
      "Epoch: 483, Train loss: 3.005, Val loss: 3.359, Epoch time = 1.851s\n",
      "Epoch: 484, Train loss: 3.105, Val loss: 3.843, Epoch time = 1.794s\n",
      "Epoch: 485, Train loss: 3.161, Val loss: 3.538, Epoch time = 1.745s\n",
      "Epoch: 486, Train loss: 3.074, Val loss: 3.840, Epoch time = 1.769s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 487, Train loss: 2.960, Val loss: 3.001, Epoch time = 1.751s\n",
      "Epoch: 488, Train loss: 3.324, Val loss: 3.340, Epoch time = 1.783s\n",
      "Epoch: 489, Train loss: 3.945, Val loss: 3.096, Epoch time = 1.831s\n",
      "Epoch: 490, Train loss: 3.760, Val loss: 4.070, Epoch time = 1.845s\n",
      "Epoch: 491, Train loss: 3.904, Val loss: 3.352, Epoch time = 1.808s\n",
      "Epoch: 492, Train loss: 3.398, Val loss: 4.156, Epoch time = 1.909s\n",
      "Epoch: 493, Train loss: 3.784, Val loss: 3.239, Epoch time = 1.958s\n",
      "Epoch: 494, Train loss: 3.325, Val loss: 3.894, Epoch time = 1.983s\n",
      "Epoch: 495, Train loss: 3.187, Val loss: 2.991, Epoch time = 1.757s\n",
      "Epoch: 496, Train loss: 3.115, Val loss: 3.282, Epoch time = 1.925s\n",
      "Epoch: 497, Train loss: 3.410, Val loss: 4.087, Epoch time = 1.968s\n",
      "Epoch: 498, Train loss: 3.430, Val loss: 3.012, Epoch time = 1.895s\n",
      "Epoch: 499, Train loss: 3.244, Val loss: 3.410, Epoch time = 1.712s\n",
      "Epoch: 500, Train loss: 3.345, Val loss: 3.131, Epoch time = 1.819s\n",
      "Epoch: 501, Train loss: 3.418, Val loss: 3.134, Epoch time = 1.777s\n",
      "Epoch: 502, Train loss: 2.865, Val loss: 3.381, Epoch time = 1.836s\n",
      "Epoch: 503, Train loss: 3.092, Val loss: 3.239, Epoch time = 1.736s\n",
      "Epoch: 504, Train loss: 4.008, Val loss: 3.135, Epoch time = 1.799s\n",
      "Epoch: 505, Train loss: 3.728, Val loss: 3.042, Epoch time = 1.728s\n",
      "Epoch: 506, Train loss: 2.991, Val loss: 3.149, Epoch time = 1.763s\n",
      "Epoch: 507, Train loss: 3.466, Val loss: 3.496, Epoch time = 1.838s\n",
      "Epoch: 508, Train loss: 3.988, Val loss: 3.680, Epoch time = 2.003s\n",
      "Epoch: 509, Train loss: 3.035, Val loss: 3.193, Epoch time = 2.012s\n",
      "Epoch: 510, Train loss: 3.092, Val loss: 2.915, Epoch time = 1.833s\n",
      "Epoch: 511, Train loss: 3.218, Val loss: 3.583, Epoch time = 1.933s\n",
      "Epoch: 512, Train loss: 3.339, Val loss: 3.239, Epoch time = 1.877s\n",
      "Epoch: 513, Train loss: 2.921, Val loss: 3.147, Epoch time = 1.955s\n",
      "Epoch: 514, Train loss: 3.209, Val loss: 3.770, Epoch time = 1.787s\n",
      "Epoch: 515, Train loss: 3.294, Val loss: 3.909, Epoch time = 1.932s\n",
      "Epoch: 516, Train loss: 4.087, Val loss: 3.150, Epoch time = 1.824s\n",
      "Epoch: 517, Train loss: 3.004, Val loss: 3.546, Epoch time = 1.751s\n",
      "Epoch: 518, Train loss: 3.651, Val loss: 3.519, Epoch time = 1.818s\n",
      "Epoch: 519, Train loss: 3.535, Val loss: 3.002, Epoch time = 1.869s\n",
      "Epoch: 520, Train loss: 3.065, Val loss: 3.168, Epoch time = 1.886s\n",
      "Epoch: 521, Train loss: 3.025, Val loss: 3.513, Epoch time = 1.971s\n",
      "Epoch: 522, Train loss: 3.373, Val loss: 3.740, Epoch time = 1.850s\n",
      "Epoch: 523, Train loss: 3.513, Val loss: 3.224, Epoch time = 1.992s\n",
      "Epoch: 524, Train loss: 3.471, Val loss: 3.295, Epoch time = 2.079s\n",
      "Epoch: 525, Train loss: 3.500, Val loss: 3.558, Epoch time = 1.858s\n",
      "Epoch: 526, Train loss: 3.314, Val loss: 3.390, Epoch time = 1.782s\n",
      "Epoch: 527, Train loss: 3.369, Val loss: 3.265, Epoch time = 1.731s\n",
      "Epoch: 528, Train loss: 3.242, Val loss: 3.358, Epoch time = 1.753s\n",
      "Epoch: 529, Train loss: 4.078, Val loss: 3.008, Epoch time = 1.748s\n",
      "Epoch: 530, Train loss: 3.201, Val loss: 3.175, Epoch time = 1.758s\n",
      "Epoch: 531, Train loss: 2.560, Val loss: 3.082, Epoch time = 1.880s\n",
      "Epoch: 532, Train loss: 3.088, Val loss: 3.562, Epoch time = 1.951s\n",
      "Epoch: 533, Train loss: 3.125, Val loss: 3.249, Epoch time = 1.965s\n",
      "Epoch: 534, Train loss: 3.312, Val loss: 3.514, Epoch time = 1.989s\n",
      "Epoch: 535, Train loss: 3.441, Val loss: 4.155, Epoch time = 1.946s\n",
      "Epoch: 536, Train loss: 3.117, Val loss: 3.438, Epoch time = 1.912s\n",
      "Epoch: 537, Train loss: 3.273, Val loss: 3.149, Epoch time = 1.916s\n",
      "Epoch: 538, Train loss: 3.234, Val loss: 3.181, Epoch time = 1.922s\n",
      "Epoch: 539, Train loss: 3.397, Val loss: 3.299, Epoch time = 2.027s\n",
      "Epoch: 540, Train loss: 2.957, Val loss: 3.412, Epoch time = 1.978s\n",
      "Epoch: 541, Train loss: 3.071, Val loss: 3.392, Epoch time = 1.892s\n",
      "Epoch: 542, Train loss: 3.331, Val loss: 3.119, Epoch time = 1.846s\n",
      "Epoch: 543, Train loss: 3.634, Val loss: 3.138, Epoch time = 1.915s\n",
      "Epoch: 544, Train loss: 3.477, Val loss: 3.152, Epoch time = 1.995s\n",
      "Epoch: 545, Train loss: 3.219, Val loss: 3.083, Epoch time = 1.958s\n",
      "Epoch: 546, Train loss: 3.171, Val loss: 3.698, Epoch time = 2.002s\n",
      "Epoch: 547, Train loss: 3.406, Val loss: 3.166, Epoch time = 1.855s\n",
      "Epoch: 548, Train loss: 3.458, Val loss: 3.188, Epoch time = 1.850s\n",
      "Epoch: 549, Train loss: 3.241, Val loss: 3.642, Epoch time = 1.780s\n",
      "Epoch: 550, Train loss: 3.809, Val loss: 3.101, Epoch time = 2.176s\n",
      "Epoch: 551, Train loss: 3.510, Val loss: 3.134, Epoch time = 1.988s\n",
      "Epoch: 552, Train loss: 3.070, Val loss: 3.462, Epoch time = 1.922s\n",
      "Epoch: 553, Train loss: 3.311, Val loss: 3.214, Epoch time = 1.828s\n",
      "Epoch: 554, Train loss: 3.361, Val loss: 3.093, Epoch time = 1.900s\n",
      "Epoch: 555, Train loss: 3.140, Val loss: 3.308, Epoch time = 2.014s\n",
      "Epoch: 556, Train loss: 3.098, Val loss: 3.335, Epoch time = 1.920s\n",
      "Epoch: 557, Train loss: 3.060, Val loss: 3.588, Epoch time = 1.978s\n",
      "Epoch: 558, Train loss: 3.075, Val loss: 2.813, Epoch time = 1.938s\n",
      "Epoch: 559, Train loss: 2.957, Val loss: 3.215, Epoch time = 1.821s\n",
      "Epoch: 560, Train loss: 3.116, Val loss: 3.315, Epoch time = 1.760s\n",
      "Epoch: 561, Train loss: 3.478, Val loss: 3.163, Epoch time = 1.778s\n",
      "Epoch: 562, Train loss: 3.053, Val loss: 3.021, Epoch time = 1.777s\n",
      "Epoch: 563, Train loss: 3.531, Val loss: 2.955, Epoch time = 1.908s\n",
      "Epoch: 564, Train loss: 2.849, Val loss: 3.182, Epoch time = 2.042s\n",
      "Epoch: 565, Train loss: 2.848, Val loss: 2.985, Epoch time = 1.918s\n",
      "Epoch: 566, Train loss: 3.559, Val loss: 2.995, Epoch time = 1.920s\n",
      "Epoch: 567, Train loss: 3.487, Val loss: 3.076, Epoch time = 1.811s\n",
      "Epoch: 568, Train loss: 3.295, Val loss: 3.165, Epoch time = 2.050s\n",
      "Epoch: 569, Train loss: 3.690, Val loss: 3.056, Epoch time = 1.888s\n",
      "Epoch: 570, Train loss: 3.226, Val loss: 3.449, Epoch time = 1.811s\n",
      "Epoch: 571, Train loss: 2.996, Val loss: 4.120, Epoch time = 1.954s\n",
      "Epoch: 572, Train loss: 3.878, Val loss: 3.731, Epoch time = 1.805s\n",
      "Epoch: 573, Train loss: 3.384, Val loss: 3.305, Epoch time = 1.873s\n",
      "Epoch: 574, Train loss: 3.299, Val loss: 3.146, Epoch time = 1.842s\n",
      "Epoch: 575, Train loss: 3.916, Val loss: 3.313, Epoch time = 1.810s\n",
      "Epoch: 576, Train loss: 4.065, Val loss: 3.276, Epoch time = 1.791s\n",
      "Epoch: 577, Train loss: 3.518, Val loss: 2.998, Epoch time = 1.901s\n",
      "Epoch: 578, Train loss: 4.126, Val loss: 3.320, Epoch time = 2.019s\n",
      "Epoch: 579, Train loss: 3.205, Val loss: 3.448, Epoch time = 1.845s\n",
      "Epoch: 580, Train loss: 3.077, Val loss: 3.134, Epoch time = 1.781s\n",
      "Epoch: 581, Train loss: 2.941, Val loss: 3.075, Epoch time = 1.785s\n",
      "Epoch: 582, Train loss: 3.021, Val loss: 3.463, Epoch time = 1.754s\n",
      "Epoch: 583, Train loss: 3.744, Val loss: 3.364, Epoch time = 1.768s\n",
      "Epoch: 584, Train loss: 3.424, Val loss: 3.559, Epoch time = 1.987s\n",
      "Epoch: 585, Train loss: 3.670, Val loss: 3.757, Epoch time = 1.810s\n",
      "Epoch: 586, Train loss: 3.161, Val loss: 3.566, Epoch time = 1.877s\n",
      "Epoch: 587, Train loss: 3.654, Val loss: 2.987, Epoch time = 1.759s\n",
      "Epoch: 588, Train loss: 3.255, Val loss: 3.487, Epoch time = 1.816s\n",
      "Epoch: 589, Train loss: 3.593, Val loss: 3.365, Epoch time = 1.868s\n",
      "Epoch: 590, Train loss: 3.684, Val loss: 3.602, Epoch time = 1.880s\n",
      "Epoch: 591, Train loss: 3.219, Val loss: 3.474, Epoch time = 2.057s\n",
      "Epoch: 592, Train loss: 2.884, Val loss: 3.316, Epoch time = 2.221s\n",
      "Epoch: 593, Train loss: 3.078, Val loss: 3.206, Epoch time = 1.770s\n",
      "Epoch: 594, Train loss: 2.980, Val loss: 2.972, Epoch time = 1.837s\n",
      "Epoch: 595, Train loss: 3.103, Val loss: 3.131, Epoch time = 1.849s\n",
      "Epoch: 596, Train loss: 3.046, Val loss: 3.671, Epoch time = 1.731s\n",
      "Epoch: 597, Train loss: 3.260, Val loss: 3.430, Epoch time = 1.916s\n",
      "Epoch: 598, Train loss: 2.845, Val loss: 3.071, Epoch time = 1.894s\n",
      "Epoch: 599, Train loss: 3.349, Val loss: 3.705, Epoch time = 1.914s\n",
      "Epoch: 600, Train loss: 3.377, Val loss: 2.953, Epoch time = 2.042s\n",
      "Epoch: 601, Train loss: 3.011, Val loss: 3.149, Epoch time = 1.985s\n",
      "Epoch: 602, Train loss: 3.192, Val loss: 3.484, Epoch time = 1.792s\n",
      "Epoch: 603, Train loss: 3.058, Val loss: 3.610, Epoch time = 1.884s\n",
      "Epoch: 604, Train loss: 3.372, Val loss: 3.190, Epoch time = 1.740s\n",
      "Epoch: 605, Train loss: 2.792, Val loss: 3.715, Epoch time = 1.819s\n",
      "Epoch: 606, Train loss: 4.036, Val loss: 2.891, Epoch time = 1.723s\n",
      "Epoch: 607, Train loss: 3.110, Val loss: 3.369, Epoch time = 1.744s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 608, Train loss: 3.361, Val loss: 3.212, Epoch time = 1.760s\n",
      "Epoch: 609, Train loss: 2.930, Val loss: 4.024, Epoch time = 1.819s\n",
      "Epoch: 610, Train loss: 2.986, Val loss: 3.987, Epoch time = 1.881s\n",
      "Epoch: 611, Train loss: 3.855, Val loss: 3.331, Epoch time = 1.784s\n",
      "Epoch: 612, Train loss: 3.478, Val loss: 3.298, Epoch time = 1.872s\n",
      "Epoch: 613, Train loss: 3.173, Val loss: 3.206, Epoch time = 2.055s\n",
      "Epoch: 614, Train loss: 3.559, Val loss: 3.133, Epoch time = 1.822s\n",
      "Epoch: 615, Train loss: 2.853, Val loss: 3.154, Epoch time = 2.140s\n",
      "Epoch: 616, Train loss: 2.779, Val loss: 3.677, Epoch time = 2.232s\n",
      "Epoch: 617, Train loss: 3.227, Val loss: 3.455, Epoch time = 2.145s\n",
      "Epoch: 618, Train loss: 3.080, Val loss: 3.545, Epoch time = 1.934s\n",
      "Epoch: 619, Train loss: 3.402, Val loss: 3.002, Epoch time = 1.756s\n",
      "Epoch: 620, Train loss: 3.120, Val loss: 3.931, Epoch time = 1.790s\n",
      "Epoch: 621, Train loss: 3.659, Val loss: 3.454, Epoch time = 1.845s\n",
      "Epoch: 622, Train loss: 3.269, Val loss: 3.060, Epoch time = 1.754s\n",
      "Epoch: 623, Train loss: 3.003, Val loss: 3.836, Epoch time = 1.847s\n",
      "Epoch: 624, Train loss: 3.635, Val loss: 2.907, Epoch time = 1.787s\n",
      "Epoch: 625, Train loss: 2.946, Val loss: 3.556, Epoch time = 1.797s\n",
      "Epoch: 626, Train loss: 3.653, Val loss: 3.928, Epoch time = 1.724s\n",
      "Epoch: 627, Train loss: 3.408, Val loss: 3.436, Epoch time = 1.754s\n",
      "Epoch: 628, Train loss: 3.388, Val loss: 3.001, Epoch time = 1.805s\n",
      "Epoch: 629, Train loss: 3.227, Val loss: 3.699, Epoch time = 1.776s\n",
      "Epoch: 630, Train loss: 3.077, Val loss: 3.107, Epoch time = 1.940s\n",
      "Epoch: 631, Train loss: 3.059, Val loss: 2.961, Epoch time = 1.914s\n",
      "Epoch: 632, Train loss: 2.888, Val loss: 4.080, Epoch time = 1.873s\n",
      "Epoch: 633, Train loss: 3.280, Val loss: 3.136, Epoch time = 1.910s\n",
      "Epoch: 634, Train loss: 3.723, Val loss: 3.085, Epoch time = 1.878s\n",
      "Epoch: 635, Train loss: 3.403, Val loss: 3.206, Epoch time = 1.754s\n",
      "Epoch: 636, Train loss: 3.249, Val loss: 3.010, Epoch time = 1.847s\n",
      "Epoch: 637, Train loss: 3.533, Val loss: 2.930, Epoch time = 1.889s\n",
      "Epoch: 638, Train loss: 3.071, Val loss: 2.837, Epoch time = 1.948s\n",
      "Epoch: 639, Train loss: 3.227, Val loss: 2.850, Epoch time = 1.975s\n",
      "Epoch: 640, Train loss: 3.262, Val loss: 2.965, Epoch time = 1.968s\n",
      "Epoch: 641, Train loss: 2.798, Val loss: 2.984, Epoch time = 1.967s\n",
      "Epoch: 642, Train loss: 3.380, Val loss: 3.123, Epoch time = 1.858s\n",
      "Epoch: 643, Train loss: 3.277, Val loss: 3.388, Epoch time = 1.726s\n",
      "Epoch: 644, Train loss: 3.311, Val loss: 3.603, Epoch time = 1.911s\n",
      "Epoch: 645, Train loss: 2.896, Val loss: 3.101, Epoch time = 1.894s\n",
      "Epoch: 646, Train loss: 3.069, Val loss: 3.268, Epoch time = 1.941s\n",
      "Epoch: 647, Train loss: 3.422, Val loss: 3.151, Epoch time = 2.084s\n",
      "Epoch: 648, Train loss: 2.903, Val loss: 3.257, Epoch time = 1.896s\n",
      "Epoch: 649, Train loss: 3.012, Val loss: 3.147, Epoch time = 1.888s\n",
      "Epoch: 650, Train loss: 2.895, Val loss: 2.776, Epoch time = 1.920s\n",
      "Epoch: 651, Train loss: 3.236, Val loss: 3.103, Epoch time = 1.959s\n",
      "Epoch: 652, Train loss: 3.231, Val loss: 3.604, Epoch time = 1.862s\n",
      "Epoch: 653, Train loss: 3.329, Val loss: 3.635, Epoch time = 1.841s\n",
      "Epoch: 654, Train loss: 3.568, Val loss: 2.882, Epoch time = 1.668s\n",
      "Epoch: 655, Train loss: 3.168, Val loss: 3.064, Epoch time = 2.143s\n",
      "Epoch: 656, Train loss: 3.267, Val loss: 3.205, Epoch time = 2.062s\n",
      "Epoch: 657, Train loss: 2.980, Val loss: 3.797, Epoch time = 1.961s\n",
      "Epoch: 658, Train loss: 3.418, Val loss: 3.810, Epoch time = 1.748s\n",
      "Epoch: 659, Train loss: 3.021, Val loss: 3.898, Epoch time = 1.954s\n",
      "Epoch: 660, Train loss: 2.851, Val loss: 2.904, Epoch time = 1.862s\n",
      "Epoch: 661, Train loss: 3.223, Val loss: 3.388, Epoch time = 1.949s\n",
      "Epoch: 662, Train loss: 3.489, Val loss: 3.598, Epoch time = 1.796s\n",
      "Epoch: 663, Train loss: 3.244, Val loss: 3.868, Epoch time = 1.770s\n",
      "Epoch: 664, Train loss: 3.211, Val loss: 3.079, Epoch time = 1.803s\n",
      "Epoch: 665, Train loss: 2.864, Val loss: 3.510, Epoch time = 1.788s\n",
      "Epoch: 666, Train loss: 2.993, Val loss: 3.208, Epoch time = 1.962s\n",
      "Epoch: 667, Train loss: 3.004, Val loss: 3.223, Epoch time = 1.912s\n",
      "Epoch: 668, Train loss: 3.577, Val loss: 3.622, Epoch time = 1.805s\n",
      "Epoch: 669, Train loss: 2.968, Val loss: 3.178, Epoch time = 1.912s\n",
      "Epoch: 670, Train loss: 3.207, Val loss: 3.080, Epoch time = 1.857s\n",
      "Epoch: 671, Train loss: 2.812, Val loss: 3.030, Epoch time = 1.805s\n",
      "Epoch: 672, Train loss: 3.044, Val loss: 3.159, Epoch time = 2.083s\n",
      "Epoch: 673, Train loss: 3.608, Val loss: 3.103, Epoch time = 1.896s\n",
      "Epoch: 674, Train loss: 3.278, Val loss: 3.372, Epoch time = 2.009s\n",
      "Epoch: 675, Train loss: 3.709, Val loss: 3.073, Epoch time = 1.904s\n",
      "Epoch: 676, Train loss: 3.143, Val loss: 3.178, Epoch time = 1.966s\n",
      "Epoch: 677, Train loss: 2.990, Val loss: 2.978, Epoch time = 1.868s\n",
      "Epoch: 678, Train loss: 3.391, Val loss: 3.674, Epoch time = 1.892s\n",
      "Epoch: 679, Train loss: 3.135, Val loss: 3.706, Epoch time = 1.994s\n",
      "Epoch: 680, Train loss: 2.862, Val loss: 3.669, Epoch time = 1.973s\n",
      "Epoch: 681, Train loss: 3.095, Val loss: 3.481, Epoch time = 1.940s\n",
      "Epoch: 682, Train loss: 3.504, Val loss: 3.024, Epoch time = 1.967s\n",
      "Epoch: 683, Train loss: 3.492, Val loss: 3.082, Epoch time = 1.884s\n",
      "Epoch: 684, Train loss: 3.079, Val loss: 2.995, Epoch time = 2.027s\n",
      "Epoch: 685, Train loss: 2.876, Val loss: 3.015, Epoch time = 2.137s\n",
      "Epoch: 686, Train loss: 3.201, Val loss: 3.483, Epoch time = 2.120s\n",
      "Epoch: 687, Train loss: 3.522, Val loss: 3.547, Epoch time = 2.307s\n",
      "Epoch: 688, Train loss: 2.928, Val loss: 3.487, Epoch time = 2.026s\n",
      "Epoch: 689, Train loss: 3.045, Val loss: 3.760, Epoch time = 2.022s\n",
      "Epoch: 690, Train loss: 3.988, Val loss: 3.170, Epoch time = 2.043s\n",
      "Epoch: 691, Train loss: 3.325, Val loss: 3.280, Epoch time = 2.014s\n",
      "Epoch: 692, Train loss: 3.474, Val loss: 3.990, Epoch time = 1.892s\n",
      "Epoch: 693, Train loss: 3.168, Val loss: 3.596, Epoch time = 1.889s\n",
      "Epoch: 694, Train loss: 3.777, Val loss: 3.793, Epoch time = 1.851s\n",
      "Epoch: 695, Train loss: 2.778, Val loss: 3.834, Epoch time = 1.976s\n",
      "Epoch: 696, Train loss: 3.061, Val loss: 3.091, Epoch time = 1.947s\n",
      "Epoch: 697, Train loss: 2.896, Val loss: 3.157, Epoch time = 1.910s\n",
      "Epoch: 698, Train loss: 3.456, Val loss: 3.511, Epoch time = 1.916s\n",
      "Epoch: 699, Train loss: 3.200, Val loss: 3.188, Epoch time = 2.346s\n",
      "Epoch: 700, Train loss: 2.964, Val loss: 3.356, Epoch time = 2.039s\n",
      "Epoch: 701, Train loss: 3.402, Val loss: 3.182, Epoch time = 2.000s\n",
      "Epoch: 702, Train loss: 3.413, Val loss: 3.443, Epoch time = 1.990s\n",
      "Epoch: 703, Train loss: 2.759, Val loss: 3.467, Epoch time = 1.981s\n",
      "Epoch: 704, Train loss: 3.531, Val loss: 3.355, Epoch time = 2.138s\n",
      "Epoch: 705, Train loss: 3.037, Val loss: 2.959, Epoch time = 1.927s\n",
      "Epoch: 706, Train loss: 3.216, Val loss: 3.110, Epoch time = 1.892s\n",
      "Epoch: 707, Train loss: 3.794, Val loss: 3.151, Epoch time = 2.029s\n",
      "Epoch: 708, Train loss: 2.987, Val loss: 2.951, Epoch time = 1.905s\n",
      "Epoch: 709, Train loss: 3.549, Val loss: 3.395, Epoch time = 1.991s\n",
      "Epoch: 710, Train loss: 3.059, Val loss: 3.290, Epoch time = 2.011s\n",
      "Epoch: 711, Train loss: 3.673, Val loss: 3.175, Epoch time = 2.102s\n",
      "Epoch: 712, Train loss: 3.017, Val loss: 3.075, Epoch time = 1.829s\n",
      "Epoch: 713, Train loss: 3.149, Val loss: 3.182, Epoch time = 2.072s\n",
      "Epoch: 714, Train loss: 3.053, Val loss: 2.962, Epoch time = 1.887s\n",
      "Epoch: 715, Train loss: 3.531, Val loss: 2.775, Epoch time = 1.916s\n",
      "Epoch: 716, Train loss: 3.357, Val loss: 2.868, Epoch time = 1.991s\n",
      "Epoch: 717, Train loss: 3.172, Val loss: 3.420, Epoch time = 2.075s\n",
      "Epoch: 718, Train loss: 3.752, Val loss: 3.072, Epoch time = 2.056s\n",
      "Epoch: 719, Train loss: 3.775, Val loss: 3.486, Epoch time = 2.102s\n",
      "Epoch: 720, Train loss: 3.012, Val loss: 3.513, Epoch time = 2.018s\n",
      "Epoch: 721, Train loss: 3.069, Val loss: 3.259, Epoch time = 2.026s\n",
      "Epoch: 722, Train loss: 2.902, Val loss: 3.182, Epoch time = 2.130s\n",
      "Epoch: 723, Train loss: 3.617, Val loss: 2.746, Epoch time = 2.212s\n",
      "Epoch: 724, Train loss: 3.532, Val loss: 3.367, Epoch time = 1.923s\n",
      "Epoch: 725, Train loss: 3.363, Val loss: 2.863, Epoch time = 1.979s\n",
      "Epoch: 726, Train loss: 3.307, Val loss: 3.121, Epoch time = 2.033s\n",
      "Epoch: 727, Train loss: 2.850, Val loss: 3.139, Epoch time = 1.932s\n",
      "Epoch: 728, Train loss: 2.947, Val loss: 3.128, Epoch time = 2.015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 729, Train loss: 3.083, Val loss: 2.943, Epoch time = 2.063s\n",
      "Epoch: 730, Train loss: 3.148, Val loss: 2.686, Epoch time = 1.930s\n",
      "Epoch: 731, Train loss: 2.874, Val loss: 2.915, Epoch time = 1.954s\n",
      "Epoch: 732, Train loss: 3.638, Val loss: 3.185, Epoch time = 1.955s\n",
      "Epoch: 733, Train loss: 3.833, Val loss: 3.004, Epoch time = 2.065s\n",
      "Epoch: 734, Train loss: 3.577, Val loss: 3.013, Epoch time = 2.058s\n",
      "Epoch: 735, Train loss: 3.315, Val loss: 3.434, Epoch time = 2.015s\n",
      "Epoch: 736, Train loss: 3.369, Val loss: 3.036, Epoch time = 2.098s\n",
      "Epoch: 737, Train loss: 2.906, Val loss: 3.208, Epoch time = 2.026s\n",
      "Epoch: 738, Train loss: 3.018, Val loss: 2.912, Epoch time = 2.246s\n",
      "Epoch: 739, Train loss: 2.649, Val loss: 2.835, Epoch time = 2.023s\n",
      "Epoch: 740, Train loss: 3.485, Val loss: 2.817, Epoch time = 1.967s\n",
      "Epoch: 741, Train loss: 3.514, Val loss: 3.262, Epoch time = 2.002s\n",
      "Epoch: 742, Train loss: 3.023, Val loss: 2.806, Epoch time = 2.012s\n",
      "Epoch: 743, Train loss: 3.819, Val loss: 3.777, Epoch time = 2.075s\n",
      "Epoch: 744, Train loss: 3.005, Val loss: 3.602, Epoch time = 1.944s\n",
      "Epoch: 745, Train loss: 3.010, Val loss: 3.190, Epoch time = 1.971s\n",
      "Epoch: 746, Train loss: 3.052, Val loss: 3.205, Epoch time = 1.983s\n",
      "Epoch: 747, Train loss: 3.327, Val loss: 3.163, Epoch time = 1.942s\n",
      "Epoch: 748, Train loss: 3.370, Val loss: 3.261, Epoch time = 2.056s\n",
      "Epoch: 749, Train loss: 3.038, Val loss: 2.632, Epoch time = 1.968s\n",
      "Epoch: 750, Train loss: 3.887, Val loss: 3.046, Epoch time = 2.072s\n",
      "Epoch: 751, Train loss: 3.211, Val loss: 3.047, Epoch time = 1.833s\n",
      "Epoch: 752, Train loss: 3.666, Val loss: 2.643, Epoch time = 1.972s\n",
      "Epoch: 753, Train loss: 3.264, Val loss: 3.351, Epoch time = 2.000s\n",
      "Epoch: 754, Train loss: 3.748, Val loss: 2.930, Epoch time = 1.937s\n",
      "Epoch: 755, Train loss: 3.568, Val loss: 3.164, Epoch time = 1.853s\n",
      "Epoch: 756, Train loss: 2.861, Val loss: 3.296, Epoch time = 2.055s\n",
      "Epoch: 757, Train loss: 3.314, Val loss: 3.315, Epoch time = 2.063s\n",
      "Epoch: 758, Train loss: 3.578, Val loss: 3.115, Epoch time = 2.095s\n",
      "Epoch: 759, Train loss: 3.305, Val loss: 2.943, Epoch time = 2.051s\n",
      "Epoch: 760, Train loss: 3.668, Val loss: 3.126, Epoch time = 1.851s\n",
      "Epoch: 761, Train loss: 2.739, Val loss: 3.709, Epoch time = 1.792s\n",
      "Epoch: 762, Train loss: 3.028, Val loss: 2.812, Epoch time = 1.834s\n",
      "Epoch: 763, Train loss: 3.534, Val loss: 2.911, Epoch time = 1.897s\n",
      "Epoch: 764, Train loss: 3.378, Val loss: 3.880, Epoch time = 1.938s\n",
      "Epoch: 765, Train loss: 3.409, Val loss: 2.873, Epoch time = 2.204s\n",
      "Epoch: 766, Train loss: 2.986, Val loss: 3.529, Epoch time = 2.019s\n",
      "Epoch: 767, Train loss: 3.031, Val loss: 3.224, Epoch time = 2.077s\n",
      "Epoch: 768, Train loss: 3.281, Val loss: 3.499, Epoch time = 2.073s\n",
      "Epoch: 769, Train loss: 2.969, Val loss: 3.579, Epoch time = 1.989s\n",
      "Epoch: 770, Train loss: 3.164, Val loss: 3.568, Epoch time = 1.801s\n",
      "Epoch: 771, Train loss: 2.922, Val loss: 2.876, Epoch time = 1.922s\n",
      "Epoch: 772, Train loss: 3.078, Val loss: 3.021, Epoch time = 1.884s\n",
      "Epoch: 773, Train loss: 4.155, Val loss: 2.793, Epoch time = 1.877s\n",
      "Epoch: 774, Train loss: 3.304, Val loss: 2.803, Epoch time = 1.951s\n",
      "Epoch: 775, Train loss: 3.079, Val loss: 2.964, Epoch time = 1.866s\n",
      "Epoch: 776, Train loss: 3.073, Val loss: 3.655, Epoch time = 1.842s\n",
      "Epoch: 777, Train loss: 2.998, Val loss: 3.166, Epoch time = 1.818s\n",
      "Epoch: 778, Train loss: 2.845, Val loss: 2.685, Epoch time = 1.959s\n",
      "Epoch: 779, Train loss: 2.889, Val loss: 2.811, Epoch time = 1.965s\n",
      "Epoch: 780, Train loss: 3.112, Val loss: 3.265, Epoch time = 2.094s\n",
      "Epoch: 781, Train loss: 3.406, Val loss: 2.993, Epoch time = 2.060s\n",
      "Epoch: 782, Train loss: 3.337, Val loss: 3.590, Epoch time = 1.872s\n",
      "Epoch: 783, Train loss: 3.002, Val loss: 3.884, Epoch time = 1.867s\n",
      "Epoch: 784, Train loss: 3.352, Val loss: 2.940, Epoch time = 1.951s\n",
      "Epoch: 785, Train loss: 2.957, Val loss: 3.675, Epoch time = 2.023s\n",
      "Epoch: 786, Train loss: 2.922, Val loss: 3.083, Epoch time = 1.991s\n",
      "Epoch: 787, Train loss: 3.092, Val loss: 3.280, Epoch time = 1.931s\n",
      "Epoch: 788, Train loss: 3.334, Val loss: 3.019, Epoch time = 1.943s\n",
      "Epoch: 789, Train loss: 3.356, Val loss: 2.968, Epoch time = 1.851s\n",
      "Epoch: 790, Train loss: 3.144, Val loss: 2.855, Epoch time = 1.865s\n",
      "Epoch: 791, Train loss: 2.890, Val loss: 3.138, Epoch time = 1.986s\n",
      "Epoch: 792, Train loss: 2.820, Val loss: 3.079, Epoch time = 1.869s\n",
      "Epoch: 793, Train loss: 3.188, Val loss: 2.854, Epoch time = 2.097s\n",
      "Epoch: 794, Train loss: 2.922, Val loss: 3.839, Epoch time = 2.035s\n",
      "Epoch: 795, Train loss: 2.986, Val loss: 3.044, Epoch time = 1.851s\n",
      "Epoch: 796, Train loss: 2.819, Val loss: 2.762, Epoch time = 1.791s\n",
      "Epoch: 797, Train loss: 3.144, Val loss: 2.965, Epoch time = 1.809s\n",
      "Epoch: 798, Train loss: 3.653, Val loss: 2.982, Epoch time = 2.046s\n",
      "Epoch: 799, Train loss: 2.887, Val loss: 3.024, Epoch time = 2.153s\n",
      "Epoch: 800, Train loss: 2.771, Val loss: 3.352, Epoch time = 2.245s\n",
      "Epoch: 801, Train loss: 3.107, Val loss: 3.623, Epoch time = 1.988s\n",
      "Epoch: 802, Train loss: 2.841, Val loss: 3.429, Epoch time = 1.977s\n",
      "Epoch: 803, Train loss: 3.710, Val loss: 2.991, Epoch time = 2.088s\n",
      "Epoch: 804, Train loss: 3.110, Val loss: 3.473, Epoch time = 2.036s\n",
      "Epoch: 805, Train loss: 3.074, Val loss: 3.063, Epoch time = 1.998s\n",
      "Epoch: 806, Train loss: 3.111, Val loss: 3.339, Epoch time = 2.037s\n",
      "Epoch: 807, Train loss: 2.771, Val loss: 3.125, Epoch time = 1.946s\n",
      "Epoch: 808, Train loss: 3.116, Val loss: 3.124, Epoch time = 1.910s\n",
      "Epoch: 809, Train loss: 3.338, Val loss: 3.462, Epoch time = 1.913s\n",
      "Epoch: 810, Train loss: 3.157, Val loss: 3.449, Epoch time = 1.965s\n",
      "Epoch: 811, Train loss: 2.851, Val loss: 2.579, Epoch time = 1.883s\n",
      "Epoch: 812, Train loss: 3.004, Val loss: 3.100, Epoch time = 1.927s\n",
      "Epoch: 813, Train loss: 3.112, Val loss: 3.216, Epoch time = 1.880s\n",
      "Epoch: 814, Train loss: 3.465, Val loss: 3.103, Epoch time = 1.910s\n",
      "Epoch: 815, Train loss: 3.138, Val loss: 4.043, Epoch time = 1.952s\n",
      "Epoch: 816, Train loss: 2.885, Val loss: 3.294, Epoch time = 1.946s\n",
      "Epoch: 817, Train loss: 3.192, Val loss: 3.320, Epoch time = 1.861s\n",
      "Epoch: 818, Train loss: 3.718, Val loss: 3.106, Epoch time = 1.807s\n",
      "Epoch: 819, Train loss: 3.272, Val loss: 3.321, Epoch time = 2.031s\n",
      "Epoch: 820, Train loss: 2.859, Val loss: 3.474, Epoch time = 1.989s\n",
      "Epoch: 821, Train loss: 3.443, Val loss: 3.453, Epoch time = 1.818s\n",
      "Epoch: 822, Train loss: 3.127, Val loss: 3.299, Epoch time = 1.797s\n",
      "Epoch: 823, Train loss: 3.052, Val loss: 3.072, Epoch time = 1.970s\n",
      "Epoch: 824, Train loss: 3.017, Val loss: 3.597, Epoch time = 2.039s\n",
      "Epoch: 825, Train loss: 3.131, Val loss: 3.288, Epoch time = 1.851s\n",
      "Epoch: 826, Train loss: 3.189, Val loss: 2.964, Epoch time = 1.913s\n",
      "Epoch: 827, Train loss: 3.113, Val loss: 3.055, Epoch time = 1.926s\n",
      "Epoch: 828, Train loss: 3.042, Val loss: 3.377, Epoch time = 1.914s\n",
      "Epoch: 829, Train loss: 2.935, Val loss: 2.941, Epoch time = 1.903s\n",
      "Epoch: 830, Train loss: 3.132, Val loss: 3.072, Epoch time = 1.944s\n",
      "Epoch: 831, Train loss: 3.045, Val loss: 4.059, Epoch time = 1.844s\n",
      "Epoch: 832, Train loss: 2.958, Val loss: 2.889, Epoch time = 1.947s\n",
      "Epoch: 833, Train loss: 2.897, Val loss: 3.002, Epoch time = 1.866s\n",
      "Epoch: 834, Train loss: 3.037, Val loss: 3.145, Epoch time = 1.924s\n",
      "Epoch: 835, Train loss: 2.848, Val loss: 2.988, Epoch time = 1.872s\n",
      "Epoch: 836, Train loss: 2.843, Val loss: 2.808, Epoch time = 1.845s\n",
      "Epoch: 837, Train loss: 3.621, Val loss: 3.286, Epoch time = 2.021s\n",
      "Epoch: 838, Train loss: 3.177, Val loss: 3.335, Epoch time = 1.897s\n",
      "Epoch: 839, Train loss: 3.494, Val loss: 3.583, Epoch time = 1.797s\n",
      "Epoch: 840, Train loss: 3.083, Val loss: 3.251, Epoch time = 1.915s\n",
      "Epoch: 841, Train loss: 2.916, Val loss: 2.995, Epoch time = 1.914s\n",
      "Epoch: 842, Train loss: 3.627, Val loss: 3.731, Epoch time = 2.008s\n",
      "Epoch: 843, Train loss: 3.360, Val loss: 3.037, Epoch time = 2.218s\n",
      "Epoch: 844, Train loss: 3.450, Val loss: 3.143, Epoch time = 1.899s\n",
      "Epoch: 845, Train loss: 3.025, Val loss: 3.621, Epoch time = 1.874s\n",
      "Epoch: 846, Train loss: 2.940, Val loss: 3.100, Epoch time = 2.110s\n",
      "Epoch: 847, Train loss: 2.516, Val loss: 3.052, Epoch time = 2.067s\n",
      "Epoch: 848, Train loss: 3.551, Val loss: 3.054, Epoch time = 2.075s\n",
      "Epoch: 849, Train loss: 3.080, Val loss: 3.179, Epoch time = 2.119s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 850, Train loss: 2.937, Val loss: 3.166, Epoch time = 2.052s\n",
      "Epoch: 851, Train loss: 2.818, Val loss: 3.265, Epoch time = 1.998s\n",
      "Epoch: 852, Train loss: 3.633, Val loss: 3.938, Epoch time = 1.955s\n",
      "Epoch: 853, Train loss: 2.963, Val loss: 3.564, Epoch time = 1.991s\n",
      "Epoch: 854, Train loss: 3.420, Val loss: 3.299, Epoch time = 1.965s\n",
      "Epoch: 855, Train loss: 3.251, Val loss: 3.151, Epoch time = 1.809s\n",
      "Epoch: 856, Train loss: 3.641, Val loss: 2.912, Epoch time = 1.854s\n",
      "Epoch: 857, Train loss: 3.247, Val loss: 2.943, Epoch time = 1.875s\n",
      "Epoch: 858, Train loss: 3.423, Val loss: 3.416, Epoch time = 1.988s\n",
      "Epoch: 859, Train loss: 2.675, Val loss: 2.810, Epoch time = 1.958s\n",
      "Epoch: 860, Train loss: 3.093, Val loss: 3.415, Epoch time = 1.799s\n",
      "Epoch: 861, Train loss: 3.074, Val loss: 3.112, Epoch time = 1.798s\n",
      "Epoch: 862, Train loss: 3.429, Val loss: 3.059, Epoch time = 1.898s\n",
      "Epoch: 863, Train loss: 3.033, Val loss: 3.056, Epoch time = 1.947s\n",
      "Epoch: 864, Train loss: 2.975, Val loss: 2.717, Epoch time = 1.995s\n",
      "Epoch: 865, Train loss: 2.904, Val loss: 3.180, Epoch time = 1.834s\n",
      "Epoch: 866, Train loss: 3.059, Val loss: 3.774, Epoch time = 2.010s\n",
      "Epoch: 867, Train loss: 2.842, Val loss: 3.284, Epoch time = 1.858s\n",
      "Epoch: 868, Train loss: 2.941, Val loss: 3.218, Epoch time = 1.924s\n",
      "Epoch: 869, Train loss: 4.072, Val loss: 3.406, Epoch time = 1.930s\n",
      "Epoch: 870, Train loss: 3.104, Val loss: 3.711, Epoch time = 2.020s\n",
      "Epoch: 871, Train loss: 3.238, Val loss: 3.271, Epoch time = 2.099s\n",
      "Epoch: 872, Train loss: 2.926, Val loss: 2.937, Epoch time = 1.861s\n",
      "Epoch: 873, Train loss: 3.503, Val loss: 3.067, Epoch time = 2.189s\n",
      "Epoch: 874, Train loss: 3.980, Val loss: 3.159, Epoch time = 1.857s\n",
      "Epoch: 875, Train loss: 2.917, Val loss: 2.627, Epoch time = 1.949s\n",
      "Epoch: 876, Train loss: 2.911, Val loss: 2.963, Epoch time = 1.868s\n",
      "Epoch: 877, Train loss: 2.931, Val loss: 3.476, Epoch time = 1.880s\n",
      "Epoch: 878, Train loss: 2.992, Val loss: 2.879, Epoch time = 2.054s\n",
      "Epoch: 879, Train loss: 3.702, Val loss: 2.998, Epoch time = 1.961s\n",
      "Epoch: 880, Train loss: 3.622, Val loss: 3.412, Epoch time = 1.995s\n",
      "Epoch: 881, Train loss: 3.289, Val loss: 3.184, Epoch time = 2.049s\n",
      "Epoch: 882, Train loss: 3.199, Val loss: 3.515, Epoch time = 2.053s\n",
      "Epoch: 883, Train loss: 3.082, Val loss: 3.006, Epoch time = 2.042s\n",
      "Epoch: 884, Train loss: 3.224, Val loss: 2.851, Epoch time = 2.101s\n",
      "Epoch: 885, Train loss: 3.297, Val loss: 2.901, Epoch time = 1.934s\n",
      "Epoch: 886, Train loss: 3.603, Val loss: 2.992, Epoch time = 2.026s\n",
      "Epoch: 887, Train loss: 2.788, Val loss: 3.240, Epoch time = 2.035s\n",
      "Epoch: 888, Train loss: 3.804, Val loss: 3.424, Epoch time = 2.296s\n",
      "Epoch: 889, Train loss: 3.521, Val loss: 2.927, Epoch time = 2.475s\n",
      "Epoch: 890, Train loss: 2.964, Val loss: 3.466, Epoch time = 2.457s\n",
      "Epoch: 891, Train loss: 2.942, Val loss: 2.804, Epoch time = 2.018s\n",
      "Epoch: 892, Train loss: 2.897, Val loss: 3.681, Epoch time = 1.907s\n",
      "Epoch: 893, Train loss: 3.057, Val loss: 2.655, Epoch time = 1.795s\n",
      "Epoch: 894, Train loss: 2.829, Val loss: 2.991, Epoch time = 1.801s\n",
      "Epoch: 895, Train loss: 2.761, Val loss: 2.867, Epoch time = 1.956s\n",
      "Epoch: 896, Train loss: 3.025, Val loss: 3.071, Epoch time = 1.912s\n",
      "Epoch: 897, Train loss: 3.108, Val loss: 2.928, Epoch time = 1.927s\n",
      "Epoch: 898, Train loss: 3.136, Val loss: 2.949, Epoch time = 1.961s\n",
      "Epoch: 899, Train loss: 3.103, Val loss: 3.125, Epoch time = 2.184s\n",
      "Epoch: 900, Train loss: 3.262, Val loss: 3.231, Epoch time = 1.972s\n",
      "Epoch: 901, Train loss: 3.137, Val loss: 3.212, Epoch time = 1.910s\n",
      "Epoch: 902, Train loss: 2.887, Val loss: 3.149, Epoch time = 2.060s\n",
      "Epoch: 903, Train loss: 2.987, Val loss: 2.788, Epoch time = 1.991s\n",
      "Epoch: 904, Train loss: 4.012, Val loss: 3.182, Epoch time = 1.893s\n",
      "Epoch: 905, Train loss: 3.214, Val loss: 3.282, Epoch time = 1.814s\n",
      "Epoch: 906, Train loss: 3.376, Val loss: 3.481, Epoch time = 1.779s\n",
      "Epoch: 907, Train loss: 2.963, Val loss: 3.770, Epoch time = 1.982s\n",
      "Epoch: 908, Train loss: 3.736, Val loss: 3.173, Epoch time = 1.955s\n",
      "Epoch: 909, Train loss: 3.194, Val loss: 3.101, Epoch time = 1.946s\n",
      "Epoch: 910, Train loss: 3.208, Val loss: 2.671, Epoch time = 1.985s\n",
      "Epoch: 911, Train loss: 2.880, Val loss: 3.320, Epoch time = 1.785s\n",
      "Epoch: 912, Train loss: 2.805, Val loss: 3.678, Epoch time = 1.758s\n",
      "Epoch: 913, Train loss: 3.360, Val loss: 3.378, Epoch time = 2.039s\n",
      "Epoch: 914, Train loss: 3.856, Val loss: 2.900, Epoch time = 1.990s\n",
      "Epoch: 915, Train loss: 2.934, Val loss: 3.080, Epoch time = 2.305s\n",
      "Epoch: 916, Train loss: 3.544, Val loss: 3.010, Epoch time = 1.869s\n",
      "Epoch: 917, Train loss: 3.419, Val loss: 3.501, Epoch time = 1.857s\n",
      "Epoch: 918, Train loss: 3.399, Val loss: 2.991, Epoch time = 1.776s\n",
      "Epoch: 919, Train loss: 3.026, Val loss: 2.922, Epoch time = 1.985s\n",
      "Epoch: 920, Train loss: 3.271, Val loss: 2.717, Epoch time = 2.121s\n",
      "Epoch: 921, Train loss: 3.264, Val loss: 3.365, Epoch time = 1.919s\n",
      "Epoch: 922, Train loss: 2.951, Val loss: 3.085, Epoch time = 1.871s\n",
      "Epoch: 923, Train loss: 2.951, Val loss: 3.550, Epoch time = 1.863s\n",
      "Epoch: 924, Train loss: 3.805, Val loss: 2.947, Epoch time = 1.916s\n",
      "Epoch: 925, Train loss: 3.206, Val loss: 2.823, Epoch time = 1.842s\n",
      "Epoch: 926, Train loss: 3.132, Val loss: 4.030, Epoch time = 2.002s\n",
      "Epoch: 927, Train loss: 2.677, Val loss: 2.943, Epoch time = 1.976s\n",
      "Epoch: 928, Train loss: 3.011, Val loss: 3.251, Epoch time = 1.857s\n",
      "Epoch: 929, Train loss: 2.931, Val loss: 3.284, Epoch time = 1.856s\n",
      "Epoch: 930, Train loss: 3.295, Val loss: 2.787, Epoch time = 2.029s\n",
      "Epoch: 931, Train loss: 3.531, Val loss: 2.778, Epoch time = 1.851s\n",
      "Epoch: 932, Train loss: 2.765, Val loss: 3.458, Epoch time = 1.776s\n",
      "Epoch: 933, Train loss: 3.030, Val loss: 3.239, Epoch time = 1.944s\n",
      "Epoch: 934, Train loss: 3.057, Val loss: 3.427, Epoch time = 1.846s\n",
      "Epoch: 935, Train loss: 3.782, Val loss: 2.845, Epoch time = 1.932s\n",
      "Epoch: 936, Train loss: 2.846, Val loss: 3.112, Epoch time = 2.022s\n",
      "Epoch: 937, Train loss: 3.261, Val loss: 3.016, Epoch time = 2.083s\n",
      "Epoch: 938, Train loss: 4.073, Val loss: 3.064, Epoch time = 1.934s\n",
      "Epoch: 939, Train loss: 3.033, Val loss: 2.944, Epoch time = 1.984s\n",
      "Epoch: 940, Train loss: 3.592, Val loss: 3.359, Epoch time = 2.004s\n",
      "Epoch: 941, Train loss: 3.193, Val loss: 3.101, Epoch time = 1.826s\n",
      "Epoch: 942, Train loss: 3.013, Val loss: 3.389, Epoch time = 1.884s\n",
      "Epoch: 943, Train loss: 2.654, Val loss: 2.762, Epoch time = 2.000s\n",
      "Epoch: 944, Train loss: 3.226, Val loss: 3.478, Epoch time = 1.791s\n",
      "Epoch: 945, Train loss: 3.104, Val loss: 3.063, Epoch time = 1.858s\n",
      "Epoch: 946, Train loss: 3.003, Val loss: 3.604, Epoch time = 1.919s\n",
      "Epoch: 947, Train loss: 2.923, Val loss: 3.460, Epoch time = 1.925s\n",
      "Epoch: 948, Train loss: 3.351, Val loss: 2.991, Epoch time = 2.020s\n",
      "Epoch: 949, Train loss: 3.057, Val loss: 3.326, Epoch time = 1.939s\n",
      "Epoch: 950, Train loss: 3.320, Val loss: 2.694, Epoch time = 1.928s\n",
      "Epoch: 951, Train loss: 3.474, Val loss: 3.188, Epoch time = 1.919s\n",
      "Epoch: 952, Train loss: 3.042, Val loss: 2.971, Epoch time = 1.781s\n",
      "Epoch: 953, Train loss: 2.747, Val loss: 3.318, Epoch time = 1.812s\n",
      "Epoch: 954, Train loss: 2.875, Val loss: 3.632, Epoch time = 1.943s\n",
      "Epoch: 955, Train loss: 2.842, Val loss: 2.778, Epoch time = 2.092s\n",
      "Epoch: 956, Train loss: 2.800, Val loss: 2.724, Epoch time = 1.997s\n",
      "Epoch: 957, Train loss: 2.688, Val loss: 2.882, Epoch time = 2.055s\n",
      "Epoch: 958, Train loss: 3.131, Val loss: 2.960, Epoch time = 1.981s\n",
      "Epoch: 959, Train loss: 2.697, Val loss: 3.146, Epoch time = 2.059s\n",
      "Epoch: 960, Train loss: 3.665, Val loss: 2.785, Epoch time = 2.119s\n",
      "Epoch: 961, Train loss: 3.117, Val loss: 3.537, Epoch time = 1.814s\n",
      "Epoch: 962, Train loss: 2.922, Val loss: 3.094, Epoch time = 2.074s\n",
      "Epoch: 963, Train loss: 3.507, Val loss: 3.229, Epoch time = 2.216s\n",
      "Epoch: 964, Train loss: 2.875, Val loss: 3.041, Epoch time = 1.897s\n",
      "Epoch: 965, Train loss: 3.284, Val loss: 3.333, Epoch time = 2.216s\n",
      "Epoch: 966, Train loss: 3.802, Val loss: 3.668, Epoch time = 2.047s\n",
      "Epoch: 967, Train loss: 3.125, Val loss: 3.227, Epoch time = 1.799s\n",
      "Epoch: 968, Train loss: 2.714, Val loss: 2.833, Epoch time = 1.851s\n",
      "Epoch: 969, Train loss: 2.863, Val loss: 2.966, Epoch time = 2.145s\n",
      "Epoch: 970, Train loss: 2.682, Val loss: 3.295, Epoch time = 2.416s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 971, Train loss: 2.713, Val loss: 3.223, Epoch time = 2.443s\n",
      "Epoch: 972, Train loss: 3.026, Val loss: 3.117, Epoch time = 2.677s\n",
      "Epoch: 973, Train loss: 2.915, Val loss: 2.956, Epoch time = 2.265s\n",
      "Epoch: 974, Train loss: 3.523, Val loss: 3.026, Epoch time = 2.426s\n",
      "Epoch: 975, Train loss: 3.479, Val loss: 3.087, Epoch time = 1.999s\n",
      "Epoch: 976, Train loss: 2.660, Val loss: 2.965, Epoch time = 1.813s\n",
      "Epoch: 977, Train loss: 3.582, Val loss: 4.129, Epoch time = 1.906s\n",
      "Epoch: 978, Train loss: 2.892, Val loss: 2.931, Epoch time = 1.997s\n",
      "Epoch: 979, Train loss: 3.228, Val loss: 3.029, Epoch time = 2.227s\n",
      "Epoch: 980, Train loss: 3.142, Val loss: 3.098, Epoch time = 1.954s\n",
      "Epoch: 981, Train loss: 3.023, Val loss: 3.123, Epoch time = 2.039s\n",
      "Epoch: 982, Train loss: 3.143, Val loss: 2.948, Epoch time = 2.085s\n",
      "Epoch: 983, Train loss: 2.848, Val loss: 3.349, Epoch time = 1.991s\n",
      "Epoch: 984, Train loss: 2.463, Val loss: 3.335, Epoch time = 2.194s\n",
      "Epoch: 985, Train loss: 3.298, Val loss: 2.988, Epoch time = 1.943s\n",
      "Epoch: 986, Train loss: 3.769, Val loss: 3.424, Epoch time = 1.964s\n",
      "Epoch: 987, Train loss: 2.849, Val loss: 2.615, Epoch time = 2.154s\n",
      "Epoch: 988, Train loss: 2.952, Val loss: 3.553, Epoch time = 2.105s\n",
      "Epoch: 989, Train loss: 3.690, Val loss: 3.116, Epoch time = 2.148s\n",
      "Epoch: 990, Train loss: 2.857, Val loss: 3.041, Epoch time = 1.932s\n",
      "Epoch: 991, Train loss: 2.730, Val loss: 2.875, Epoch time = 2.090s\n",
      "Epoch: 992, Train loss: 2.985, Val loss: 2.971, Epoch time = 2.008s\n",
      "Epoch: 993, Train loss: 2.946, Val loss: 2.978, Epoch time = 1.845s\n",
      "Epoch: 994, Train loss: 3.066, Val loss: 2.952, Epoch time = 2.061s\n",
      "Epoch: 995, Train loss: 2.852, Val loss: 2.981, Epoch time = 2.041s\n",
      "Epoch: 996, Train loss: 2.913, Val loss: 3.472, Epoch time = 2.145s\n",
      "Epoch: 997, Train loss: 3.190, Val loss: 2.751, Epoch time = 1.997s\n",
      "Epoch: 998, Train loss: 3.430, Val loss: 2.848, Epoch time = 2.069s\n",
      "Epoch: 999, Train loss: 3.255, Val loss: 3.230, Epoch time = 2.093s\n",
      "Epoch: 1000, Train loss: 2.819, Val loss: 2.881, Epoch time = 2.026s\n",
      "Epoch: 1001, Train loss: 2.919, Val loss: 2.705, Epoch time = 2.058s\n",
      "Epoch: 1002, Train loss: 2.873, Val loss: 3.058, Epoch time = 1.924s\n",
      "Epoch: 1003, Train loss: 3.003, Val loss: 3.215, Epoch time = 1.927s\n",
      "Epoch: 1004, Train loss: 2.786, Val loss: 3.067, Epoch time = 1.900s\n",
      "Epoch: 1005, Train loss: 2.879, Val loss: 3.607, Epoch time = 1.795s\n",
      "Epoch: 1006, Train loss: 3.337, Val loss: 3.016, Epoch time = 1.847s\n",
      "Epoch: 1007, Train loss: 2.541, Val loss: 3.066, Epoch time = 1.975s\n",
      "Epoch: 1008, Train loss: 2.979, Val loss: 2.886, Epoch time = 1.842s\n",
      "Epoch: 1009, Train loss: 2.811, Val loss: 3.116, Epoch time = 1.907s\n",
      "Epoch: 1010, Train loss: 3.135, Val loss: 3.090, Epoch time = 1.950s\n",
      "Epoch: 1011, Train loss: 2.723, Val loss: 3.657, Epoch time = 1.959s\n",
      "Epoch: 1012, Train loss: 3.152, Val loss: 4.007, Epoch time = 2.080s\n",
      "Epoch: 1013, Train loss: 3.129, Val loss: 3.369, Epoch time = 2.196s\n",
      "Epoch: 1014, Train loss: 3.729, Val loss: 2.775, Epoch time = 2.237s\n",
      "Epoch: 1015, Train loss: 3.354, Val loss: 3.142, Epoch time = 2.062s\n",
      "Epoch: 1016, Train loss: 3.504, Val loss: 3.164, Epoch time = 2.084s\n",
      "Epoch: 1017, Train loss: 2.787, Val loss: 3.184, Epoch time = 2.094s\n",
      "Epoch: 1018, Train loss: 3.625, Val loss: 2.785, Epoch time = 2.038s\n",
      "Epoch: 1019, Train loss: 2.981, Val loss: 3.040, Epoch time = 1.884s\n",
      "Epoch: 1020, Train loss: 3.079, Val loss: 2.985, Epoch time = 1.813s\n",
      "Epoch: 1021, Train loss: 3.634, Val loss: 3.435, Epoch time = 1.887s\n",
      "Epoch: 1022, Train loss: 2.940, Val loss: 3.881, Epoch time = 1.904s\n",
      "Epoch: 1023, Train loss: 2.958, Val loss: 2.489, Epoch time = 2.117s\n",
      "Epoch: 1024, Train loss: 3.512, Val loss: 3.296, Epoch time = 2.067s\n",
      "Epoch: 1025, Train loss: 2.935, Val loss: 3.268, Epoch time = 1.866s\n",
      "Epoch: 1026, Train loss: 2.794, Val loss: 3.136, Epoch time = 1.792s\n",
      "Epoch: 1027, Train loss: 2.954, Val loss: 3.090, Epoch time = 1.832s\n",
      "Epoch: 1028, Train loss: 3.327, Val loss: 3.131, Epoch time = 1.784s\n",
      "Epoch: 1029, Train loss: 3.210, Val loss: 2.642, Epoch time = 1.794s\n",
      "Epoch: 1030, Train loss: 3.097, Val loss: 3.370, Epoch time = 2.146s\n",
      "Epoch: 1031, Train loss: 3.082, Val loss: 3.389, Epoch time = 1.956s\n",
      "Epoch: 1032, Train loss: 2.835, Val loss: 2.959, Epoch time = 1.892s\n",
      "Epoch: 1033, Train loss: 3.095, Val loss: 2.975, Epoch time = 1.996s\n",
      "Epoch: 1034, Train loss: 3.034, Val loss: 3.240, Epoch time = 1.875s\n",
      "Epoch: 1035, Train loss: 2.995, Val loss: 3.136, Epoch time = 1.938s\n",
      "Epoch: 1036, Train loss: 2.846, Val loss: 3.000, Epoch time = 2.054s\n",
      "Epoch: 1037, Train loss: 3.317, Val loss: 3.352, Epoch time = 1.784s\n",
      "Epoch: 1038, Train loss: 3.687, Val loss: 3.234, Epoch time = 1.903s\n",
      "Epoch: 1039, Train loss: 2.829, Val loss: 2.554, Epoch time = 2.025s\n",
      "Epoch: 1040, Train loss: 2.786, Val loss: 2.927, Epoch time = 1.820s\n",
      "Epoch: 1041, Train loss: 3.100, Val loss: 3.709, Epoch time = 2.031s\n",
      "Epoch: 1042, Train loss: 3.339, Val loss: 3.008, Epoch time = 1.957s\n",
      "Epoch: 1043, Train loss: 3.644, Val loss: 3.122, Epoch time = 1.867s\n",
      "Epoch: 1044, Train loss: 3.061, Val loss: 3.859, Epoch time = 1.916s\n",
      "Epoch: 1045, Train loss: 3.274, Val loss: 3.268, Epoch time = 1.905s\n",
      "Epoch: 1046, Train loss: 3.250, Val loss: 2.948, Epoch time = 2.054s\n",
      "Epoch: 1047, Train loss: 3.137, Val loss: 3.612, Epoch time = 1.965s\n",
      "Epoch: 1048, Train loss: 3.108, Val loss: 3.266, Epoch time = 1.914s\n",
      "Epoch: 1049, Train loss: 3.351, Val loss: 2.784, Epoch time = 1.944s\n",
      "Epoch: 1050, Train loss: 3.448, Val loss: 2.917, Epoch time = 1.841s\n",
      "Epoch: 1051, Train loss: 3.348, Val loss: 3.765, Epoch time = 1.811s\n",
      "Epoch: 1052, Train loss: 3.055, Val loss: 2.919, Epoch time = 1.821s\n",
      "Epoch: 1053, Train loss: 3.327, Val loss: 2.997, Epoch time = 1.793s\n",
      "Epoch: 1054, Train loss: 3.336, Val loss: 3.114, Epoch time = 1.808s\n",
      "Epoch: 1055, Train loss: 3.210, Val loss: 3.492, Epoch time = 1.829s\n",
      "Epoch: 1056, Train loss: 3.138, Val loss: 3.020, Epoch time = 2.040s\n",
      "Epoch: 1057, Train loss: 2.908, Val loss: 2.593, Epoch time = 1.882s\n",
      "Epoch: 1058, Train loss: 3.179, Val loss: 2.952, Epoch time = 1.794s\n",
      "Epoch: 1059, Train loss: 2.744, Val loss: 3.063, Epoch time = 2.040s\n",
      "Epoch: 1060, Train loss: 3.420, Val loss: 2.863, Epoch time = 1.961s\n",
      "Epoch: 1061, Train loss: 2.617, Val loss: 3.166, Epoch time = 1.992s\n",
      "Epoch: 1062, Train loss: 2.851, Val loss: 3.323, Epoch time = 1.954s\n",
      "Epoch: 1063, Train loss: 2.798, Val loss: 4.133, Epoch time = 1.949s\n",
      "Epoch: 1064, Train loss: 2.767, Val loss: 3.021, Epoch time = 1.949s\n",
      "Epoch: 1065, Train loss: 2.905, Val loss: 2.796, Epoch time = 1.952s\n",
      "Epoch: 1066, Train loss: 3.164, Val loss: 2.755, Epoch time = 2.108s\n",
      "Epoch: 1067, Train loss: 3.651, Val loss: 3.618, Epoch time = 1.770s\n",
      "Epoch: 1068, Train loss: 3.456, Val loss: 3.064, Epoch time = 1.831s\n",
      "Epoch: 1069, Train loss: 3.351, Val loss: 3.640, Epoch time = 1.773s\n",
      "Epoch: 1070, Train loss: 3.275, Val loss: 2.869, Epoch time = 1.820s\n",
      "Epoch: 1071, Train loss: 3.000, Val loss: 3.134, Epoch time = 1.772s\n",
      "Epoch: 1072, Train loss: 3.113, Val loss: 3.191, Epoch time = 2.082s\n",
      "Epoch: 1073, Train loss: 2.798, Val loss: 3.273, Epoch time = 1.984s\n",
      "Epoch: 1074, Train loss: 3.225, Val loss: 3.905, Epoch time = 2.066s\n",
      "Epoch: 1075, Train loss: 3.501, Val loss: 3.044, Epoch time = 1.951s\n",
      "Epoch: 1076, Train loss: 2.811, Val loss: 2.801, Epoch time = 2.028s\n",
      "Epoch: 1077, Train loss: 3.019, Val loss: 2.902, Epoch time = 1.926s\n",
      "Epoch: 1078, Train loss: 3.542, Val loss: 3.144, Epoch time = 2.036s\n",
      "Epoch: 1079, Train loss: 2.653, Val loss: 2.710, Epoch time = 1.885s\n",
      "Epoch: 1080, Train loss: 3.092, Val loss: 3.650, Epoch time = 1.878s\n",
      "Epoch: 1081, Train loss: 2.865, Val loss: 2.963, Epoch time = 1.872s\n",
      "Epoch: 1082, Train loss: 2.783, Val loss: 3.776, Epoch time = 1.994s\n",
      "Epoch: 1083, Train loss: 2.899, Val loss: 3.215, Epoch time = 2.036s\n",
      "Epoch: 1084, Train loss: 3.203, Val loss: 3.065, Epoch time = 1.942s\n",
      "Epoch: 1085, Train loss: 3.101, Val loss: 3.191, Epoch time = 1.931s\n",
      "Epoch: 1086, Train loss: 2.709, Val loss: 3.022, Epoch time = 1.864s\n",
      "Epoch: 1087, Train loss: 3.167, Val loss: 2.861, Epoch time = 1.794s\n",
      "Epoch: 1088, Train loss: 2.760, Val loss: 3.063, Epoch time = 1.829s\n",
      "Epoch: 1089, Train loss: 3.209, Val loss: 3.155, Epoch time = 1.786s\n",
      "Epoch: 1090, Train loss: 3.334, Val loss: 2.851, Epoch time = 1.798s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1091, Train loss: 2.929, Val loss: 3.550, Epoch time = 1.932s\n",
      "Epoch: 1092, Train loss: 2.669, Val loss: 3.289, Epoch time = 2.005s\n",
      "Epoch: 1093, Train loss: 2.819, Val loss: 3.266, Epoch time = 1.872s\n",
      "Epoch: 1094, Train loss: 2.905, Val loss: 3.565, Epoch time = 1.853s\n",
      "Epoch: 1095, Train loss: 2.984, Val loss: 3.367, Epoch time = 1.864s\n",
      "Epoch: 1096, Train loss: 2.647, Val loss: 2.829, Epoch time = 2.154s\n",
      "Epoch: 1097, Train loss: 3.046, Val loss: 3.507, Epoch time = 2.120s\n",
      "Epoch: 1098, Train loss: 3.447, Val loss: 3.334, Epoch time = 1.921s\n",
      "Epoch: 1099, Train loss: 2.704, Val loss: 2.885, Epoch time = 1.964s\n",
      "Epoch: 1100, Train loss: 2.725, Val loss: 2.915, Epoch time = 1.965s\n",
      "Epoch: 1101, Train loss: 2.785, Val loss: 3.453, Epoch time = 2.001s\n",
      "Epoch: 1102, Train loss: 3.825, Val loss: 3.279, Epoch time = 1.951s\n",
      "Epoch: 1103, Train loss: 2.903, Val loss: 3.157, Epoch time = 2.099s\n",
      "Epoch: 1104, Train loss: 3.020, Val loss: 2.548, Epoch time = 2.048s\n",
      "Epoch: 1105, Train loss: 2.945, Val loss: 2.649, Epoch time = 2.117s\n",
      "Epoch: 1106, Train loss: 2.849, Val loss: 3.340, Epoch time = 2.034s\n",
      "Epoch: 1107, Train loss: 3.361, Val loss: 2.864, Epoch time = 2.078s\n",
      "Epoch: 1108, Train loss: 2.933, Val loss: 3.461, Epoch time = 2.077s\n",
      "Epoch: 1109, Train loss: 2.939, Val loss: 3.591, Epoch time = 2.302s\n",
      "Epoch: 1110, Train loss: 2.792, Val loss: 2.903, Epoch time = 2.251s\n",
      "Epoch: 1111, Train loss: 2.802, Val loss: 2.590, Epoch time = 2.049s\n",
      "Epoch: 1112, Train loss: 3.483, Val loss: 2.969, Epoch time = 2.056s\n",
      "Epoch: 1113, Train loss: 3.054, Val loss: 2.874, Epoch time = 2.123s\n",
      "Epoch: 1114, Train loss: 2.802, Val loss: 3.018, Epoch time = 2.298s\n",
      "Epoch: 1115, Train loss: 2.761, Val loss: 2.808, Epoch time = 2.330s\n",
      "Epoch: 1116, Train loss: 2.751, Val loss: 2.617, Epoch time = 2.031s\n",
      "Epoch: 1117, Train loss: 3.200, Val loss: 2.697, Epoch time = 1.905s\n",
      "Epoch: 1118, Train loss: 2.960, Val loss: 3.101, Epoch time = 2.078s\n",
      "Epoch: 1119, Train loss: 2.924, Val loss: 2.993, Epoch time = 1.966s\n",
      "Epoch: 1120, Train loss: 2.817, Val loss: 2.854, Epoch time = 2.212s\n",
      "Epoch: 1121, Train loss: 2.650, Val loss: 3.163, Epoch time = 2.057s\n",
      "Epoch: 1122, Train loss: 3.190, Val loss: 3.290, Epoch time = 2.172s\n",
      "Epoch: 1123, Train loss: 3.566, Val loss: 3.084, Epoch time = 2.326s\n",
      "Epoch: 1124, Train loss: 3.528, Val loss: 3.370, Epoch time = 2.237s\n",
      "Epoch: 1125, Train loss: 2.913, Val loss: 3.350, Epoch time = 2.151s\n",
      "Epoch: 1126, Train loss: 3.140, Val loss: 2.936, Epoch time = 2.081s\n",
      "Epoch: 1127, Train loss: 2.772, Val loss: 2.823, Epoch time = 2.024s\n",
      "Epoch: 1128, Train loss: 3.829, Val loss: 2.908, Epoch time = 2.110s\n",
      "Epoch: 1129, Train loss: 3.517, Val loss: 2.852, Epoch time = 1.978s\n",
      "Epoch: 1130, Train loss: 3.179, Val loss: 2.936, Epoch time = 1.977s\n",
      "Epoch: 1131, Train loss: 2.947, Val loss: 2.785, Epoch time = 1.992s\n",
      "Epoch: 1132, Train loss: 2.485, Val loss: 3.087, Epoch time = 2.033s\n",
      "Epoch: 1133, Train loss: 2.902, Val loss: 3.536, Epoch time = 2.023s\n",
      "Epoch: 1134, Train loss: 3.652, Val loss: 3.350, Epoch time = 1.993s\n",
      "Epoch: 1135, Train loss: 3.155, Val loss: 3.103, Epoch time = 2.149s\n",
      "Epoch: 1136, Train loss: 2.855, Val loss: 3.113, Epoch time = 2.067s\n",
      "Epoch: 1137, Train loss: 2.828, Val loss: 2.961, Epoch time = 2.050s\n",
      "Epoch: 1138, Train loss: 3.700, Val loss: 3.470, Epoch time = 1.894s\n",
      "Epoch: 1139, Train loss: 2.885, Val loss: 3.585, Epoch time = 2.009s\n",
      "Epoch: 1140, Train loss: 2.896, Val loss: 2.824, Epoch time = 2.040s\n",
      "Epoch: 1141, Train loss: 3.142, Val loss: 2.904, Epoch time = 1.931s\n",
      "Epoch: 1142, Train loss: 3.391, Val loss: 3.245, Epoch time = 1.852s\n",
      "Epoch: 1143, Train loss: 2.714, Val loss: 3.124, Epoch time = 1.867s\n",
      "Epoch: 1144, Train loss: 3.110, Val loss: 3.425, Epoch time = 1.883s\n",
      "Epoch: 1145, Train loss: 3.396, Val loss: 3.558, Epoch time = 1.846s\n",
      "Epoch: 1146, Train loss: 3.027, Val loss: 3.221, Epoch time = 2.147s\n",
      "Epoch: 1147, Train loss: 3.638, Val loss: 3.413, Epoch time = 1.874s\n",
      "Epoch: 1148, Train loss: 3.393, Val loss: 2.964, Epoch time = 1.921s\n",
      "Epoch: 1149, Train loss: 2.816, Val loss: 2.797, Epoch time = 1.926s\n",
      "Epoch: 1150, Train loss: 3.465, Val loss: 2.930, Epoch time = 1.933s\n",
      "Epoch: 1151, Train loss: 3.261, Val loss: 3.121, Epoch time = 2.015s\n",
      "Epoch: 1152, Train loss: 2.928, Val loss: 2.623, Epoch time = 1.984s\n",
      "Epoch: 1153, Train loss: 3.757, Val loss: 3.001, Epoch time = 2.166s\n",
      "Epoch: 1154, Train loss: 3.482, Val loss: 2.558, Epoch time = 2.062s\n",
      "Epoch: 1155, Train loss: 2.619, Val loss: 2.915, Epoch time = 2.162s\n",
      "Epoch: 1156, Train loss: 3.175, Val loss: 2.657, Epoch time = 2.236s\n",
      "Epoch: 1157, Train loss: 3.652, Val loss: 2.744, Epoch time = 2.171s\n",
      "Epoch: 1158, Train loss: 3.132, Val loss: 3.055, Epoch time = 2.199s\n",
      "Epoch: 1159, Train loss: 3.510, Val loss: 3.104, Epoch time = 1.909s\n",
      "Epoch: 1160, Train loss: 3.131, Val loss: 2.911, Epoch time = 1.966s\n",
      "Epoch: 1161, Train loss: 3.121, Val loss: 3.145, Epoch time = 2.099s\n",
      "Epoch: 1162, Train loss: 3.174, Val loss: 3.501, Epoch time = 1.989s\n",
      "Epoch: 1163, Train loss: 3.348, Val loss: 2.976, Epoch time = 1.964s\n",
      "Epoch: 1164, Train loss: 3.069, Val loss: 2.732, Epoch time = 1.858s\n",
      "Epoch: 1165, Train loss: 3.304, Val loss: 2.962, Epoch time = 1.895s\n",
      "Epoch: 1166, Train loss: 3.177, Val loss: 2.920, Epoch time = 1.855s\n",
      "Epoch: 1167, Train loss: 3.118, Val loss: 3.813, Epoch time = 1.855s\n",
      "Epoch: 1168, Train loss: 3.356, Val loss: 3.113, Epoch time = 1.861s\n",
      "Epoch: 1169, Train loss: 2.621, Val loss: 2.849, Epoch time = 1.860s\n",
      "Epoch: 1170, Train loss: 3.573, Val loss: 2.797, Epoch time = 1.875s\n",
      "Epoch: 1171, Train loss: 4.016, Val loss: 3.191, Epoch time = 1.815s\n",
      "Epoch: 1172, Train loss: 3.634, Val loss: 3.203, Epoch time = 1.917s\n",
      "Epoch: 1173, Train loss: 3.019, Val loss: 3.001, Epoch time = 2.236s\n",
      "Epoch: 1174, Train loss: 2.995, Val loss: 3.159, Epoch time = 2.070s\n",
      "Epoch: 1175, Train loss: 2.720, Val loss: 3.557, Epoch time = 2.301s\n",
      "Epoch: 1176, Train loss: 2.916, Val loss: 3.276, Epoch time = 2.077s\n",
      "Epoch: 1177, Train loss: 3.803, Val loss: 2.688, Epoch time = 2.065s\n",
      "Epoch: 1178, Train loss: 3.477, Val loss: 3.730, Epoch time = 2.016s\n",
      "Epoch: 1179, Train loss: 3.123, Val loss: 2.909, Epoch time = 1.964s\n",
      "Epoch: 1180, Train loss: 2.969, Val loss: 3.647, Epoch time = 1.977s\n",
      "Epoch: 1181, Train loss: 2.901, Val loss: 3.451, Epoch time = 1.984s\n",
      "Epoch: 1182, Train loss: 3.641, Val loss: 2.885, Epoch time = 2.022s\n",
      "Epoch: 1183, Train loss: 3.095, Val loss: 2.684, Epoch time = 1.954s\n",
      "Epoch: 1184, Train loss: 3.522, Val loss: 2.870, Epoch time = 1.960s\n",
      "Epoch: 1185, Train loss: 3.182, Val loss: 2.967, Epoch time = 2.045s\n",
      "Epoch: 1186, Train loss: 3.223, Val loss: 2.807, Epoch time = 1.875s\n",
      "Epoch: 1187, Train loss: 3.307, Val loss: 3.240, Epoch time = 1.897s\n",
      "Epoch: 1188, Train loss: 3.687, Val loss: 2.755, Epoch time = 1.820s\n",
      "Epoch: 1189, Train loss: 3.076, Val loss: 2.775, Epoch time = 1.805s\n",
      "Epoch: 1190, Train loss: 3.587, Val loss: 3.311, Epoch time = 1.806s\n",
      "Epoch: 1191, Train loss: 2.889, Val loss: 3.333, Epoch time = 1.803s\n",
      "Epoch: 1192, Train loss: 2.789, Val loss: 2.704, Epoch time = 2.149s\n",
      "Epoch: 1193, Train loss: 3.130, Val loss: 3.498, Epoch time = 2.006s\n",
      "Epoch: 1194, Train loss: 2.858, Val loss: 3.047, Epoch time = 1.804s\n",
      "Epoch: 1195, Train loss: 2.577, Val loss: 2.839, Epoch time = 1.925s\n",
      "Epoch: 1196, Train loss: 3.325, Val loss: 3.473, Epoch time = 2.059s\n",
      "Epoch: 1197, Train loss: 2.685, Val loss: 3.433, Epoch time = 2.085s\n",
      "Epoch: 1198, Train loss: 3.501, Val loss: 3.032, Epoch time = 1.968s\n",
      "Epoch: 1199, Train loss: 3.369, Val loss: 3.534, Epoch time = 1.845s\n",
      "Epoch: 1200, Train loss: 3.336, Val loss: 3.107, Epoch time = 1.853s\n",
      "Epoch: 1201, Train loss: 3.169, Val loss: 2.916, Epoch time = 1.969s\n",
      "Epoch: 1202, Train loss: 2.904, Val loss: 2.807, Epoch time = 1.940s\n",
      "Epoch: 1203, Train loss: 3.264, Val loss: 3.284, Epoch time = 1.763s\n",
      "Epoch: 1204, Train loss: 2.952, Val loss: 2.893, Epoch time = 1.855s\n",
      "Epoch: 1205, Train loss: 3.244, Val loss: 3.339, Epoch time = 1.839s\n",
      "Epoch: 1206, Train loss: 2.850, Val loss: 3.034, Epoch time = 1.985s\n",
      "Epoch: 1207, Train loss: 3.466, Val loss: 3.042, Epoch time = 1.843s\n",
      "Epoch: 1208, Train loss: 3.123, Val loss: 3.042, Epoch time = 1.830s\n",
      "Epoch: 1209, Train loss: 3.529, Val loss: 2.888, Epoch time = 1.843s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1210, Train loss: 2.921, Val loss: 2.965, Epoch time = 1.851s\n",
      "Epoch: 1211, Train loss: 3.466, Val loss: 3.286, Epoch time = 1.893s\n",
      "Epoch: 1212, Train loss: 3.045, Val loss: 2.771, Epoch time = 1.857s\n",
      "Epoch: 1213, Train loss: 2.848, Val loss: 3.146, Epoch time = 1.837s\n",
      "Epoch: 1214, Train loss: 2.762, Val loss: 3.145, Epoch time = 1.994s\n",
      "Epoch: 1215, Train loss: 2.747, Val loss: 3.239, Epoch time = 2.007s\n",
      "Epoch: 1216, Train loss: 3.510, Val loss: 3.131, Epoch time = 1.854s\n",
      "Epoch: 1217, Train loss: 2.883, Val loss: 2.904, Epoch time = 1.991s\n",
      "Epoch: 1218, Train loss: 3.069, Val loss: 2.757, Epoch time = 2.037s\n",
      "Epoch: 1219, Train loss: 2.905, Val loss: 3.145, Epoch time = 2.053s\n",
      "Epoch: 1220, Train loss: 3.099, Val loss: 2.967, Epoch time = 2.093s\n",
      "Epoch: 1221, Train loss: 2.644, Val loss: 2.968, Epoch time = 2.752s\n",
      "Epoch: 1222, Train loss: 2.948, Val loss: 2.777, Epoch time = 2.258s\n",
      "Epoch: 1223, Train loss: 2.790, Val loss: 3.356, Epoch time = 2.340s\n",
      "Epoch: 1224, Train loss: 3.487, Val loss: 2.953, Epoch time = 2.133s\n",
      "Epoch: 1225, Train loss: 2.806, Val loss: 2.868, Epoch time = 2.106s\n",
      "Epoch: 1226, Train loss: 2.694, Val loss: 3.221, Epoch time = 1.944s\n",
      "Epoch: 1227, Train loss: 3.073, Val loss: 3.052, Epoch time = 2.094s\n",
      "Epoch: 1228, Train loss: 2.746, Val loss: 3.224, Epoch time = 2.183s\n",
      "Epoch: 1229, Train loss: 3.439, Val loss: 2.712, Epoch time = 2.064s\n",
      "Epoch: 1230, Train loss: 2.668, Val loss: 2.525, Epoch time = 2.048s\n",
      "Epoch: 1231, Train loss: 3.196, Val loss: 2.833, Epoch time = 1.917s\n",
      "Epoch: 1232, Train loss: 2.933, Val loss: 3.825, Epoch time = 1.996s\n",
      "Epoch: 1233, Train loss: 3.552, Val loss: 3.258, Epoch time = 2.030s\n",
      "Epoch: 1234, Train loss: 3.536, Val loss: 3.439, Epoch time = 1.977s\n",
      "Epoch: 1235, Train loss: 3.314, Val loss: 2.901, Epoch time = 1.970s\n",
      "Epoch: 1236, Train loss: 3.424, Val loss: 3.038, Epoch time = 1.943s\n",
      "Epoch: 1237, Train loss: 2.895, Val loss: 2.882, Epoch time = 1.956s\n",
      "Epoch: 1238, Train loss: 2.828, Val loss: 2.814, Epoch time = 1.831s\n",
      "Epoch: 1239, Train loss: 3.149, Val loss: 3.051, Epoch time = 1.904s\n",
      "Epoch: 1240, Train loss: 3.017, Val loss: 3.010, Epoch time = 2.163s\n",
      "Epoch: 1241, Train loss: 4.092, Val loss: 2.874, Epoch time = 1.979s\n",
      "Epoch: 1242, Train loss: 2.678, Val loss: 2.906, Epoch time = 2.144s\n",
      "Epoch: 1243, Train loss: 2.985, Val loss: 3.084, Epoch time = 2.208s\n",
      "Epoch: 1244, Train loss: 2.884, Val loss: 2.660, Epoch time = 2.130s\n",
      "Epoch: 1245, Train loss: 2.875, Val loss: 2.792, Epoch time = 2.471s\n",
      "Epoch: 1246, Train loss: 3.319, Val loss: 3.851, Epoch time = 2.073s\n",
      "Epoch: 1247, Train loss: 2.981, Val loss: 2.959, Epoch time = 1.924s\n",
      "Epoch: 1248, Train loss: 2.979, Val loss: 3.002, Epoch time = 1.925s\n",
      "Epoch: 1249, Train loss: 3.066, Val loss: 3.154, Epoch time = 2.152s\n",
      "Epoch: 1250, Train loss: 2.680, Val loss: 2.916, Epoch time = 2.111s\n",
      "Epoch: 1251, Train loss: 3.668, Val loss: 3.226, Epoch time = 2.192s\n",
      "Epoch: 1252, Train loss: 3.261, Val loss: 2.920, Epoch time = 2.109s\n",
      "Epoch: 1253, Train loss: 2.896, Val loss: 3.364, Epoch time = 2.149s\n",
      "Epoch: 1254, Train loss: 2.768, Val loss: 3.105, Epoch time = 2.167s\n",
      "Epoch: 1255, Train loss: 3.191, Val loss: 3.101, Epoch time = 2.059s\n",
      "Epoch: 1256, Train loss: 3.588, Val loss: 3.883, Epoch time = 2.120s\n",
      "Epoch: 1257, Train loss: 2.945, Val loss: 3.233, Epoch time = 2.114s\n",
      "Epoch: 1258, Train loss: 3.360, Val loss: 3.506, Epoch time = 2.057s\n",
      "Epoch: 1259, Train loss: 2.767, Val loss: 2.746, Epoch time = 2.050s\n",
      "Epoch: 1260, Train loss: 2.938, Val loss: 2.939, Epoch time = 2.068s\n",
      "Epoch: 1261, Train loss: 3.031, Val loss: 2.875, Epoch time = 2.010s\n",
      "Epoch: 1262, Train loss: 2.895, Val loss: 3.026, Epoch time = 2.110s\n",
      "Epoch: 1263, Train loss: 3.442, Val loss: 2.889, Epoch time = 2.073s\n",
      "Epoch: 1264, Train loss: 2.954, Val loss: 2.958, Epoch time = 2.033s\n",
      "Epoch: 1265, Train loss: 3.188, Val loss: 3.333, Epoch time = 1.980s\n",
      "Epoch: 1266, Train loss: 3.010, Val loss: 3.012, Epoch time = 1.950s\n",
      "Epoch: 1267, Train loss: 2.891, Val loss: 3.320, Epoch time = 1.939s\n",
      "Epoch: 1268, Train loss: 3.246, Val loss: 2.717, Epoch time = 1.914s\n",
      "Epoch: 1269, Train loss: 3.326, Val loss: 2.814, Epoch time = 1.774s\n",
      "Epoch: 1270, Train loss: 3.381, Val loss: 3.245, Epoch time = 1.771s\n",
      "Epoch: 1271, Train loss: 3.341, Val loss: 2.867, Epoch time = 1.831s\n",
      "Epoch: 1272, Train loss: 3.107, Val loss: 2.811, Epoch time = 1.883s\n",
      "Epoch: 1273, Train loss: 2.811, Val loss: 3.316, Epoch time = 1.815s\n",
      "Epoch: 1274, Train loss: 2.919, Val loss: 2.785, Epoch time = 1.838s\n",
      "Epoch: 1275, Train loss: 3.735, Val loss: 3.764, Epoch time = 1.778s\n",
      "Epoch: 1276, Train loss: 2.661, Val loss: 3.193, Epoch time = 1.899s\n",
      "Epoch: 1277, Train loss: 2.845, Val loss: 3.418, Epoch time = 2.024s\n",
      "Epoch: 1278, Train loss: 2.843, Val loss: 2.841, Epoch time = 1.976s\n",
      "Epoch: 1279, Train loss: 2.908, Val loss: 2.958, Epoch time = 1.995s\n",
      "Epoch: 1280, Train loss: 2.800, Val loss: 2.763, Epoch time = 1.992s\n",
      "Epoch: 1281, Train loss: 2.557, Val loss: 2.918, Epoch time = 1.968s\n",
      "Epoch: 1282, Train loss: 2.673, Val loss: 3.057, Epoch time = 1.985s\n",
      "Epoch: 1283, Train loss: 2.621, Val loss: 3.515, Epoch time = 1.913s\n",
      "Epoch: 1284, Train loss: 2.765, Val loss: 3.200, Epoch time = 2.065s\n",
      "Epoch: 1285, Train loss: 3.235, Val loss: 3.107, Epoch time = 1.925s\n",
      "Epoch: 1286, Train loss: 3.052, Val loss: 2.870, Epoch time = 2.078s\n",
      "Epoch: 1287, Train loss: 3.871, Val loss: 2.910, Epoch time = 1.877s\n",
      "Epoch: 1288, Train loss: 3.411, Val loss: 2.848, Epoch time = 1.845s\n",
      "Epoch: 1289, Train loss: 2.938, Val loss: 2.593, Epoch time = 2.154s\n",
      "Epoch: 1290, Train loss: 3.048, Val loss: 3.151, Epoch time = 1.990s\n",
      "Epoch: 1291, Train loss: 3.540, Val loss: 2.794, Epoch time = 2.035s\n",
      "Epoch: 1292, Train loss: 2.797, Val loss: 3.401, Epoch time = 1.999s\n",
      "Epoch: 1293, Train loss: 3.109, Val loss: 3.496, Epoch time = 1.945s\n",
      "Epoch: 1294, Train loss: 3.381, Val loss: 2.773, Epoch time = 1.927s\n",
      "Epoch: 1295, Train loss: 2.887, Val loss: 2.762, Epoch time = 1.978s\n",
      "Epoch: 1296, Train loss: 2.947, Val loss: 2.640, Epoch time = 1.962s\n",
      "Epoch: 1297, Train loss: 2.749, Val loss: 3.578, Epoch time = 1.907s\n",
      "Epoch: 1298, Train loss: 3.244, Val loss: 3.075, Epoch time = 1.965s\n",
      "Epoch: 1299, Train loss: 3.507, Val loss: 2.527, Epoch time = 1.876s\n",
      "Epoch: 1300, Train loss: 3.279, Val loss: 2.735, Epoch time = 1.896s\n",
      "Epoch: 1301, Train loss: 2.761, Val loss: 2.949, Epoch time = 1.807s\n",
      "Epoch: 1302, Train loss: 2.701, Val loss: 2.687, Epoch time = 1.782s\n",
      "Epoch: 1303, Train loss: 2.561, Val loss: 3.299, Epoch time = 1.745s\n",
      "Epoch: 1304, Train loss: 3.073, Val loss: 3.591, Epoch time = 1.789s\n",
      "Epoch: 1305, Train loss: 2.756, Val loss: 2.990, Epoch time = 1.815s\n",
      "Epoch: 1306, Train loss: 3.494, Val loss: 2.776, Epoch time = 1.854s\n",
      "Epoch: 1307, Train loss: 2.795, Val loss: 2.968, Epoch time = 1.981s\n",
      "Epoch: 1308, Train loss: 3.372, Val loss: 2.800, Epoch time = 1.856s\n",
      "Epoch: 1309, Train loss: 3.070, Val loss: 3.566, Epoch time = 1.827s\n",
      "Epoch: 1310, Train loss: 2.925, Val loss: 3.017, Epoch time = 1.940s\n",
      "Epoch: 1311, Train loss: 2.962, Val loss: 2.782, Epoch time = 1.955s\n",
      "Epoch: 1312, Train loss: 3.477, Val loss: 2.775, Epoch time = 1.910s\n",
      "Epoch: 1313, Train loss: 2.854, Val loss: 3.444, Epoch time = 1.986s\n",
      "Epoch: 1314, Train loss: 2.717, Val loss: 3.102, Epoch time = 1.884s\n",
      "Epoch: 1315, Train loss: 3.397, Val loss: 2.846, Epoch time = 1.716s\n",
      "Epoch: 1316, Train loss: 2.936, Val loss: 3.091, Epoch time = 1.891s\n",
      "Epoch: 1317, Train loss: 2.870, Val loss: 3.038, Epoch time = 1.941s\n",
      "Epoch: 1318, Train loss: 3.148, Val loss: 2.677, Epoch time = 1.896s\n",
      "Epoch: 1319, Train loss: 3.369, Val loss: 2.933, Epoch time = 1.919s\n",
      "Epoch: 1320, Train loss: 2.764, Val loss: 3.102, Epoch time = 1.903s\n",
      "Epoch: 1321, Train loss: 2.920, Val loss: 3.405, Epoch time = 1.954s\n",
      "Epoch: 1322, Train loss: 2.979, Val loss: 2.749, Epoch time = 2.161s\n",
      "Epoch: 1323, Train loss: 3.117, Val loss: 3.200, Epoch time = 1.851s\n",
      "Epoch: 1324, Train loss: 3.601, Val loss: 3.354, Epoch time = 1.770s\n",
      "Epoch: 1325, Train loss: 2.685, Val loss: 2.912, Epoch time = 1.799s\n",
      "Epoch: 1326, Train loss: 3.158, Val loss: 3.173, Epoch time = 1.780s\n",
      "Epoch: 1327, Train loss: 2.916, Val loss: 2.631, Epoch time = 1.761s\n",
      "Epoch: 1328, Train loss: 3.711, Val loss: 3.006, Epoch time = 1.835s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1329, Train loss: 2.911, Val loss: 3.002, Epoch time = 1.860s\n",
      "Epoch: 1330, Train loss: 2.690, Val loss: 2.831, Epoch time = 1.783s\n",
      "Epoch: 1331, Train loss: 2.865, Val loss: 3.049, Epoch time = 1.883s\n",
      "Epoch: 1332, Train loss: 2.724, Val loss: 3.051, Epoch time = 1.981s\n",
      "Epoch: 1333, Train loss: 3.029, Val loss: 2.870, Epoch time = 1.944s\n",
      "Epoch: 1334, Train loss: 2.966, Val loss: 2.577, Epoch time = 1.944s\n",
      "Epoch: 1335, Train loss: 2.830, Val loss: 2.825, Epoch time = 2.319s\n",
      "Epoch: 1336, Train loss: 3.023, Val loss: 2.842, Epoch time = 1.990s\n",
      "Epoch: 1337, Train loss: 2.748, Val loss: 2.578, Epoch time = 2.010s\n",
      "Epoch: 1338, Train loss: 2.642, Val loss: 3.258, Epoch time = 1.844s\n",
      "Epoch: 1339, Train loss: 3.097, Val loss: 2.872, Epoch time = 1.772s\n",
      "Epoch: 1340, Train loss: 2.725, Val loss: 3.075, Epoch time = 1.855s\n",
      "Epoch: 1341, Train loss: 3.355, Val loss: 3.091, Epoch time = 1.826s\n",
      "Epoch: 1342, Train loss: 3.005, Val loss: 3.229, Epoch time = 1.927s\n",
      "Epoch: 1343, Train loss: 3.724, Val loss: 3.398, Epoch time = 2.094s\n",
      "Epoch: 1344, Train loss: 2.971, Val loss: 3.070, Epoch time = 2.164s\n",
      "Epoch: 1345, Train loss: 3.175, Val loss: 3.474, Epoch time = 1.965s\n",
      "Epoch: 1346, Train loss: 2.641, Val loss: 3.006, Epoch time = 1.683s\n",
      "Epoch: 1347, Train loss: 3.168, Val loss: 2.987, Epoch time = 1.649s\n",
      "Epoch: 1348, Train loss: 3.429, Val loss: 3.112, Epoch time = 1.611s\n",
      "Epoch: 1349, Train loss: 2.835, Val loss: 3.107, Epoch time = 1.753s\n",
      "Epoch: 1350, Train loss: 2.669, Val loss: 3.101, Epoch time = 1.690s\n",
      "Epoch: 1351, Train loss: 2.777, Val loss: 3.713, Epoch time = 1.691s\n",
      "Epoch: 1352, Train loss: 3.137, Val loss: 2.905, Epoch time = 1.711s\n",
      "Epoch: 1353, Train loss: 2.973, Val loss: 2.939, Epoch time = 1.716s\n",
      "Epoch: 1354, Train loss: 3.553, Val loss: 2.973, Epoch time = 1.689s\n",
      "Epoch: 1355, Train loss: 2.899, Val loss: 3.032, Epoch time = 1.985s\n",
      "Epoch: 1356, Train loss: 2.969, Val loss: 3.140, Epoch time = 1.696s\n",
      "Epoch: 1357, Train loss: 2.744, Val loss: 2.879, Epoch time = 1.669s\n",
      "Epoch: 1358, Train loss: 2.764, Val loss: 4.243, Epoch time = 1.711s\n",
      "Epoch: 1359, Train loss: 2.800, Val loss: 2.696, Epoch time = 1.509s\n",
      "Epoch: 1360, Train loss: 2.934, Val loss: 3.404, Epoch time = 2.183s\n",
      "Epoch: 1361, Train loss: 2.944, Val loss: 2.988, Epoch time = 1.897s\n",
      "Epoch: 1362, Train loss: 2.687, Val loss: 3.222, Epoch time = 1.865s\n",
      "Epoch: 1363, Train loss: 3.727, Val loss: 3.737, Epoch time = 1.853s\n",
      "Epoch: 1364, Train loss: 2.700, Val loss: 2.846, Epoch time = 1.822s\n",
      "Epoch: 1365, Train loss: 2.927, Val loss: 3.468, Epoch time = 1.599s\n",
      "Epoch: 1366, Train loss: 3.126, Val loss: 2.945, Epoch time = 1.594s\n",
      "Epoch: 1367, Train loss: 2.946, Val loss: 2.850, Epoch time = 1.565s\n",
      "Epoch: 1368, Train loss: 2.647, Val loss: 2.811, Epoch time = 1.571s\n",
      "Epoch: 1369, Train loss: 2.947, Val loss: 3.483, Epoch time = 1.610s\n",
      "Epoch: 1370, Train loss: 3.117, Val loss: 3.070, Epoch time = 1.550s\n",
      "Epoch: 1371, Train loss: 2.951, Val loss: 3.069, Epoch time = 1.564s\n",
      "Epoch: 1372, Train loss: 3.049, Val loss: 2.949, Epoch time = 1.696s\n",
      "Epoch: 1373, Train loss: 3.065, Val loss: 3.875, Epoch time = 1.816s\n",
      "Epoch: 1374, Train loss: 2.592, Val loss: 2.816, Epoch time = 1.631s\n",
      "Epoch: 1375, Train loss: 2.835, Val loss: 3.403, Epoch time = 1.682s\n",
      "Epoch: 1376, Train loss: 2.795, Val loss: 2.935, Epoch time = 1.606s\n",
      "Epoch: 1377, Train loss: 2.772, Val loss: 2.706, Epoch time = 1.691s\n",
      "Epoch: 1378, Train loss: 3.201, Val loss: 3.018, Epoch time = 1.842s\n",
      "Epoch: 1379, Train loss: 2.957, Val loss: 3.207, Epoch time = 1.707s\n",
      "Epoch: 1380, Train loss: 2.997, Val loss: 3.145, Epoch time = 1.633s\n",
      "Epoch: 1381, Train loss: 2.790, Val loss: 3.010, Epoch time = 1.596s\n",
      "Epoch: 1382, Train loss: 3.005, Val loss: 3.026, Epoch time = 1.734s\n",
      "Epoch: 1383, Train loss: 2.902, Val loss: 3.691, Epoch time = 1.580s\n",
      "Epoch: 1384, Train loss: 2.865, Val loss: 2.686, Epoch time = 1.567s\n",
      "Epoch: 1385, Train loss: 2.926, Val loss: 3.007, Epoch time = 1.546s\n",
      "Epoch: 1386, Train loss: 2.882, Val loss: 3.462, Epoch time = 1.567s\n",
      "Epoch: 1387, Train loss: 2.765, Val loss: 3.240, Epoch time = 1.570s\n",
      "Epoch: 1388, Train loss: 3.111, Val loss: 3.039, Epoch time = 1.519s\n",
      "Epoch: 1389, Train loss: 2.855, Val loss: 3.122, Epoch time = 1.582s\n",
      "Epoch: 1390, Train loss: 2.550, Val loss: 2.904, Epoch time = 1.712s\n",
      "Epoch: 1391, Train loss: 2.882, Val loss: 3.272, Epoch time = 1.687s\n",
      "Epoch: 1392, Train loss: 3.785, Val loss: 2.867, Epoch time = 2.472s\n",
      "Epoch: 1393, Train loss: 2.775, Val loss: 3.009, Epoch time = 1.806s\n",
      "Epoch: 1394, Train loss: 3.471, Val loss: 2.838, Epoch time = 1.644s\n",
      "Epoch: 1395, Train loss: 3.450, Val loss: 2.805, Epoch time = 1.590s\n",
      "Epoch: 1396, Train loss: 2.652, Val loss: 3.158, Epoch time = 1.685s\n",
      "Epoch: 1397, Train loss: 3.058, Val loss: 3.442, Epoch time = 1.591s\n",
      "Epoch: 1398, Train loss: 3.212, Val loss: 3.116, Epoch time = 1.582s\n",
      "Epoch: 1399, Train loss: 3.406, Val loss: 2.893, Epoch time = 1.678s\n",
      "Epoch: 1400, Train loss: 2.941, Val loss: 2.872, Epoch time = 1.653s\n",
      "Epoch: 1401, Train loss: 3.532, Val loss: 3.320, Epoch time = 1.581s\n",
      "Epoch: 1402, Train loss: 3.300, Val loss: 2.732, Epoch time = 1.636s\n",
      "Epoch: 1403, Train loss: 3.529, Val loss: 3.138, Epoch time = 1.612s\n",
      "Epoch: 1404, Train loss: 3.041, Val loss: 2.854, Epoch time = 1.716s\n",
      "Epoch: 1405, Train loss: 3.032, Val loss: 2.722, Epoch time = 1.691s\n",
      "Epoch: 1406, Train loss: 3.200, Val loss: 2.780, Epoch time = 1.602s\n",
      "Epoch: 1407, Train loss: 3.185, Val loss: 3.217, Epoch time = 1.622s\n",
      "Epoch: 1408, Train loss: 2.728, Val loss: 3.179, Epoch time = 1.615s\n",
      "Epoch: 1409, Train loss: 2.946, Val loss: 2.881, Epoch time = 1.762s\n",
      "Epoch: 1410, Train loss: 2.787, Val loss: 3.114, Epoch time = 1.532s\n",
      "Epoch: 1411, Train loss: 2.966, Val loss: 3.118, Epoch time = 1.554s\n",
      "Epoch: 1412, Train loss: 3.666, Val loss: 2.688, Epoch time = 1.559s\n",
      "Epoch: 1413, Train loss: 3.450, Val loss: 3.661, Epoch time = 1.524s\n",
      "Epoch: 1414, Train loss: 3.398, Val loss: 2.891, Epoch time = 1.525s\n",
      "Epoch: 1415, Train loss: 3.634, Val loss: 2.911, Epoch time = 1.682s\n",
      "Epoch: 1416, Train loss: 2.673, Val loss: 2.649, Epoch time = 1.579s\n",
      "Epoch: 1417, Train loss: 2.779, Val loss: 2.953, Epoch time = 1.584s\n",
      "Epoch: 1418, Train loss: 2.905, Val loss: 2.923, Epoch time = 1.547s\n",
      "Epoch: 1419, Train loss: 3.044, Val loss: 3.434, Epoch time = 1.649s\n",
      "Epoch: 1420, Train loss: 2.683, Val loss: 3.235, Epoch time = 1.722s\n",
      "Epoch: 1421, Train loss: 2.631, Val loss: 2.493, Epoch time = 1.757s\n",
      "Epoch: 1422, Train loss: 2.882, Val loss: 3.311, Epoch time = 1.671s\n",
      "Epoch: 1423, Train loss: 3.181, Val loss: 2.891, Epoch time = 1.927s\n",
      "Epoch: 1424, Train loss: 3.115, Val loss: 2.911, Epoch time = 1.593s\n",
      "Epoch: 1425, Train loss: 2.879, Val loss: 3.093, Epoch time = 1.708s\n",
      "Epoch: 1426, Train loss: 3.411, Val loss: 2.839, Epoch time = 1.718s\n",
      "Epoch: 1427, Train loss: 2.919, Val loss: 3.270, Epoch time = 1.617s\n",
      "Epoch: 1428, Train loss: 2.798, Val loss: 2.972, Epoch time = 1.527s\n",
      "Epoch: 1429, Train loss: 3.383, Val loss: 2.641, Epoch time = 1.579s\n",
      "Epoch: 1430, Train loss: 2.880, Val loss: 2.947, Epoch time = 1.526s\n",
      "Epoch: 1431, Train loss: 3.412, Val loss: 3.452, Epoch time = 1.545s\n",
      "Epoch: 1432, Train loss: 2.905, Val loss: 2.854, Epoch time = 1.513s\n",
      "Epoch: 1433, Train loss: 2.875, Val loss: 2.865, Epoch time = 1.556s\n",
      "Epoch: 1434, Train loss: 2.842, Val loss: 2.936, Epoch time = 1.648s\n",
      "Epoch: 1435, Train loss: 3.283, Val loss: 2.802, Epoch time = 1.613s\n",
      "Epoch: 1436, Train loss: 2.883, Val loss: 3.294, Epoch time = 1.582s\n",
      "Epoch: 1437, Train loss: 3.544, Val loss: 3.166, Epoch time = 1.660s\n",
      "Epoch: 1438, Train loss: 2.837, Val loss: 3.222, Epoch time = 1.660s\n",
      "Epoch: 1439, Train loss: 2.810, Val loss: 3.488, Epoch time = 1.600s\n",
      "Epoch: 1440, Train loss: 2.991, Val loss: 2.865, Epoch time = 1.660s\n",
      "Epoch: 1441, Train loss: 2.919, Val loss: 2.985, Epoch time = 1.641s\n",
      "Epoch: 1442, Train loss: 2.780, Val loss: 3.238, Epoch time = 1.615s\n",
      "Epoch: 1443, Train loss: 3.158, Val loss: 3.046, Epoch time = 1.559s\n",
      "Epoch: 1444, Train loss: 2.863, Val loss: 3.074, Epoch time = 1.539s\n",
      "Epoch: 1445, Train loss: 3.135, Val loss: 3.116, Epoch time = 1.579s\n",
      "Epoch: 1446, Train loss: 3.609, Val loss: 2.776, Epoch time = 1.525s\n",
      "Epoch: 1447, Train loss: 2.739, Val loss: 3.002, Epoch time = 1.672s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1448, Train loss: 2.769, Val loss: 3.179, Epoch time = 1.636s\n",
      "Epoch: 1449, Train loss: 3.019, Val loss: 3.302, Epoch time = 1.531s\n",
      "Epoch: 1450, Train loss: 2.809, Val loss: 2.913, Epoch time = 1.542s\n",
      "Epoch: 1451, Train loss: 3.906, Val loss: 2.889, Epoch time = 1.921s\n",
      "Epoch: 1452, Train loss: 3.532, Val loss: 2.562, Epoch time = 1.595s\n",
      "Epoch: 1453, Train loss: 3.195, Val loss: 2.925, Epoch time = 1.617s\n",
      "Epoch: 1454, Train loss: 2.880, Val loss: 2.939, Epoch time = 1.540s\n",
      "Epoch: 1455, Train loss: 2.744, Val loss: 3.523, Epoch time = 1.542s\n",
      "Epoch: 1456, Train loss: 3.029, Val loss: 3.252, Epoch time = 1.507s\n",
      "Epoch: 1457, Train loss: 3.770, Val loss: 2.670, Epoch time = 1.589s\n",
      "Epoch: 1458, Train loss: 2.951, Val loss: 2.873, Epoch time = 1.547s\n",
      "Epoch: 1459, Train loss: 2.806, Val loss: 3.089, Epoch time = 1.617s\n",
      "Epoch: 1460, Train loss: 3.043, Val loss: 3.530, Epoch time = 1.595s\n",
      "Epoch: 1461, Train loss: 3.790, Val loss: 2.632, Epoch time = 1.543s\n",
      "Epoch: 1462, Train loss: 2.795, Val loss: 2.893, Epoch time = 1.549s\n",
      "Epoch: 1463, Train loss: 3.076, Val loss: 3.745, Epoch time = 1.533s\n",
      "Epoch: 1464, Train loss: 3.332, Val loss: 3.059, Epoch time = 1.790s\n",
      "Epoch: 1465, Train loss: 2.850, Val loss: 3.165, Epoch time = 1.532s\n",
      "Epoch: 1466, Train loss: 2.861, Val loss: 3.414, Epoch time = 1.572s\n",
      "Epoch: 1467, Train loss: 2.977, Val loss: 3.184, Epoch time = 1.558s\n",
      "Epoch: 1468, Train loss: 2.950, Val loss: 3.127, Epoch time = 1.635s\n",
      "Epoch: 1469, Train loss: 3.195, Val loss: 3.166, Epoch time = 1.705s\n",
      "Epoch: 1470, Train loss: 2.641, Val loss: 2.744, Epoch time = 1.620s\n",
      "Epoch: 1471, Train loss: 2.962, Val loss: 2.810, Epoch time = 1.699s\n",
      "Epoch: 1472, Train loss: 2.922, Val loss: 3.245, Epoch time = 1.656s\n",
      "Epoch: 1473, Train loss: 2.467, Val loss: 2.848, Epoch time = 1.680s\n",
      "Epoch: 1474, Train loss: 3.490, Val loss: 2.863, Epoch time = 1.675s\n",
      "Epoch: 1475, Train loss: 3.067, Val loss: 2.677, Epoch time = 1.631s\n",
      "Epoch: 1476, Train loss: 2.720, Val loss: 3.538, Epoch time = 1.545s\n",
      "Epoch: 1477, Train loss: 2.747, Val loss: 2.890, Epoch time = 1.519s\n",
      "Epoch: 1478, Train loss: 2.946, Val loss: 2.883, Epoch time = 1.545s\n",
      "Epoch: 1479, Train loss: 2.894, Val loss: 3.315, Epoch time = 1.502s\n",
      "Epoch: 1480, Train loss: 2.792, Val loss: 3.636, Epoch time = 1.474s\n",
      "Epoch: 1481, Train loss: 2.851, Val loss: 3.531, Epoch time = 1.552s\n",
      "Epoch: 1482, Train loss: 2.688, Val loss: 2.986, Epoch time = 1.547s\n",
      "Epoch: 1483, Train loss: 3.224, Val loss: 2.550, Epoch time = 1.562s\n",
      "Epoch: 1484, Train loss: 2.776, Val loss: 3.351, Epoch time = 1.543s\n",
      "Epoch: 1485, Train loss: 3.416, Val loss: 3.284, Epoch time = 1.561s\n",
      "Epoch: 1486, Train loss: 3.446, Val loss: 2.637, Epoch time = 1.524s\n",
      "Epoch: 1487, Train loss: 3.372, Val loss: 3.072, Epoch time = 1.528s\n",
      "Epoch: 1488, Train loss: 3.409, Val loss: 2.838, Epoch time = 1.631s\n",
      "Epoch: 1489, Train loss: 3.174, Val loss: 3.277, Epoch time = 1.582s\n",
      "Epoch: 1490, Train loss: 2.840, Val loss: 2.833, Epoch time = 1.598s\n",
      "Epoch: 1491, Train loss: 2.806, Val loss: 2.979, Epoch time = 1.583s\n",
      "Epoch: 1492, Train loss: 2.722, Val loss: 2.639, Epoch time = 1.502s\n",
      "Epoch: 1493, Train loss: 3.104, Val loss: 3.043, Epoch time = 1.522s\n",
      "Epoch: 1494, Train loss: 2.877, Val loss: 3.237, Epoch time = 1.491s\n",
      "Epoch: 1495, Train loss: 2.876, Val loss: 2.742, Epoch time = 1.557s\n",
      "Epoch: 1496, Train loss: 3.121, Val loss: 3.009, Epoch time = 1.604s\n",
      "Epoch: 1497, Train loss: 3.173, Val loss: 3.218, Epoch time = 1.647s\n",
      "Epoch: 1498, Train loss: 3.035, Val loss: 2.876, Epoch time = 1.555s\n",
      "Epoch: 1499, Train loss: 2.973, Val loss: 2.970, Epoch time = 1.519s\n",
      "Epoch: 1500, Train loss: 2.859, Val loss: 3.171, Epoch time = 1.547s\n",
      "Epoch: 1501, Train loss: 2.790, Val loss: 2.864, Epoch time = 1.511s\n",
      "Epoch: 1502, Train loss: 3.275, Val loss: 2.632, Epoch time = 1.489s\n",
      "Epoch: 1503, Train loss: 3.188, Val loss: 3.526, Epoch time = 1.519s\n",
      "Epoch: 1504, Train loss: 3.009, Val loss: 2.797, Epoch time = 1.546s\n",
      "Epoch: 1505, Train loss: 3.004, Val loss: 3.484, Epoch time = 1.538s\n",
      "Epoch: 1506, Train loss: 2.782, Val loss: 2.803, Epoch time = 1.545s\n",
      "Epoch: 1507, Train loss: 3.002, Val loss: 2.924, Epoch time = 1.515s\n",
      "Epoch: 1508, Train loss: 3.555, Val loss: 2.651, Epoch time = 1.603s\n",
      "Epoch: 1509, Train loss: 3.484, Val loss: 2.848, Epoch time = 1.583s\n",
      "Epoch: 1510, Train loss: 3.609, Val loss: 2.642, Epoch time = 1.540s\n",
      "Epoch: 1511, Train loss: 2.796, Val loss: 3.004, Epoch time = 1.582s\n",
      "Epoch: 1512, Train loss: 2.900, Val loss: 2.782, Epoch time = 1.593s\n",
      "Epoch: 1513, Train loss: 2.892, Val loss: 2.520, Epoch time = 1.565s\n",
      "Epoch: 1514, Train loss: 3.229, Val loss: 2.658, Epoch time = 1.552s\n",
      "Epoch: 1515, Train loss: 3.381, Val loss: 2.848, Epoch time = 1.580s\n",
      "Epoch: 1516, Train loss: 3.286, Val loss: 3.019, Epoch time = 1.605s\n",
      "Epoch: 1517, Train loss: 2.692, Val loss: 3.031, Epoch time = 1.562s\n",
      "Epoch: 1518, Train loss: 2.886, Val loss: 3.150, Epoch time = 1.593s\n",
      "Epoch: 1519, Train loss: 2.526, Val loss: 3.070, Epoch time = 1.611s\n",
      "Epoch: 1520, Train loss: 2.920, Val loss: 3.100, Epoch time = 1.595s\n",
      "Epoch: 1521, Train loss: 3.348, Val loss: 3.044, Epoch time = 1.551s\n",
      "Epoch: 1522, Train loss: 3.322, Val loss: 2.696, Epoch time = 1.534s\n",
      "Epoch: 1523, Train loss: 2.843, Val loss: 2.907, Epoch time = 1.532s\n",
      "Epoch: 1524, Train loss: 3.070, Val loss: 3.506, Epoch time = 1.530s\n",
      "Epoch: 1525, Train loss: 2.641, Val loss: 2.898, Epoch time = 1.506s\n",
      "Epoch: 1526, Train loss: 3.790, Val loss: 2.928, Epoch time = 1.465s\n",
      "Epoch: 1527, Train loss: 2.767, Val loss: 3.191, Epoch time = 1.555s\n",
      "Epoch: 1528, Train loss: 2.808, Val loss: 3.493, Epoch time = 1.621s\n",
      "Epoch: 1529, Train loss: 2.948, Val loss: 2.602, Epoch time = 1.595s\n",
      "Epoch: 1530, Train loss: 2.938, Val loss: 3.016, Epoch time = 1.593s\n",
      "Epoch: 1531, Train loss: 3.520, Val loss: 2.539, Epoch time = 1.546s\n",
      "Epoch: 1532, Train loss: 2.822, Val loss: 2.633, Epoch time = 1.511s\n",
      "Epoch: 1533, Train loss: 2.794, Val loss: 2.709, Epoch time = 1.594s\n",
      "Epoch: 1534, Train loss: 3.190, Val loss: 3.296, Epoch time = 1.544s\n",
      "Epoch: 1535, Train loss: 2.962, Val loss: 2.933, Epoch time = 1.538s\n",
      "Epoch: 1536, Train loss: 2.725, Val loss: 3.012, Epoch time = 1.523s\n",
      "Epoch: 1537, Train loss: 2.914, Val loss: 3.373, Epoch time = 1.525s\n",
      "Epoch: 1538, Train loss: 2.956, Val loss: 2.683, Epoch time = 1.519s\n",
      "Epoch: 1539, Train loss: 2.963, Val loss: 2.967, Epoch time = 1.595s\n",
      "Epoch: 1540, Train loss: 2.972, Val loss: 3.266, Epoch time = 1.616s\n",
      "Epoch: 1541, Train loss: 2.885, Val loss: 2.700, Epoch time = 1.641s\n",
      "Epoch: 1542, Train loss: 3.242, Val loss: 3.309, Epoch time = 1.568s\n",
      "Epoch: 1543, Train loss: 3.051, Val loss: 3.277, Epoch time = 1.617s\n",
      "Epoch: 1544, Train loss: 3.435, Val loss: 2.780, Epoch time = 1.552s\n",
      "Epoch: 1545, Train loss: 3.439, Val loss: 3.335, Epoch time = 1.490s\n",
      "Epoch: 1546, Train loss: 3.151, Val loss: 2.646, Epoch time = 1.588s\n",
      "Epoch: 1547, Train loss: 3.496, Val loss: 2.548, Epoch time = 1.572s\n",
      "Epoch: 1548, Train loss: 3.156, Val loss: 3.124, Epoch time = 1.507s\n",
      "Epoch: 1549, Train loss: 2.801, Val loss: 2.878, Epoch time = 1.513s\n",
      "Epoch: 1550, Train loss: 3.019, Val loss: 2.842, Epoch time = 1.501s\n",
      "Epoch: 1551, Train loss: 2.712, Val loss: 3.051, Epoch time = 1.551s\n",
      "Epoch: 1552, Train loss: 2.777, Val loss: 3.575, Epoch time = 1.605s\n",
      "Epoch: 1553, Train loss: 2.700, Val loss: 2.788, Epoch time = 1.575s\n",
      "Epoch: 1554, Train loss: 2.783, Val loss: 2.934, Epoch time = 1.536s\n",
      "Epoch: 1555, Train loss: 3.596, Val loss: 2.756, Epoch time = 1.501s\n",
      "Epoch: 1556, Train loss: 3.147, Val loss: 2.821, Epoch time = 1.504s\n",
      "Epoch: 1557, Train loss: 3.343, Val loss: 2.984, Epoch time = 1.507s\n",
      "Epoch: 1558, Train loss: 3.187, Val loss: 3.049, Epoch time = 1.513s\n",
      "Epoch: 1559, Train loss: 3.583, Val loss: 2.927, Epoch time = 1.515s\n",
      "Epoch: 1560, Train loss: 3.140, Val loss: 2.964, Epoch time = 1.617s\n",
      "Epoch: 1561, Train loss: 3.037, Val loss: 2.983, Epoch time = 1.493s\n",
      "Epoch: 1562, Train loss: 2.974, Val loss: 2.939, Epoch time = 1.545s\n",
      "Epoch: 1563, Train loss: 3.320, Val loss: 3.077, Epoch time = 1.626s\n",
      "Epoch: 1564, Train loss: 3.130, Val loss: 2.999, Epoch time = 1.856s\n",
      "Epoch: 1565, Train loss: 2.801, Val loss: 3.209, Epoch time = 2.438s\n",
      "Epoch: 1566, Train loss: 3.026, Val loss: 2.608, Epoch time = 2.024s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1567, Train loss: 3.084, Val loss: 3.087, Epoch time = 2.140s\n",
      "Epoch: 1568, Train loss: 2.963, Val loss: 2.650, Epoch time = 2.086s\n",
      "Epoch: 1569, Train loss: 2.974, Val loss: 2.774, Epoch time = 1.781s\n",
      "Epoch: 1570, Train loss: 2.902, Val loss: 2.605, Epoch time = 1.955s\n",
      "Epoch: 1571, Train loss: 3.340, Val loss: 2.969, Epoch time = 1.985s\n",
      "Epoch: 1572, Train loss: 3.245, Val loss: 3.193, Epoch time = 1.982s\n",
      "Epoch: 1573, Train loss: 2.710, Val loss: 2.766, Epoch time = 2.043s\n",
      "Epoch: 1574, Train loss: 3.213, Val loss: 4.187, Epoch time = 2.041s\n",
      "Epoch: 1575, Train loss: 3.050, Val loss: 2.905, Epoch time = 2.008s\n",
      "Epoch: 1576, Train loss: 3.033, Val loss: 3.157, Epoch time = 1.931s\n",
      "Epoch: 1577, Train loss: 3.010, Val loss: 3.368, Epoch time = 2.055s\n",
      "Epoch: 1578, Train loss: 2.921, Val loss: 3.156, Epoch time = 2.003s\n",
      "Epoch: 1579, Train loss: 3.044, Val loss: 2.958, Epoch time = 2.122s\n",
      "Epoch: 1580, Train loss: 2.608, Val loss: 3.740, Epoch time = 2.025s\n",
      "Epoch: 1581, Train loss: 3.343, Val loss: 2.900, Epoch time = 1.980s\n",
      "Epoch: 1582, Train loss: 3.407, Val loss: 3.025, Epoch time = 1.766s\n",
      "Epoch: 1583, Train loss: 2.761, Val loss: 2.741, Epoch time = 1.959s\n",
      "Epoch: 1584, Train loss: 3.106, Val loss: 2.688, Epoch time = 1.880s\n",
      "Epoch: 1585, Train loss: 3.040, Val loss: 2.656, Epoch time = 1.886s\n",
      "Epoch: 1586, Train loss: 3.013, Val loss: 2.755, Epoch time = 1.775s\n",
      "Epoch: 1587, Train loss: 2.810, Val loss: 3.210, Epoch time = 1.829s\n",
      "Epoch: 1588, Train loss: 2.797, Val loss: 3.013, Epoch time = 1.823s\n",
      "Epoch: 1589, Train loss: 2.490, Val loss: 2.917, Epoch time = 1.805s\n",
      "Epoch: 1590, Train loss: 3.483, Val loss: 3.190, Epoch time = 1.907s\n",
      "Epoch: 1591, Train loss: 3.082, Val loss: 3.412, Epoch time = 1.840s\n",
      "Epoch: 1592, Train loss: 3.248, Val loss: 2.841, Epoch time = 1.779s\n",
      "Epoch: 1593, Train loss: 2.934, Val loss: 3.021, Epoch time = 1.812s\n",
      "Epoch: 1594, Train loss: 2.816, Val loss: 2.821, Epoch time = 1.981s\n",
      "Epoch: 1595, Train loss: 2.564, Val loss: 2.871, Epoch time = 1.968s\n",
      "Epoch: 1596, Train loss: 2.744, Val loss: 2.487, Epoch time = 2.126s\n",
      "Epoch: 1597, Train loss: 3.083, Val loss: 2.847, Epoch time = 2.022s\n",
      "Epoch: 1598, Train loss: 3.313, Val loss: 2.790, Epoch time = 2.117s\n",
      "Epoch: 1599, Train loss: 3.038, Val loss: 2.640, Epoch time = 2.018s\n",
      "Epoch: 1600, Train loss: 3.168, Val loss: 2.789, Epoch time = 2.018s\n",
      "Epoch: 1601, Train loss: 2.580, Val loss: 2.774, Epoch time = 1.974s\n",
      "Epoch: 1602, Train loss: 3.057, Val loss: 2.743, Epoch time = 2.025s\n",
      "Epoch: 1603, Train loss: 3.531, Val loss: 2.946, Epoch time = 2.073s\n",
      "Epoch: 1604, Train loss: 3.556, Val loss: 3.393, Epoch time = 2.019s\n",
      "Epoch: 1605, Train loss: 2.451, Val loss: 3.308, Epoch time = 1.974s\n",
      "Epoch: 1606, Train loss: 2.689, Val loss: 2.946, Epoch time = 1.829s\n",
      "Epoch: 1607, Train loss: 3.045, Val loss: 3.365, Epoch time = 1.876s\n",
      "Epoch: 1608, Train loss: 2.726, Val loss: 2.879, Epoch time = 2.155s\n",
      "Epoch: 1609, Train loss: 2.873, Val loss: 2.968, Epoch time = 1.974s\n",
      "Epoch: 1610, Train loss: 3.980, Val loss: 2.915, Epoch time = 1.840s\n",
      "Epoch: 1611, Train loss: 2.801, Val loss: 2.950, Epoch time = 1.844s\n",
      "Epoch: 1612, Train loss: 2.874, Val loss: 2.846, Epoch time = 1.873s\n",
      "Epoch: 1613, Train loss: 2.765, Val loss: 3.200, Epoch time = 1.908s\n",
      "Epoch: 1614, Train loss: 3.159, Val loss: 2.953, Epoch time = 2.039s\n",
      "Epoch: 1615, Train loss: 2.654, Val loss: 3.453, Epoch time = 1.811s\n",
      "Epoch: 1616, Train loss: 3.232, Val loss: 2.851, Epoch time = 1.868s\n",
      "Epoch: 1617, Train loss: 2.917, Val loss: 2.868, Epoch time = 2.062s\n",
      "Epoch: 1618, Train loss: 3.266, Val loss: 3.089, Epoch time = 1.987s\n",
      "Epoch: 1619, Train loss: 3.012, Val loss: 2.591, Epoch time = 2.002s\n",
      "Epoch: 1620, Train loss: 3.270, Val loss: 3.158, Epoch time = 1.949s\n",
      "Epoch: 1621, Train loss: 2.966, Val loss: 3.440, Epoch time = 1.957s\n",
      "Epoch: 1622, Train loss: 2.963, Val loss: 2.823, Epoch time = 2.130s\n",
      "Epoch: 1623, Train loss: 2.917, Val loss: 3.317, Epoch time = 2.187s\n",
      "Epoch: 1624, Train loss: 3.455, Val loss: 3.162, Epoch time = 2.005s\n",
      "Epoch: 1625, Train loss: 2.899, Val loss: 2.876, Epoch time = 1.887s\n",
      "Epoch: 1626, Train loss: 2.810, Val loss: 2.976, Epoch time = 1.867s\n",
      "Epoch: 1627, Train loss: 3.182, Val loss: 2.641, Epoch time = 1.871s\n",
      "Epoch: 1628, Train loss: 3.459, Val loss: 2.989, Epoch time = 1.926s\n",
      "Epoch: 1629, Train loss: 2.712, Val loss: 3.806, Epoch time = 1.914s\n",
      "Epoch: 1630, Train loss: 3.042, Val loss: 2.845, Epoch time = 1.854s\n",
      "Epoch: 1631, Train loss: 3.057, Val loss: 2.781, Epoch time = 1.899s\n",
      "Epoch: 1632, Train loss: 2.751, Val loss: 3.179, Epoch time = 1.833s\n",
      "Epoch: 1633, Train loss: 3.193, Val loss: 3.532, Epoch time = 1.921s\n",
      "Epoch: 1634, Train loss: 2.593, Val loss: 2.716, Epoch time = 1.840s\n",
      "Epoch: 1635, Train loss: 3.315, Val loss: 3.530, Epoch time = 1.890s\n",
      "Epoch: 1636, Train loss: 3.065, Val loss: 2.494, Epoch time = 2.048s\n",
      "Epoch: 1637, Train loss: 2.975, Val loss: 3.700, Epoch time = 1.843s\n",
      "Epoch: 1638, Train loss: 3.348, Val loss: 2.896, Epoch time = 1.826s\n",
      "Epoch: 1639, Train loss: 3.052, Val loss: 2.801, Epoch time = 1.841s\n",
      "Epoch: 1640, Train loss: 2.869, Val loss: 3.025, Epoch time = 1.817s\n",
      "Epoch: 1641, Train loss: 3.099, Val loss: 2.982, Epoch time = 1.844s\n",
      "Epoch: 1642, Train loss: 3.103, Val loss: 3.019, Epoch time = 1.857s\n",
      "Epoch: 1643, Train loss: 3.276, Val loss: 3.227, Epoch time = 1.948s\n",
      "Epoch: 1644, Train loss: 3.278, Val loss: 2.808, Epoch time = 1.907s\n",
      "Epoch: 1645, Train loss: 3.260, Val loss: 3.227, Epoch time = 1.974s\n",
      "Epoch: 1646, Train loss: 2.990, Val loss: 2.778, Epoch time = 1.916s\n",
      "Epoch: 1647, Train loss: 3.209, Val loss: 3.161, Epoch time = 1.922s\n",
      "Epoch: 1648, Train loss: 3.071, Val loss: 2.962, Epoch time = 1.878s\n",
      "Epoch: 1649, Train loss: 2.997, Val loss: 3.173, Epoch time = 1.982s\n",
      "Epoch: 1650, Train loss: 2.967, Val loss: 2.929, Epoch time = 2.001s\n",
      "Epoch: 1651, Train loss: 2.940, Val loss: 2.700, Epoch time = 2.053s\n",
      "Epoch: 1652, Train loss: 2.954, Val loss: 2.968, Epoch time = 2.327s\n",
      "Epoch: 1653, Train loss: 3.068, Val loss: 2.790, Epoch time = 1.924s\n",
      "Epoch: 1654, Train loss: 2.967, Val loss: 2.862, Epoch time = 2.082s\n",
      "Epoch: 1655, Train loss: 2.720, Val loss: 3.581, Epoch time = 1.903s\n",
      "Epoch: 1656, Train loss: 3.019, Val loss: 2.844, Epoch time = 1.922s\n",
      "Epoch: 1657, Train loss: 2.784, Val loss: 2.611, Epoch time = 2.006s\n",
      "Epoch: 1658, Train loss: 2.936, Val loss: 2.895, Epoch time = 2.156s\n",
      "Epoch: 1659, Train loss: 2.912, Val loss: 3.257, Epoch time = 1.900s\n",
      "Epoch: 1660, Train loss: 3.543, Val loss: 3.421, Epoch time = 1.811s\n",
      "Epoch: 1661, Train loss: 2.673, Val loss: 2.716, Epoch time = 1.893s\n",
      "Epoch: 1662, Train loss: 2.827, Val loss: 2.812, Epoch time = 2.065s\n",
      "Epoch: 1663, Train loss: 3.048, Val loss: 2.743, Epoch time = 2.073s\n",
      "Epoch: 1664, Train loss: 2.613, Val loss: 2.661, Epoch time = 2.292s\n",
      "Epoch: 1665, Train loss: 3.229, Val loss: 2.678, Epoch time = 2.000s\n",
      "Epoch: 1666, Train loss: 2.933, Val loss: 3.493, Epoch time = 2.184s\n",
      "Epoch: 1667, Train loss: 2.973, Val loss: 3.128, Epoch time = 2.181s\n",
      "Epoch: 1668, Train loss: 2.862, Val loss: 2.878, Epoch time = 2.113s\n",
      "Epoch: 1669, Train loss: 2.815, Val loss: 3.188, Epoch time = 1.926s\n",
      "Epoch: 1670, Train loss: 2.886, Val loss: 3.617, Epoch time = 1.836s\n",
      "Epoch: 1671, Train loss: 2.854, Val loss: 3.253, Epoch time = 1.861s\n",
      "Epoch: 1672, Train loss: 2.866, Val loss: 3.201, Epoch time = 2.053s\n",
      "Epoch: 1673, Train loss: 2.900, Val loss: 3.050, Epoch time = 2.240s\n",
      "Epoch: 1674, Train loss: 3.119, Val loss: 2.945, Epoch time = 1.968s\n",
      "Epoch: 1675, Train loss: 3.035, Val loss: 2.886, Epoch time = 2.043s\n",
      "Epoch: 1676, Train loss: 3.160, Val loss: 3.458, Epoch time = 2.292s\n",
      "Epoch: 1677, Train loss: 2.824, Val loss: 3.121, Epoch time = 2.102s\n",
      "Epoch: 1678, Train loss: 3.047, Val loss: 2.731, Epoch time = 2.090s\n",
      "Epoch: 1679, Train loss: 2.827, Val loss: 3.011, Epoch time = 2.160s\n",
      "Epoch: 1680, Train loss: 3.620, Val loss: 2.919, Epoch time = 2.002s\n",
      "Epoch: 1681, Train loss: 2.947, Val loss: 3.067, Epoch time = 2.361s\n",
      "Epoch: 1682, Train loss: 3.254, Val loss: 3.210, Epoch time = 1.948s\n",
      "Epoch: 1683, Train loss: 2.742, Val loss: 3.554, Epoch time = 2.283s\n",
      "Epoch: 1684, Train loss: 3.053, Val loss: 2.822, Epoch time = 2.037s\n",
      "Epoch: 1685, Train loss: 3.490, Val loss: 2.947, Epoch time = 2.135s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1686, Train loss: 3.452, Val loss: 2.682, Epoch time = 2.117s\n",
      "Epoch: 1687, Train loss: 3.236, Val loss: 3.195, Epoch time = 2.062s\n",
      "Epoch: 1688, Train loss: 2.814, Val loss: 3.340, Epoch time = 2.014s\n",
      "Epoch: 1689, Train loss: 3.665, Val loss: 2.759, Epoch time = 1.933s\n",
      "Epoch: 1690, Train loss: 2.854, Val loss: 2.774, Epoch time = 2.144s\n",
      "Epoch: 1691, Train loss: 2.836, Val loss: 3.387, Epoch time = 1.866s\n",
      "Epoch: 1692, Train loss: 2.544, Val loss: 3.405, Epoch time = 1.867s\n",
      "Epoch: 1693, Train loss: 2.666, Val loss: 3.300, Epoch time = 2.047s\n",
      "Epoch: 1694, Train loss: 3.556, Val loss: 2.957, Epoch time = 2.009s\n",
      "Epoch: 1695, Train loss: 2.878, Val loss: 3.158, Epoch time = 1.892s\n",
      "Epoch: 1696, Train loss: 2.962, Val loss: 3.577, Epoch time = 2.200s\n",
      "Epoch: 1697, Train loss: 3.251, Val loss: 2.874, Epoch time = 2.153s\n",
      "Epoch: 1698, Train loss: 2.921, Val loss: 3.038, Epoch time = 3.238s\n",
      "Epoch: 1699, Train loss: 2.880, Val loss: 2.795, Epoch time = 2.327s\n",
      "Epoch: 1700, Train loss: 3.486, Val loss: 2.838, Epoch time = 2.204s\n",
      "Epoch: 1701, Train loss: 2.917, Val loss: 2.681, Epoch time = 2.363s\n",
      "Epoch: 1702, Train loss: 2.639, Val loss: 2.850, Epoch time = 2.357s\n",
      "Epoch: 1703, Train loss: 2.831, Val loss: 3.373, Epoch time = 2.250s\n",
      "Epoch: 1704, Train loss: 3.128, Val loss: 3.534, Epoch time = 2.018s\n",
      "Epoch: 1705, Train loss: 2.917, Val loss: 3.572, Epoch time = 2.443s\n",
      "Epoch: 1706, Train loss: 3.135, Val loss: 2.636, Epoch time = 2.039s\n",
      "Epoch: 1707, Train loss: 2.890, Val loss: 3.660, Epoch time = 2.184s\n",
      "Epoch: 1708, Train loss: 2.952, Val loss: 2.658, Epoch time = 2.182s\n",
      "Epoch: 1709, Train loss: 2.963, Val loss: 2.778, Epoch time = 1.902s\n",
      "Epoch: 1710, Train loss: 3.118, Val loss: 3.071, Epoch time = 2.040s\n",
      "Epoch: 1711, Train loss: 3.266, Val loss: 3.220, Epoch time = 2.074s\n",
      "Epoch: 1712, Train loss: 3.072, Val loss: 3.016, Epoch time = 2.120s\n",
      "Epoch: 1713, Train loss: 2.976, Val loss: 3.183, Epoch time = 2.213s\n",
      "Epoch: 1714, Train loss: 2.904, Val loss: 3.560, Epoch time = 2.204s\n",
      "Epoch: 1715, Train loss: 3.069, Val loss: 2.505, Epoch time = 2.437s\n",
      "Epoch: 1716, Train loss: 2.877, Val loss: 3.112, Epoch time = 2.185s\n",
      "Epoch: 1717, Train loss: 3.222, Val loss: 3.048, Epoch time = 2.403s\n",
      "Epoch: 1718, Train loss: 2.638, Val loss: 2.952, Epoch time = 2.475s\n",
      "Epoch: 1719, Train loss: 3.178, Val loss: 3.990, Epoch time = 2.341s\n",
      "Epoch: 1720, Train loss: 2.621, Val loss: 2.963, Epoch time = 2.589s\n",
      "Epoch: 1721, Train loss: 3.163, Val loss: 3.137, Epoch time = 2.229s\n",
      "Epoch: 1722, Train loss: 3.210, Val loss: 2.826, Epoch time = 2.052s\n",
      "Epoch: 1723, Train loss: 2.823, Val loss: 2.754, Epoch time = 2.360s\n",
      "Epoch: 1724, Train loss: 3.177, Val loss: 3.150, Epoch time = 2.491s\n",
      "Epoch: 1725, Train loss: 2.795, Val loss: 2.657, Epoch time = 2.284s\n",
      "Epoch: 1726, Train loss: 3.454, Val loss: 3.119, Epoch time = 2.067s\n",
      "Epoch: 1727, Train loss: 2.712, Val loss: 2.696, Epoch time = 2.138s\n",
      "Epoch: 1728, Train loss: 3.397, Val loss: 3.476, Epoch time = 1.821s\n",
      "Epoch: 1729, Train loss: 3.101, Val loss: 3.045, Epoch time = 1.906s\n",
      "Epoch: 1730, Train loss: 3.028, Val loss: 2.781, Epoch time = 1.944s\n",
      "Epoch: 1731, Train loss: 2.742, Val loss: 3.179, Epoch time = 1.999s\n",
      "Epoch: 1732, Train loss: 2.921, Val loss: 3.162, Epoch time = 1.837s\n",
      "Epoch: 1733, Train loss: 3.099, Val loss: 2.752, Epoch time = 1.975s\n",
      "Epoch: 1734, Train loss: 2.999, Val loss: 2.997, Epoch time = 2.012s\n",
      "Epoch: 1735, Train loss: 3.615, Val loss: 2.854, Epoch time = 2.074s\n",
      "Epoch: 1736, Train loss: 2.912, Val loss: 3.150, Epoch time = 2.153s\n",
      "Epoch: 1737, Train loss: 3.037, Val loss: 3.080, Epoch time = 1.983s\n",
      "Epoch: 1738, Train loss: 2.719, Val loss: 2.730, Epoch time = 1.949s\n",
      "Epoch: 1739, Train loss: 2.852, Val loss: 3.357, Epoch time = 1.863s\n",
      "Epoch: 1740, Train loss: 2.878, Val loss: 3.375, Epoch time = 1.829s\n",
      "Epoch: 1741, Train loss: 3.438, Val loss: 3.678, Epoch time = 2.046s\n",
      "Epoch: 1742, Train loss: 3.100, Val loss: 3.438, Epoch time = 1.881s\n",
      "Epoch: 1743, Train loss: 2.970, Val loss: 3.393, Epoch time = 2.123s\n",
      "Epoch: 1744, Train loss: 2.706, Val loss: 3.446, Epoch time = 1.916s\n",
      "Epoch: 1745, Train loss: 2.910, Val loss: 3.006, Epoch time = 1.944s\n",
      "Epoch: 1746, Train loss: 2.926, Val loss: 2.817, Epoch time = 1.895s\n",
      "Epoch: 1747, Train loss: 3.317, Val loss: 2.778, Epoch time = 2.094s\n",
      "Epoch: 1748, Train loss: 2.921, Val loss: 3.418, Epoch time = 2.149s\n",
      "Epoch: 1749, Train loss: 3.004, Val loss: 2.926, Epoch time = 2.248s\n",
      "Epoch: 1750, Train loss: 2.842, Val loss: 3.418, Epoch time = 2.282s\n",
      "Epoch: 1751, Train loss: 2.618, Val loss: 3.117, Epoch time = 2.091s\n",
      "Epoch: 1752, Train loss: 3.104, Val loss: 3.341, Epoch time = 1.979s\n",
      "Epoch: 1753, Train loss: 2.567, Val loss: 3.034, Epoch time = 2.001s\n",
      "Epoch: 1754, Train loss: 3.704, Val loss: 2.985, Epoch time = 1.941s\n",
      "Epoch: 1755, Train loss: 2.684, Val loss: 3.615, Epoch time = 1.911s\n",
      "Epoch: 1756, Train loss: 2.722, Val loss: 2.801, Epoch time = 2.242s\n",
      "Epoch: 1757, Train loss: 2.895, Val loss: 3.208, Epoch time = 1.985s\n",
      "Epoch: 1758, Train loss: 3.253, Val loss: 2.497, Epoch time = 2.005s\n",
      "Epoch: 1759, Train loss: 2.818, Val loss: 3.034, Epoch time = 2.045s\n",
      "Epoch: 1760, Train loss: 2.701, Val loss: 2.945, Epoch time = 2.061s\n",
      "Epoch: 1761, Train loss: 2.626, Val loss: 2.942, Epoch time = 2.104s\n",
      "Epoch: 1762, Train loss: 2.856, Val loss: 2.949, Epoch time = 2.072s\n",
      "Epoch: 1763, Train loss: 3.498, Val loss: 3.063, Epoch time = 1.864s\n",
      "Epoch: 1764, Train loss: 2.732, Val loss: 3.419, Epoch time = 2.073s\n",
      "Epoch: 1765, Train loss: 2.982, Val loss: 3.181, Epoch time = 2.022s\n",
      "Epoch: 1766, Train loss: 3.063, Val loss: 3.196, Epoch time = 1.953s\n",
      "Epoch: 1767, Train loss: 3.092, Val loss: 3.170, Epoch time = 1.930s\n",
      "Epoch: 1768, Train loss: 2.819, Val loss: 2.730, Epoch time = 2.008s\n",
      "Epoch: 1769, Train loss: 2.701, Val loss: 3.502, Epoch time = 2.120s\n",
      "Epoch: 1770, Train loss: 2.962, Val loss: 2.886, Epoch time = 2.030s\n",
      "Epoch: 1771, Train loss: 2.704, Val loss: 2.565, Epoch time = 2.144s\n",
      "Epoch: 1772, Train loss: 3.273, Val loss: 2.864, Epoch time = 2.125s\n",
      "Epoch: 1773, Train loss: 2.893, Val loss: 2.982, Epoch time = 1.889s\n",
      "Epoch: 1774, Train loss: 3.272, Val loss: 2.821, Epoch time = 2.048s\n",
      "Epoch: 1775, Train loss: 2.726, Val loss: 3.374, Epoch time = 2.063s\n",
      "Epoch: 1776, Train loss: 2.873, Val loss: 2.796, Epoch time = 2.168s\n",
      "Epoch: 1777, Train loss: 3.239, Val loss: 3.101, Epoch time = 2.299s\n",
      "Epoch: 1778, Train loss: 2.717, Val loss: 2.935, Epoch time = 2.127s\n",
      "Epoch: 1779, Train loss: 3.211, Val loss: 2.630, Epoch time = 2.311s\n",
      "Epoch: 1780, Train loss: 2.989, Val loss: 2.799, Epoch time = 2.113s\n",
      "Epoch: 1781, Train loss: 3.405, Val loss: 2.764, Epoch time = 2.453s\n",
      "Epoch: 1782, Train loss: 2.989, Val loss: 2.850, Epoch time = 2.463s\n",
      "Epoch: 1783, Train loss: 3.282, Val loss: 2.703, Epoch time = 2.214s\n",
      "Epoch: 1784, Train loss: 2.737, Val loss: 3.225, Epoch time = 2.066s\n",
      "Epoch: 1785, Train loss: 2.797, Val loss: 2.885, Epoch time = 1.959s\n",
      "Epoch: 1786, Train loss: 2.817, Val loss: 2.770, Epoch time = 1.959s\n",
      "Epoch: 1787, Train loss: 3.279, Val loss: 3.315, Epoch time = 1.852s\n",
      "Epoch: 1788, Train loss: 3.144, Val loss: 3.131, Epoch time = 1.942s\n",
      "Epoch: 1789, Train loss: 2.936, Val loss: 2.987, Epoch time = 1.897s\n",
      "Epoch: 1790, Train loss: 2.840, Val loss: 2.865, Epoch time = 1.943s\n",
      "Epoch: 1791, Train loss: 3.161, Val loss: 2.476, Epoch time = 1.870s\n",
      "Epoch: 1792, Train loss: 3.151, Val loss: 2.948, Epoch time = 2.458s\n",
      "Epoch: 1793, Train loss: 2.818, Val loss: 2.998, Epoch time = 2.214s\n",
      "Epoch: 1794, Train loss: 2.838, Val loss: 2.848, Epoch time = 2.192s\n",
      "Epoch: 1795, Train loss: 3.208, Val loss: 3.047, Epoch time = 2.207s\n",
      "Epoch: 1796, Train loss: 3.063, Val loss: 3.015, Epoch time = 2.095s\n",
      "Epoch: 1797, Train loss: 2.786, Val loss: 2.735, Epoch time = 2.295s\n",
      "Epoch: 1798, Train loss: 3.258, Val loss: 2.654, Epoch time = 2.090s\n",
      "Epoch: 1799, Train loss: 2.993, Val loss: 3.345, Epoch time = 2.116s\n",
      "Epoch: 1800, Train loss: 2.982, Val loss: 2.855, Epoch time = 2.161s\n",
      "Epoch: 1801, Train loss: 2.961, Val loss: 2.575, Epoch time = 1.956s\n",
      "Epoch: 1802, Train loss: 3.127, Val loss: 2.756, Epoch time = 1.920s\n",
      "Epoch: 1803, Train loss: 3.361, Val loss: 2.768, Epoch time = 1.922s\n",
      "Epoch: 1804, Train loss: 3.101, Val loss: 3.146, Epoch time = 2.131s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1805, Train loss: 3.678, Val loss: 2.540, Epoch time = 2.168s\n",
      "Epoch: 1806, Train loss: 2.714, Val loss: 3.462, Epoch time = 2.061s\n",
      "Epoch: 1807, Train loss: 2.557, Val loss: 2.966, Epoch time = 2.502s\n",
      "Epoch: 1808, Train loss: 3.477, Val loss: 2.947, Epoch time = 2.427s\n",
      "Epoch: 1809, Train loss: 2.891, Val loss: 2.676, Epoch time = 2.443s\n",
      "Epoch: 1810, Train loss: 3.086, Val loss: 2.918, Epoch time = 2.181s\n",
      "Epoch: 1811, Train loss: 3.764, Val loss: 3.444, Epoch time = 2.321s\n",
      "Epoch: 1812, Train loss: 2.558, Val loss: 2.807, Epoch time = 2.219s\n",
      "Epoch: 1813, Train loss: 2.885, Val loss: 2.866, Epoch time = 2.433s\n",
      "Epoch: 1814, Train loss: 2.596, Val loss: 2.641, Epoch time = 2.089s\n",
      "Epoch: 1815, Train loss: 2.987, Val loss: 2.920, Epoch time = 2.192s\n",
      "Epoch: 1816, Train loss: 3.175, Val loss: 2.957, Epoch time = 2.120s\n",
      "Epoch: 1817, Train loss: 3.309, Val loss: 2.815, Epoch time = 2.098s\n",
      "Epoch: 1818, Train loss: 2.797, Val loss: 2.994, Epoch time = 1.993s\n",
      "Epoch: 1819, Train loss: 2.831, Val loss: 2.875, Epoch time = 1.977s\n",
      "Epoch: 1820, Train loss: 3.548, Val loss: 2.716, Epoch time = 1.969s\n",
      "Epoch: 1821, Train loss: 2.713, Val loss: 3.374, Epoch time = 2.207s\n",
      "Epoch: 1822, Train loss: 3.563, Val loss: 2.719, Epoch time = 1.869s\n",
      "Epoch: 1823, Train loss: 2.896, Val loss: 3.519, Epoch time = 1.956s\n",
      "Epoch: 1824, Train loss: 2.612, Val loss: 2.815, Epoch time = 1.904s\n",
      "Epoch: 1825, Train loss: 2.659, Val loss: 3.082, Epoch time = 1.892s\n",
      "Epoch: 1826, Train loss: 2.734, Val loss: 3.211, Epoch time = 2.059s\n",
      "Epoch: 1827, Train loss: 3.288, Val loss: 2.907, Epoch time = 2.021s\n",
      "Epoch: 1828, Train loss: 2.598, Val loss: 3.209, Epoch time = 2.077s\n",
      "Epoch: 1829, Train loss: 3.024, Val loss: 3.698, Epoch time = 2.008s\n",
      "Epoch: 1830, Train loss: 3.077, Val loss: 3.220, Epoch time = 1.974s\n",
      "Epoch: 1831, Train loss: 3.021, Val loss: 2.896, Epoch time = 1.947s\n",
      "Epoch: 1832, Train loss: 2.689, Val loss: 3.120, Epoch time = 2.025s\n",
      "Epoch: 1833, Train loss: 3.080, Val loss: 3.238, Epoch time = 2.036s\n",
      "Epoch: 1834, Train loss: 2.842, Val loss: 3.245, Epoch time = 2.071s\n",
      "Epoch: 1835, Train loss: 2.920, Val loss: 2.999, Epoch time = 2.144s\n",
      "Epoch: 1836, Train loss: 2.669, Val loss: 3.092, Epoch time = 1.933s\n",
      "Epoch: 1837, Train loss: 2.448, Val loss: 3.310, Epoch time = 1.963s\n",
      "Epoch: 1838, Train loss: 3.768, Val loss: 2.681, Epoch time = 2.162s\n",
      "Epoch: 1839, Train loss: 2.516, Val loss: 2.485, Epoch time = 2.128s\n",
      "Epoch: 1840, Train loss: 3.458, Val loss: 3.842, Epoch time = 2.071s\n",
      "Epoch: 1841, Train loss: 3.097, Val loss: 3.508, Epoch time = 1.918s\n",
      "Epoch: 1842, Train loss: 2.812, Val loss: 2.629, Epoch time = 1.886s\n",
      "Epoch: 1843, Train loss: 3.463, Val loss: 3.150, Epoch time = 2.275s\n",
      "Epoch: 1844, Train loss: 2.634, Val loss: 2.638, Epoch time = 2.134s\n",
      "Epoch: 1845, Train loss: 3.655, Val loss: 3.134, Epoch time = 1.947s\n",
      "Epoch: 1846, Train loss: 2.842, Val loss: 2.831, Epoch time = 2.098s\n",
      "Epoch: 1847, Train loss: 2.988, Val loss: 2.971, Epoch time = 2.312s\n",
      "Epoch: 1848, Train loss: 2.672, Val loss: 2.736, Epoch time = 2.188s\n",
      "Epoch: 1849, Train loss: 3.466, Val loss: 3.467, Epoch time = 2.135s\n",
      "Epoch: 1850, Train loss: 2.949, Val loss: 2.727, Epoch time = 2.199s\n",
      "Epoch: 1851, Train loss: 3.036, Val loss: 3.000, Epoch time = 2.062s\n",
      "Epoch: 1852, Train loss: 3.213, Val loss: 3.035, Epoch time = 2.198s\n",
      "Epoch: 1853, Train loss: 2.884, Val loss: 3.414, Epoch time = 1.877s\n",
      "Epoch: 1854, Train loss: 2.902, Val loss: 3.543, Epoch time = 1.905s\n",
      "Epoch: 1855, Train loss: 2.930, Val loss: 3.352, Epoch time = 1.857s\n",
      "Epoch: 1856, Train loss: 2.942, Val loss: 3.302, Epoch time = 2.183s\n",
      "Epoch: 1857, Train loss: 3.477, Val loss: 2.738, Epoch time = 1.935s\n",
      "Epoch: 1858, Train loss: 3.189, Val loss: 3.085, Epoch time = 1.965s\n",
      "Epoch: 1859, Train loss: 2.817, Val loss: 3.061, Epoch time = 1.977s\n",
      "Epoch: 1860, Train loss: 2.829, Val loss: 3.110, Epoch time = 1.892s\n",
      "Epoch: 1861, Train loss: 2.958, Val loss: 2.787, Epoch time = 1.947s\n",
      "Epoch: 1862, Train loss: 2.982, Val loss: 2.728, Epoch time = 2.018s\n",
      "Epoch: 1863, Train loss: 3.210, Val loss: 2.760, Epoch time = 1.952s\n",
      "Epoch: 1864, Train loss: 3.256, Val loss: 3.487, Epoch time = 2.066s\n",
      "Epoch: 1865, Train loss: 2.508, Val loss: 2.715, Epoch time = 2.144s\n",
      "Epoch: 1866, Train loss: 2.827, Val loss: 2.654, Epoch time = 1.948s\n",
      "Epoch: 1867, Train loss: 2.525, Val loss: 2.898, Epoch time = 1.858s\n",
      "Epoch: 1868, Train loss: 2.611, Val loss: 2.741, Epoch time = 1.940s\n",
      "Epoch: 1869, Train loss: 2.705, Val loss: 2.965, Epoch time = 1.931s\n",
      "Epoch: 1870, Train loss: 2.741, Val loss: 2.745, Epoch time = 2.037s\n",
      "Epoch: 1871, Train loss: 2.851, Val loss: 3.469, Epoch time = 2.146s\n",
      "Epoch: 1872, Train loss: 3.325, Val loss: 2.909, Epoch time = 1.888s\n",
      "Epoch: 1873, Train loss: 3.586, Val loss: 2.774, Epoch time = 1.973s\n",
      "Epoch: 1874, Train loss: 2.706, Val loss: 2.943, Epoch time = 2.010s\n",
      "Epoch: 1875, Train loss: 2.972, Val loss: 3.178, Epoch time = 2.285s\n",
      "Epoch: 1876, Train loss: 2.775, Val loss: 2.715, Epoch time = 2.060s\n",
      "Epoch: 1877, Train loss: 2.696, Val loss: 3.637, Epoch time = 2.125s\n",
      "Epoch: 1878, Train loss: 2.776, Val loss: 3.325, Epoch time = 2.034s\n",
      "Epoch: 1879, Train loss: 3.215, Val loss: 2.719, Epoch time = 2.098s\n",
      "Epoch: 1880, Train loss: 3.270, Val loss: 2.783, Epoch time = 2.098s\n",
      "Epoch: 1881, Train loss: 2.749, Val loss: 2.982, Epoch time = 2.035s\n",
      "Epoch: 1882, Train loss: 3.348, Val loss: 3.129, Epoch time = 1.898s\n",
      "Epoch: 1883, Train loss: 3.192, Val loss: 3.288, Epoch time = 2.149s\n",
      "Epoch: 1884, Train loss: 2.735, Val loss: 3.386, Epoch time = 2.016s\n",
      "Epoch: 1885, Train loss: 2.662, Val loss: 2.849, Epoch time = 1.883s\n",
      "Epoch: 1886, Train loss: 3.444, Val loss: 2.782, Epoch time = 1.832s\n",
      "Epoch: 1887, Train loss: 3.178, Val loss: 2.870, Epoch time = 1.905s\n",
      "Epoch: 1888, Train loss: 2.870, Val loss: 2.586, Epoch time = 1.871s\n",
      "Epoch: 1889, Train loss: 2.671, Val loss: 3.085, Epoch time = 1.876s\n",
      "Epoch: 1890, Train loss: 3.585, Val loss: 3.212, Epoch time = 1.924s\n",
      "Epoch: 1891, Train loss: 2.996, Val loss: 2.914, Epoch time = 2.212s\n",
      "Epoch: 1892, Train loss: 2.904, Val loss: 2.567, Epoch time = 2.253s\n",
      "Epoch: 1893, Train loss: 2.637, Val loss: 2.872, Epoch time = 2.200s\n",
      "Epoch: 1894, Train loss: 3.202, Val loss: 3.487, Epoch time = 1.898s\n",
      "Epoch: 1895, Train loss: 2.912, Val loss: 3.059, Epoch time = 2.302s\n",
      "Epoch: 1896, Train loss: 2.791, Val loss: 2.547, Epoch time = 1.928s\n",
      "Epoch: 1897, Train loss: 3.611, Val loss: 2.990, Epoch time = 1.915s\n",
      "Epoch: 1898, Train loss: 3.113, Val loss: 2.820, Epoch time = 1.972s\n",
      "Epoch: 1899, Train loss: 3.039, Val loss: 2.955, Epoch time = 1.896s\n",
      "Epoch: 1900, Train loss: 3.153, Val loss: 2.796, Epoch time = 2.046s\n",
      "Epoch: 1901, Train loss: 3.076, Val loss: 2.724, Epoch time = 2.032s\n",
      "Epoch: 1902, Train loss: 2.855, Val loss: 3.256, Epoch time = 1.893s\n",
      "Epoch: 1903, Train loss: 2.837, Val loss: 2.626, Epoch time = 1.865s\n",
      "Epoch: 1904, Train loss: 2.948, Val loss: 3.121, Epoch time = 2.214s\n",
      "Epoch: 1905, Train loss: 3.288, Val loss: 2.806, Epoch time = 2.424s\n",
      "Epoch: 1906, Train loss: 2.994, Val loss: 2.812, Epoch time = 2.304s\n",
      "Epoch: 1907, Train loss: 2.748, Val loss: 2.619, Epoch time = 2.503s\n",
      "Epoch: 1908, Train loss: 3.384, Val loss: 2.780, Epoch time = 2.331s\n",
      "Epoch: 1909, Train loss: 2.968, Val loss: 3.653, Epoch time = 2.328s\n",
      "Epoch: 1910, Train loss: 2.973, Val loss: 2.752, Epoch time = 2.305s\n",
      "Epoch: 1911, Train loss: 3.286, Val loss: 3.679, Epoch time = 2.215s\n",
      "Epoch: 1912, Train loss: 3.109, Val loss: 2.739, Epoch time = 1.916s\n",
      "Epoch: 1913, Train loss: 2.930, Val loss: 2.566, Epoch time = 1.927s\n",
      "Epoch: 1914, Train loss: 2.685, Val loss: 2.891, Epoch time = 2.109s\n",
      "Epoch: 1915, Train loss: 2.798, Val loss: 2.578, Epoch time = 2.035s\n",
      "Epoch: 1916, Train loss: 2.932, Val loss: 2.697, Epoch time = 2.044s\n",
      "Epoch: 1917, Train loss: 3.164, Val loss: 2.980, Epoch time = 2.027s\n",
      "Epoch: 1918, Train loss: 2.761, Val loss: 2.857, Epoch time = 2.176s\n",
      "Epoch: 1919, Train loss: 2.793, Val loss: 3.296, Epoch time = 1.865s\n",
      "Epoch: 1920, Train loss: 2.684, Val loss: 2.784, Epoch time = 1.956s\n",
      "Epoch: 1921, Train loss: 2.953, Val loss: 2.834, Epoch time = 2.009s\n",
      "Epoch: 1922, Train loss: 2.682, Val loss: 2.828, Epoch time = 2.056s\n",
      "Epoch: 1923, Train loss: 2.860, Val loss: 3.501, Epoch time = 2.158s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1924, Train loss: 2.632, Val loss: 2.880, Epoch time = 2.094s\n",
      "Epoch: 1925, Train loss: 2.688, Val loss: 2.776, Epoch time = 2.334s\n",
      "Epoch: 1926, Train loss: 2.736, Val loss: 2.919, Epoch time = 2.098s\n",
      "Epoch: 1927, Train loss: 3.001, Val loss: 2.659, Epoch time = 2.076s\n",
      "Epoch: 1928, Train loss: 3.332, Val loss: 3.047, Epoch time = 1.964s\n",
      "Epoch: 1929, Train loss: 2.730, Val loss: 3.044, Epoch time = 1.934s\n",
      "Epoch: 1930, Train loss: 3.767, Val loss: 2.940, Epoch time = 2.092s\n",
      "Epoch: 1931, Train loss: 3.816, Val loss: 2.876, Epoch time = 1.894s\n",
      "Epoch: 1932, Train loss: 3.032, Val loss: 2.910, Epoch time = 2.005s\n",
      "Epoch: 1933, Train loss: 2.907, Val loss: 3.354, Epoch time = 2.133s\n",
      "Epoch: 1934, Train loss: 2.841, Val loss: 2.986, Epoch time = 2.095s\n",
      "Epoch: 1935, Train loss: 2.844, Val loss: 2.988, Epoch time = 2.048s\n",
      "Epoch: 1936, Train loss: 3.561, Val loss: 2.799, Epoch time = 2.219s\n",
      "Epoch: 1937, Train loss: 3.089, Val loss: 2.807, Epoch time = 2.223s\n",
      "Epoch: 1938, Train loss: 3.188, Val loss: 2.870, Epoch time = 2.324s\n",
      "Epoch: 1939, Train loss: 3.158, Val loss: 2.800, Epoch time = 2.182s\n",
      "Epoch: 1940, Train loss: 2.772, Val loss: 2.737, Epoch time = 2.079s\n",
      "Epoch: 1941, Train loss: 2.925, Val loss: 3.015, Epoch time = 2.090s\n",
      "Epoch: 1942, Train loss: 3.155, Val loss: 2.884, Epoch time = 1.963s\n",
      "Epoch: 1943, Train loss: 3.705, Val loss: 2.901, Epoch time = 1.946s\n",
      "Epoch: 1944, Train loss: 3.481, Val loss: 2.936, Epoch time = 2.050s\n",
      "Epoch: 1945, Train loss: 2.909, Val loss: 2.640, Epoch time = 2.085s\n",
      "Epoch: 1946, Train loss: 2.875, Val loss: 2.867, Epoch time = 2.027s\n",
      "Epoch: 1947, Train loss: 2.822, Val loss: 3.402, Epoch time = 2.149s\n",
      "Epoch: 1948, Train loss: 3.387, Val loss: 2.891, Epoch time = 1.864s\n",
      "Epoch: 1949, Train loss: 3.375, Val loss: 2.956, Epoch time = 1.922s\n",
      "Epoch: 1950, Train loss: 2.957, Val loss: 3.117, Epoch time = 1.867s\n",
      "Epoch: 1951, Train loss: 2.904, Val loss: 2.919, Epoch time = 2.075s\n",
      "Epoch: 1952, Train loss: 2.480, Val loss: 3.050, Epoch time = 2.079s\n",
      "Epoch: 1953, Train loss: 2.789, Val loss: 2.611, Epoch time = 1.924s\n",
      "Epoch: 1954, Train loss: 2.769, Val loss: 2.955, Epoch time = 1.901s\n",
      "Epoch: 1955, Train loss: 2.972, Val loss: 2.899, Epoch time = 1.877s\n",
      "Epoch: 1956, Train loss: 3.345, Val loss: 3.080, Epoch time = 2.013s\n",
      "Epoch: 1957, Train loss: 3.328, Val loss: 2.767, Epoch time = 2.110s\n",
      "Epoch: 1958, Train loss: 3.350, Val loss: 2.983, Epoch time = 1.973s\n",
      "Epoch: 1959, Train loss: 3.456, Val loss: 3.083, Epoch time = 2.109s\n",
      "Epoch: 1960, Train loss: 2.768, Val loss: 2.812, Epoch time = 2.142s\n",
      "Epoch: 1961, Train loss: 3.397, Val loss: 3.109, Epoch time = 1.990s\n",
      "Epoch: 1962, Train loss: 3.490, Val loss: 2.820, Epoch time = 2.016s\n",
      "Epoch: 1963, Train loss: 2.875, Val loss: 2.521, Epoch time = 1.997s\n",
      "Epoch: 1964, Train loss: 3.081, Val loss: 2.897, Epoch time = 2.175s\n",
      "Epoch: 1965, Train loss: 3.594, Val loss: 2.743, Epoch time = 2.315s\n",
      "Epoch: 1966, Train loss: 2.957, Val loss: 2.831, Epoch time = 2.190s\n",
      "Epoch: 1967, Train loss: 3.077, Val loss: 3.286, Epoch time = 1.973s\n",
      "Epoch: 1968, Train loss: 2.860, Val loss: 3.040, Epoch time = 2.251s\n",
      "Epoch: 1969, Train loss: 2.874, Val loss: 3.368, Epoch time = 2.191s\n",
      "Epoch: 1970, Train loss: 2.971, Val loss: 2.684, Epoch time = 2.178s\n",
      "Epoch: 1971, Train loss: 3.176, Val loss: 2.836, Epoch time = 2.116s\n",
      "Epoch: 1972, Train loss: 3.460, Val loss: 3.145, Epoch time = 2.039s\n",
      "Epoch: 1973, Train loss: 3.024, Val loss: 2.633, Epoch time = 1.879s\n",
      "Epoch: 1974, Train loss: 2.793, Val loss: 3.265, Epoch time = 2.072s\n",
      "Epoch: 1975, Train loss: 3.051, Val loss: 3.132, Epoch time = 1.927s\n",
      "Epoch: 1976, Train loss: 3.183, Val loss: 2.644, Epoch time = 1.904s\n",
      "Epoch: 1977, Train loss: 2.625, Val loss: 3.677, Epoch time = 1.928s\n",
      "Epoch: 1978, Train loss: 2.980, Val loss: 2.602, Epoch time = 2.018s\n",
      "Epoch: 1979, Train loss: 2.957, Val loss: 2.847, Epoch time = 2.318s\n",
      "Epoch: 1980, Train loss: 2.811, Val loss: 3.247, Epoch time = 2.136s\n",
      "Epoch: 1981, Train loss: 3.140, Val loss: 3.347, Epoch time = 2.077s\n",
      "Epoch: 1982, Train loss: 2.655, Val loss: 2.643, Epoch time = 2.062s\n",
      "Epoch: 1983, Train loss: 3.068, Val loss: 2.959, Epoch time = 2.080s\n",
      "Epoch: 1984, Train loss: 2.970, Val loss: 3.178, Epoch time = 2.483s\n",
      "Epoch: 1985, Train loss: 2.914, Val loss: 3.530, Epoch time = 2.515s\n",
      "Epoch: 1986, Train loss: 3.643, Val loss: 3.086, Epoch time = 2.257s\n",
      "Epoch: 1987, Train loss: 3.075, Val loss: 3.128, Epoch time = 2.151s\n",
      "Epoch: 1988, Train loss: 3.259, Val loss: 2.887, Epoch time = 2.155s\n",
      "Epoch: 1989, Train loss: 2.943, Val loss: 2.880, Epoch time = 2.137s\n",
      "Epoch: 1990, Train loss: 2.940, Val loss: 3.278, Epoch time = 2.159s\n",
      "Epoch: 1991, Train loss: 3.695, Val loss: 2.857, Epoch time = 2.031s\n",
      "Epoch: 1992, Train loss: 2.678, Val loss: 2.619, Epoch time = 2.085s\n",
      "Epoch: 1993, Train loss: 2.824, Val loss: 2.877, Epoch time = 2.349s\n",
      "Epoch: 1994, Train loss: 2.666, Val loss: 2.829, Epoch time = 2.467s\n",
      "Epoch: 1995, Train loss: 3.482, Val loss: 2.458, Epoch time = 2.312s\n",
      "Epoch: 1996, Train loss: 2.833, Val loss: 2.981, Epoch time = 2.146s\n",
      "Epoch: 1997, Train loss: 3.206, Val loss: 2.873, Epoch time = 1.972s\n",
      "Epoch: 1998, Train loss: 3.050, Val loss: 3.219, Epoch time = 2.112s\n",
      "Epoch: 1999, Train loss: 2.705, Val loss: 2.724, Epoch time = 2.228s\n",
      "Epoch: 2000, Train loss: 3.307, Val loss: 2.937, Epoch time = 2.170s\n",
      "Epoch: 2001, Train loss: 3.653, Val loss: 2.702, Epoch time = 2.025s\n",
      "Epoch: 2002, Train loss: 3.307, Val loss: 2.876, Epoch time = 1.915s\n",
      "Epoch: 2003, Train loss: 3.223, Val loss: 3.005, Epoch time = 2.231s\n",
      "Epoch: 2004, Train loss: 2.937, Val loss: 2.848, Epoch time = 2.058s\n",
      "Epoch: 2005, Train loss: 3.184, Val loss: 2.966, Epoch time = 2.108s\n",
      "Epoch: 2006, Train loss: 2.781, Val loss: 3.171, Epoch time = 2.128s\n",
      "Epoch: 2007, Train loss: 3.436, Val loss: 2.818, Epoch time = 2.528s\n",
      "Epoch: 2008, Train loss: 2.938, Val loss: 3.001, Epoch time = 2.168s\n",
      "Epoch: 2009, Train loss: 2.683, Val loss: 2.808, Epoch time = 2.360s\n",
      "Epoch: 2010, Train loss: 2.720, Val loss: 3.179, Epoch time = 2.396s\n",
      "Epoch: 2011, Train loss: 2.840, Val loss: 2.679, Epoch time = 2.346s\n",
      "Epoch: 2012, Train loss: 2.847, Val loss: 2.868, Epoch time = 2.835s\n",
      "Epoch: 2013, Train loss: 3.601, Val loss: 2.797, Epoch time = 2.169s\n",
      "Epoch: 2014, Train loss: 2.700, Val loss: 3.245, Epoch time = 2.514s\n",
      "Epoch: 2015, Train loss: 2.927, Val loss: 3.434, Epoch time = 2.314s\n",
      "Epoch: 2016, Train loss: 2.934, Val loss: 3.269, Epoch time = 1.963s\n",
      "Epoch: 2017, Train loss: 3.350, Val loss: 3.294, Epoch time = 1.966s\n",
      "Epoch: 2018, Train loss: 2.759, Val loss: 2.888, Epoch time = 2.001s\n",
      "Epoch: 2019, Train loss: 2.609, Val loss: 3.438, Epoch time = 2.044s\n",
      "Epoch: 2020, Train loss: 2.790, Val loss: 3.149, Epoch time = 2.295s\n",
      "Epoch: 2021, Train loss: 3.107, Val loss: 2.630, Epoch time = 2.147s\n",
      "Epoch: 2022, Train loss: 3.194, Val loss: 3.136, Epoch time = 2.279s\n",
      "Epoch: 2023, Train loss: 3.130, Val loss: 2.542, Epoch time = 2.137s\n",
      "Epoch: 2024, Train loss: 3.355, Val loss: 3.196, Epoch time = 2.076s\n",
      "Epoch: 2025, Train loss: 2.886, Val loss: 2.867, Epoch time = 2.167s\n",
      "Epoch: 2026, Train loss: 2.884, Val loss: 2.793, Epoch time = 2.063s\n",
      "Epoch: 2027, Train loss: 3.143, Val loss: 2.942, Epoch time = 1.883s\n",
      "Epoch: 2028, Train loss: 2.503, Val loss: 2.536, Epoch time = 2.163s\n",
      "Epoch: 2029, Train loss: 3.129, Val loss: 3.045, Epoch time = 2.121s\n",
      "Epoch: 2030, Train loss: 3.122, Val loss: 2.776, Epoch time = 2.137s\n",
      "Epoch: 2031, Train loss: 2.893, Val loss: 2.729, Epoch time = 2.259s\n",
      "Epoch: 2032, Train loss: 2.544, Val loss: 3.551, Epoch time = 2.250s\n",
      "Epoch: 2033, Train loss: 3.148, Val loss: 2.765, Epoch time = 2.469s\n",
      "Epoch: 2034, Train loss: 2.921, Val loss: 3.398, Epoch time = 2.168s\n",
      "Epoch: 2035, Train loss: 2.838, Val loss: 3.522, Epoch time = 2.266s\n",
      "Epoch: 2036, Train loss: 3.171, Val loss: 2.928, Epoch time = 2.634s\n",
      "Epoch: 2037, Train loss: 3.047, Val loss: 2.809, Epoch time = 2.145s\n",
      "Epoch: 2038, Train loss: 3.240, Val loss: 2.691, Epoch time = 2.084s\n",
      "Epoch: 2039, Train loss: 2.922, Val loss: 2.500, Epoch time = 1.986s\n",
      "Epoch: 2040, Train loss: 3.607, Val loss: 2.673, Epoch time = 2.105s\n",
      "Epoch: 2041, Train loss: 2.821, Val loss: 2.917, Epoch time = 2.083s\n",
      "Epoch: 2042, Train loss: 3.178, Val loss: 2.570, Epoch time = 1.904s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2043, Train loss: 2.723, Val loss: 2.851, Epoch time = 2.171s\n",
      "Epoch: 2044, Train loss: 2.894, Val loss: 3.439, Epoch time = 2.103s\n",
      "Epoch: 2045, Train loss: 2.997, Val loss: 2.932, Epoch time = 2.076s\n",
      "Epoch: 2046, Train loss: 2.713, Val loss: 2.962, Epoch time = 2.001s\n",
      "Epoch: 2047, Train loss: 2.741, Val loss: 2.768, Epoch time = 1.927s\n",
      "Epoch: 2048, Train loss: 3.347, Val loss: 3.319, Epoch time = 1.948s\n",
      "Epoch: 2049, Train loss: 3.687, Val loss: 2.887, Epoch time = 1.973s\n",
      "Epoch: 2050, Train loss: 2.909, Val loss: 3.006, Epoch time = 1.955s\n",
      "Epoch: 2051, Train loss: 2.963, Val loss: 2.744, Epoch time = 1.952s\n",
      "Epoch: 2052, Train loss: 3.528, Val loss: 3.602, Epoch time = 1.887s\n",
      "Epoch: 2053, Train loss: 3.661, Val loss: 2.812, Epoch time = 1.884s\n",
      "Epoch: 2054, Train loss: 2.849, Val loss: 3.007, Epoch time = 1.983s\n",
      "Epoch: 2055, Train loss: 2.976, Val loss: 3.374, Epoch time = 1.975s\n",
      "Epoch: 2056, Train loss: 2.791, Val loss: 3.398, Epoch time = 1.966s\n",
      "Epoch: 2057, Train loss: 3.153, Val loss: 2.861, Epoch time = 2.154s\n",
      "Epoch: 2058, Train loss: 2.643, Val loss: 3.565, Epoch time = 2.022s\n",
      "Epoch: 2059, Train loss: 3.370, Val loss: 2.826, Epoch time = 2.210s\n",
      "Epoch: 2060, Train loss: 3.251, Val loss: 2.690, Epoch time = 2.094s\n",
      "Epoch: 2061, Train loss: 2.538, Val loss: 2.846, Epoch time = 2.226s\n",
      "Epoch: 2062, Train loss: 3.550, Val loss: 2.947, Epoch time = 1.910s\n",
      "Epoch: 2063, Train loss: 3.216, Val loss: 3.092, Epoch time = 1.934s\n",
      "Epoch: 2064, Train loss: 2.987, Val loss: 3.356, Epoch time = 1.927s\n",
      "Epoch: 2065, Train loss: 2.814, Val loss: 2.895, Epoch time = 2.273s\n",
      "Epoch: 2066, Train loss: 3.173, Val loss: 2.799, Epoch time = 2.253s\n",
      "Epoch: 2067, Train loss: 2.890, Val loss: 2.996, Epoch time = 2.374s\n",
      "Epoch: 2068, Train loss: 2.930, Val loss: 3.431, Epoch time = 2.143s\n",
      "Epoch: 2069, Train loss: 3.502, Val loss: 2.968, Epoch time = 2.323s\n",
      "Epoch: 2070, Train loss: 2.501, Val loss: 2.723, Epoch time = 2.258s\n",
      "Epoch: 2071, Train loss: 3.009, Val loss: 2.616, Epoch time = 2.000s\n",
      "Epoch: 2072, Train loss: 2.675, Val loss: 2.751, Epoch time = 1.952s\n",
      "Epoch: 2073, Train loss: 3.064, Val loss: 3.049, Epoch time = 2.029s\n",
      "Epoch: 2074, Train loss: 3.028, Val loss: 3.165, Epoch time = 1.913s\n",
      "Epoch: 2075, Train loss: 3.073, Val loss: 2.981, Epoch time = 1.951s\n",
      "Epoch: 2076, Train loss: 3.691, Val loss: 2.897, Epoch time = 1.975s\n",
      "Epoch: 2077, Train loss: 3.046, Val loss: 2.716, Epoch time = 2.080s\n",
      "Epoch: 2078, Train loss: 3.357, Val loss: 2.860, Epoch time = 1.968s\n",
      "Epoch: 2079, Train loss: 2.772, Val loss: 2.557, Epoch time = 2.235s\n",
      "Epoch: 2080, Train loss: 3.165, Val loss: 2.777, Epoch time = 2.221s\n",
      "Epoch: 2081, Train loss: 2.920, Val loss: 2.811, Epoch time = 2.291s\n",
      "Epoch: 2082, Train loss: 2.872, Val loss: 2.719, Epoch time = 2.178s\n",
      "Epoch: 2083, Train loss: 3.335, Val loss: 2.583, Epoch time = 2.074s\n",
      "Epoch: 2084, Train loss: 2.570, Val loss: 2.851, Epoch time = 2.166s\n",
      "Epoch: 2085, Train loss: 2.858, Val loss: 3.316, Epoch time = 2.012s\n",
      "Epoch: 2086, Train loss: 2.831, Val loss: 2.583, Epoch time = 1.988s\n",
      "Epoch: 2087, Train loss: 3.045, Val loss: 3.129, Epoch time = 1.976s\n",
      "Epoch: 2088, Train loss: 2.760, Val loss: 2.681, Epoch time = 1.951s\n",
      "Epoch: 2089, Train loss: 2.572, Val loss: 2.793, Epoch time = 2.219s\n",
      "Epoch: 2090, Train loss: 2.638, Val loss: 2.724, Epoch time = 2.197s\n",
      "Epoch: 2091, Train loss: 2.721, Val loss: 3.275, Epoch time = 1.994s\n",
      "Epoch: 2092, Train loss: 3.131, Val loss: 3.244, Epoch time = 2.105s\n",
      "Epoch: 2093, Train loss: 2.666, Val loss: 3.205, Epoch time = 2.221s\n",
      "Epoch: 2094, Train loss: 3.106, Val loss: 2.818, Epoch time = 2.015s\n",
      "Epoch: 2095, Train loss: 2.632, Val loss: 4.009, Epoch time = 2.136s\n",
      "Epoch: 2096, Train loss: 2.665, Val loss: 3.009, Epoch time = 2.139s\n",
      "Epoch: 2097, Train loss: 3.322, Val loss: 2.870, Epoch time = 2.179s\n",
      "Epoch: 2098, Train loss: 3.470, Val loss: 2.597, Epoch time = 2.057s\n",
      "Epoch: 2099, Train loss: 2.924, Val loss: 3.079, Epoch time = 2.023s\n",
      "Epoch: 2100, Train loss: 3.271, Val loss: 2.804, Epoch time = 1.966s\n",
      "Epoch: 2101, Train loss: 3.255, Val loss: 3.208, Epoch time = 2.073s\n",
      "Epoch: 2102, Train loss: 3.149, Val loss: 2.714, Epoch time = 2.081s\n",
      "Epoch: 2103, Train loss: 2.915, Val loss: 2.601, Epoch time = 2.141s\n",
      "Epoch: 2104, Train loss: 2.678, Val loss: 2.894, Epoch time = 2.131s\n",
      "Epoch: 2105, Train loss: 2.981, Val loss: 2.777, Epoch time = 2.162s\n",
      "Epoch: 2106, Train loss: 2.794, Val loss: 2.629, Epoch time = 2.577s\n",
      "Epoch: 2107, Train loss: 2.598, Val loss: 2.688, Epoch time = 2.350s\n",
      "Epoch: 2108, Train loss: 2.858, Val loss: 3.011, Epoch time = 2.356s\n",
      "Epoch: 2109, Train loss: 2.711, Val loss: 3.080, Epoch time = 2.510s\n",
      "Epoch: 2110, Train loss: 3.088, Val loss: 2.748, Epoch time = 2.375s\n",
      "Epoch: 2111, Train loss: 3.210, Val loss: 3.162, Epoch time = 2.075s\n",
      "Epoch: 2112, Train loss: 3.231, Val loss: 3.008, Epoch time = 2.112s\n",
      "Epoch: 2113, Train loss: 2.754, Val loss: 2.749, Epoch time = 2.108s\n",
      "Epoch: 2114, Train loss: 3.061, Val loss: 3.000, Epoch time = 2.614s\n",
      "Epoch: 2115, Train loss: 3.300, Val loss: 2.936, Epoch time = 2.331s\n",
      "Epoch: 2116, Train loss: 2.645, Val loss: 3.017, Epoch time = 1.952s\n",
      "Epoch: 2117, Train loss: 3.323, Val loss: 2.669, Epoch time = 2.249s\n",
      "Epoch: 2118, Train loss: 2.971, Val loss: 3.486, Epoch time = 2.064s\n",
      "Epoch: 2119, Train loss: 2.923, Val loss: 3.111, Epoch time = 2.066s\n",
      "Epoch: 2120, Train loss: 2.608, Val loss: 2.860, Epoch time = 2.282s\n",
      "Epoch: 2121, Train loss: 2.933, Val loss: 2.626, Epoch time = 2.054s\n",
      "Epoch: 2122, Train loss: 2.790, Val loss: 3.196, Epoch time = 1.996s\n",
      "Epoch: 2123, Train loss: 3.260, Val loss: 2.821, Epoch time = 2.124s\n",
      "Epoch: 2124, Train loss: 3.243, Val loss: 3.285, Epoch time = 2.116s\n",
      "Epoch: 2125, Train loss: 2.934, Val loss: 2.900, Epoch time = 2.137s\n",
      "Epoch: 2126, Train loss: 2.809, Val loss: 2.653, Epoch time = 2.161s\n",
      "Epoch: 2127, Train loss: 2.753, Val loss: 2.961, Epoch time = 2.152s\n",
      "Epoch: 2128, Train loss: 2.856, Val loss: 3.198, Epoch time = 2.141s\n",
      "Epoch: 2129, Train loss: 2.845, Val loss: 3.039, Epoch time = 2.042s\n",
      "Epoch: 2130, Train loss: 3.025, Val loss: 3.237, Epoch time = 2.073s\n",
      "Epoch: 2131, Train loss: 3.236, Val loss: 3.530, Epoch time = 2.153s\n",
      "Epoch: 2132, Train loss: 3.095, Val loss: 3.242, Epoch time = 2.204s\n",
      "Epoch: 2133, Train loss: 2.851, Val loss: 2.832, Epoch time = 2.126s\n",
      "Epoch: 2134, Train loss: 3.093, Val loss: 2.894, Epoch time = 2.077s\n",
      "Epoch: 2135, Train loss: 2.802, Val loss: 2.839, Epoch time = 2.249s\n",
      "Epoch: 2136, Train loss: 2.865, Val loss: 3.042, Epoch time = 2.148s\n",
      "Epoch: 2137, Train loss: 2.788, Val loss: 3.512, Epoch time = 2.121s\n",
      "Epoch: 2138, Train loss: 2.580, Val loss: 3.016, Epoch time = 1.884s\n",
      "Epoch: 2139, Train loss: 3.003, Val loss: 2.909, Epoch time = 1.901s\n",
      "Epoch: 2140, Train loss: 2.931, Val loss: 2.865, Epoch time = 2.081s\n",
      "Epoch: 2141, Train loss: 2.773, Val loss: 3.275, Epoch time = 2.115s\n",
      "Epoch: 2142, Train loss: 2.715, Val loss: 2.908, Epoch time = 1.953s\n",
      "Epoch: 2143, Train loss: 3.058, Val loss: 2.985, Epoch time = 1.932s\n",
      "Epoch: 2144, Train loss: 3.103, Val loss: 3.483, Epoch time = 2.196s\n",
      "Epoch: 2145, Train loss: 3.018, Val loss: 2.729, Epoch time = 2.177s\n",
      "Epoch: 2146, Train loss: 3.377, Val loss: 2.893, Epoch time = 2.063s\n",
      "Epoch: 2147, Train loss: 3.358, Val loss: 3.071, Epoch time = 2.117s\n",
      "Epoch: 2148, Train loss: 2.829, Val loss: 3.521, Epoch time = 2.474s\n",
      "Epoch: 2149, Train loss: 2.880, Val loss: 3.370, Epoch time = 2.004s\n",
      "Epoch: 2150, Train loss: 2.654, Val loss: 2.833, Epoch time = 2.931s\n",
      "Epoch: 2151, Train loss: 2.565, Val loss: 2.971, Epoch time = 2.191s\n",
      "Epoch: 2152, Train loss: 3.041, Val loss: 2.647, Epoch time = 2.125s\n",
      "Epoch: 2153, Train loss: 2.809, Val loss: 2.897, Epoch time = 2.125s\n",
      "Epoch: 2154, Train loss: 2.880, Val loss: 3.862, Epoch time = 2.048s\n",
      "Epoch: 2155, Train loss: 2.639, Val loss: 2.586, Epoch time = 2.131s\n",
      "Epoch: 2156, Train loss: 2.618, Val loss: 3.545, Epoch time = 2.017s\n",
      "Epoch: 2157, Train loss: 3.028, Val loss: 3.522, Epoch time = 2.095s\n",
      "Epoch: 2158, Train loss: 3.162, Val loss: 2.959, Epoch time = 1.855s\n",
      "Epoch: 2159, Train loss: 2.890, Val loss: 2.970, Epoch time = 1.824s\n",
      "Epoch: 2160, Train loss: 2.675, Val loss: 3.126, Epoch time = 1.964s\n",
      "Epoch: 2161, Train loss: 3.228, Val loss: 3.163, Epoch time = 2.090s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2162, Train loss: 2.866, Val loss: 2.661, Epoch time = 2.186s\n",
      "Epoch: 2163, Train loss: 2.817, Val loss: 2.556, Epoch time = 2.274s\n",
      "Epoch: 2164, Train loss: 3.629, Val loss: 2.938, Epoch time = 2.016s\n",
      "Epoch: 2165, Train loss: 2.664, Val loss: 2.684, Epoch time = 1.975s\n",
      "Epoch: 2166, Train loss: 3.164, Val loss: 2.919, Epoch time = 2.065s\n",
      "Epoch: 2167, Train loss: 2.973, Val loss: 2.688, Epoch time = 2.057s\n",
      "Epoch: 2168, Train loss: 2.788, Val loss: 2.924, Epoch time = 2.110s\n",
      "Epoch: 2169, Train loss: 3.064, Val loss: 2.970, Epoch time = 2.163s\n",
      "Epoch: 2170, Train loss: 2.789, Val loss: 2.561, Epoch time = 2.075s\n",
      "Epoch: 2171, Train loss: 2.774, Val loss: 2.862, Epoch time = 2.146s\n",
      "Epoch: 2172, Train loss: 3.048, Val loss: 2.988, Epoch time = 2.140s\n",
      "Epoch: 2173, Train loss: 2.841, Val loss: 2.963, Epoch time = 2.092s\n",
      "Epoch: 2174, Train loss: 3.046, Val loss: 3.114, Epoch time = 2.291s\n",
      "Epoch: 2175, Train loss: 3.556, Val loss: 2.719, Epoch time = 2.168s\n",
      "Epoch: 2176, Train loss: 2.822, Val loss: 2.586, Epoch time = 2.120s\n",
      "Epoch: 2177, Train loss: 2.911, Val loss: 2.965, Epoch time = 2.095s\n",
      "Epoch: 2178, Train loss: 3.030, Val loss: 2.741, Epoch time = 2.635s\n",
      "Epoch: 2179, Train loss: 3.080, Val loss: 2.738, Epoch time = 2.286s\n",
      "Epoch: 2180, Train loss: 2.863, Val loss: 3.031, Epoch time = 2.069s\n",
      "Epoch: 2181, Train loss: 2.522, Val loss: 2.887, Epoch time = 1.956s\n",
      "Epoch: 2182, Train loss: 2.563, Val loss: 3.583, Epoch time = 1.978s\n",
      "Epoch: 2183, Train loss: 2.809, Val loss: 2.857, Epoch time = 1.914s\n",
      "Epoch: 2184, Train loss: 2.851, Val loss: 2.992, Epoch time = 2.087s\n",
      "Epoch: 2185, Train loss: 2.596, Val loss: 2.939, Epoch time = 2.060s\n",
      "Epoch: 2186, Train loss: 2.701, Val loss: 2.691, Epoch time = 2.079s\n",
      "Epoch: 2187, Train loss: 2.755, Val loss: 2.808, Epoch time = 2.044s\n",
      "Epoch: 2188, Train loss: 3.068, Val loss: 3.114, Epoch time = 2.040s\n",
      "Epoch: 2189, Train loss: 3.169, Val loss: 3.471, Epoch time = 1.992s\n",
      "Epoch: 2190, Train loss: 2.726, Val loss: 3.291, Epoch time = 2.038s\n",
      "Epoch: 2191, Train loss: 2.791, Val loss: 3.029, Epoch time = 2.029s\n",
      "Epoch: 2192, Train loss: 2.698, Val loss: 3.007, Epoch time = 1.838s\n",
      "Epoch: 2193, Train loss: 3.583, Val loss: 2.988, Epoch time = 1.867s\n",
      "Epoch: 2194, Train loss: 3.321, Val loss: 3.142, Epoch time = 1.900s\n",
      "Epoch: 2195, Train loss: 2.946, Val loss: 2.940, Epoch time = 1.909s\n",
      "Epoch: 2196, Train loss: 2.858, Val loss: 2.851, Epoch time = 1.834s\n",
      "Epoch: 2197, Train loss: 3.055, Val loss: 3.064, Epoch time = 1.831s\n",
      "Epoch: 2198, Train loss: 2.549, Val loss: 2.944, Epoch time = 1.891s\n",
      "Epoch: 2199, Train loss: 2.636, Val loss: 3.134, Epoch time = 2.060s\n",
      "Epoch: 2200, Train loss: 2.701, Val loss: 2.922, Epoch time = 2.078s\n",
      "Epoch: 2201, Train loss: 3.217, Val loss: 2.689, Epoch time = 1.998s\n",
      "Epoch: 2202, Train loss: 3.115, Val loss: 2.945, Epoch time = 1.989s\n",
      "Epoch: 2203, Train loss: 3.265, Val loss: 2.632, Epoch time = 2.021s\n",
      "Epoch: 2204, Train loss: 2.800, Val loss: 2.633, Epoch time = 2.074s\n",
      "Epoch: 2205, Train loss: 3.223, Val loss: 2.540, Epoch time = 1.828s\n",
      "Epoch: 2206, Train loss: 2.942, Val loss: 2.786, Epoch time = 1.814s\n",
      "Epoch: 2207, Train loss: 3.241, Val loss: 2.728, Epoch time = 1.869s\n",
      "Epoch: 2208, Train loss: 2.853, Val loss: 2.658, Epoch time = 1.865s\n",
      "Epoch: 2209, Train loss: 2.670, Val loss: 2.794, Epoch time = 1.874s\n",
      "Epoch: 2210, Train loss: 2.882, Val loss: 2.869, Epoch time = 1.920s\n",
      "Epoch: 2211, Train loss: 3.400, Val loss: 2.757, Epoch time = 1.896s\n",
      "Epoch: 2212, Train loss: 3.048, Val loss: 2.854, Epoch time = 2.000s\n",
      "Epoch: 2213, Train loss: 2.631, Val loss: 2.898, Epoch time = 1.873s\n",
      "Epoch: 2214, Train loss: 3.651, Val loss: 2.589, Epoch time = 1.873s\n",
      "Epoch: 2215, Train loss: 2.778, Val loss: 3.178, Epoch time = 1.881s\n",
      "Epoch: 2216, Train loss: 2.737, Val loss: 2.695, Epoch time = 1.983s\n",
      "Epoch: 2217, Train loss: 2.838, Val loss: 3.267, Epoch time = 1.881s\n",
      "Epoch: 2218, Train loss: 3.163, Val loss: 3.046, Epoch time = 1.916s\n",
      "Epoch: 2219, Train loss: 2.982, Val loss: 2.990, Epoch time = 1.896s\n",
      "Epoch: 2220, Train loss: 2.831, Val loss: 2.918, Epoch time = 2.035s\n",
      "Epoch: 2221, Train loss: 2.889, Val loss: 2.706, Epoch time = 1.886s\n",
      "Epoch: 2222, Train loss: 2.756, Val loss: 2.456, Epoch time = 1.890s\n",
      "Epoch: 2223, Train loss: 3.439, Val loss: 3.145, Epoch time = 1.900s\n",
      "Epoch: 2224, Train loss: 2.566, Val loss: 2.830, Epoch time = 1.941s\n",
      "Epoch: 2225, Train loss: 3.145, Val loss: 3.573, Epoch time = 1.920s\n",
      "Epoch: 2226, Train loss: 3.075, Val loss: 3.315, Epoch time = 1.897s\n",
      "Epoch: 2227, Train loss: 3.017, Val loss: 2.596, Epoch time = 2.020s\n",
      "Epoch: 2228, Train loss: 2.955, Val loss: 2.905, Epoch time = 2.067s\n",
      "Epoch: 2229, Train loss: 3.162, Val loss: 2.760, Epoch time = 1.979s\n",
      "Epoch: 2230, Train loss: 3.036, Val loss: 2.666, Epoch time = 1.875s\n",
      "Epoch: 2231, Train loss: 2.780, Val loss: 2.630, Epoch time = 1.938s\n",
      "Epoch: 2232, Train loss: 2.860, Val loss: 2.954, Epoch time = 2.028s\n",
      "Epoch: 2233, Train loss: 2.990, Val loss: 3.111, Epoch time = 2.022s\n",
      "Epoch: 2234, Train loss: 3.548, Val loss: 3.213, Epoch time = 1.966s\n",
      "Epoch: 2235, Train loss: 2.758, Val loss: 3.405, Epoch time = 2.044s\n",
      "Epoch: 2236, Train loss: 2.791, Val loss: 2.802, Epoch time = 2.006s\n",
      "Epoch: 2237, Train loss: 3.037, Val loss: 2.911, Epoch time = 1.961s\n",
      "Epoch: 2238, Train loss: 2.748, Val loss: 2.753, Epoch time = 1.967s\n",
      "Epoch: 2239, Train loss: 3.385, Val loss: 2.912, Epoch time = 1.902s\n",
      "Epoch: 2240, Train loss: 2.835, Val loss: 2.680, Epoch time = 1.869s\n",
      "Epoch: 2241, Train loss: 3.320, Val loss: 2.947, Epoch time = 2.025s\n",
      "Epoch: 2242, Train loss: 3.051, Val loss: 2.822, Epoch time = 2.008s\n",
      "Epoch: 2243, Train loss: 3.246, Val loss: 3.164, Epoch time = 1.976s\n",
      "Epoch: 2244, Train loss: 3.283, Val loss: 3.209, Epoch time = 1.879s\n",
      "Epoch: 2245, Train loss: 3.591, Val loss: 2.659, Epoch time = 1.884s\n",
      "Epoch: 2246, Train loss: 2.600, Val loss: 3.722, Epoch time = 1.865s\n",
      "Epoch: 2247, Train loss: 3.338, Val loss: 3.303, Epoch time = 1.882s\n",
      "Epoch: 2248, Train loss: 3.126, Val loss: 3.159, Epoch time = 1.914s\n",
      "Epoch: 2249, Train loss: 3.055, Val loss: 3.613, Epoch time = 1.903s\n",
      "Epoch: 2250, Train loss: 2.824, Val loss: 2.988, Epoch time = 1.931s\n",
      "Epoch: 2251, Train loss: 2.878, Val loss: 2.836, Epoch time = 1.885s\n",
      "Epoch: 2252, Train loss: 2.743, Val loss: 2.850, Epoch time = 1.903s\n",
      "Epoch: 2253, Train loss: 3.536, Val loss: 3.652, Epoch time = 1.991s\n",
      "Epoch: 2254, Train loss: 2.916, Val loss: 2.576, Epoch time = 1.991s\n",
      "Epoch: 2255, Train loss: 2.840, Val loss: 2.844, Epoch time = 2.022s\n",
      "Epoch: 2256, Train loss: 2.970, Val loss: 3.238, Epoch time = 2.023s\n",
      "Epoch: 2257, Train loss: 3.172, Val loss: 2.575, Epoch time = 2.053s\n",
      "Epoch: 2258, Train loss: 3.261, Val loss: 3.095, Epoch time = 1.949s\n",
      "Epoch: 2259, Train loss: 2.526, Val loss: 2.841, Epoch time = 1.926s\n",
      "Epoch: 2260, Train loss: 2.839, Val loss: 3.141, Epoch time = 1.884s\n",
      "Epoch: 2261, Train loss: 3.253, Val loss: 3.201, Epoch time = 1.881s\n",
      "Epoch: 2262, Train loss: 3.031, Val loss: 2.695, Epoch time = 1.969s\n",
      "Epoch: 2263, Train loss: 3.155, Val loss: 2.850, Epoch time = 2.032s\n",
      "Epoch: 2264, Train loss: 2.829, Val loss: 2.822, Epoch time = 2.029s\n",
      "Epoch: 2265, Train loss: 3.206, Val loss: 2.871, Epoch time = 1.848s\n",
      "Epoch: 2266, Train loss: 3.271, Val loss: 3.094, Epoch time = 1.884s\n",
      "Epoch: 2267, Train loss: 3.152, Val loss: 2.804, Epoch time = 1.829s\n",
      "Epoch: 2268, Train loss: 2.667, Val loss: 2.823, Epoch time = 1.600s\n",
      "Epoch: 2269, Train loss: 2.758, Val loss: 2.619, Epoch time = 1.636s\n",
      "Epoch: 2270, Train loss: 3.495, Val loss: 2.777, Epoch time = 1.794s\n",
      "Epoch: 2271, Train loss: 3.927, Val loss: 3.465, Epoch time = 1.570s\n",
      "Epoch: 2272, Train loss: 3.004, Val loss: 2.581, Epoch time = 1.597s\n",
      "Epoch: 2273, Train loss: 2.915, Val loss: 3.143, Epoch time = 1.690s\n",
      "Epoch: 2274, Train loss: 2.848, Val loss: 2.796, Epoch time = 1.598s\n",
      "Epoch: 2275, Train loss: 2.862, Val loss: 2.445, Epoch time = 1.631s\n",
      "Epoch: 2276, Train loss: 3.151, Val loss: 3.080, Epoch time = 1.578s\n",
      "Epoch: 2277, Train loss: 2.861, Val loss: 2.890, Epoch time = 1.586s\n",
      "Epoch: 2278, Train loss: 2.873, Val loss: 2.672, Epoch time = 1.593s\n",
      "Epoch: 2279, Train loss: 3.085, Val loss: 3.122, Epoch time = 1.630s\n",
      "Epoch: 2280, Train loss: 2.719, Val loss: 3.732, Epoch time = 1.716s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2281, Train loss: 2.854, Val loss: 2.845, Epoch time = 1.674s\n",
      "Epoch: 2282, Train loss: 2.834, Val loss: 2.690, Epoch time = 1.718s\n",
      "Epoch: 2283, Train loss: 2.850, Val loss: 3.069, Epoch time = 1.690s\n",
      "Epoch: 2284, Train loss: 2.776, Val loss: 3.094, Epoch time = 1.649s\n",
      "Epoch: 2285, Train loss: 2.754, Val loss: 2.653, Epoch time = 1.662s\n",
      "Epoch: 2286, Train loss: 3.061, Val loss: 3.481, Epoch time = 1.655s\n",
      "Epoch: 2287, Train loss: 2.835, Val loss: 3.180, Epoch time = 1.721s\n",
      "Epoch: 2288, Train loss: 3.035, Val loss: 2.921, Epoch time = 1.592s\n",
      "Epoch: 2289, Train loss: 3.121, Val loss: 3.290, Epoch time = 1.616s\n",
      "Epoch: 2290, Train loss: 3.553, Val loss: 3.220, Epoch time = 1.538s\n",
      "Epoch: 2291, Train loss: 3.540, Val loss: 3.031, Epoch time = 1.541s\n",
      "Epoch: 2292, Train loss: 2.858, Val loss: 2.775, Epoch time = 1.574s\n",
      "Epoch: 2293, Train loss: 2.921, Val loss: 2.767, Epoch time = 1.705s\n",
      "Epoch: 2294, Train loss: 2.737, Val loss: 2.965, Epoch time = 1.678s\n",
      "Epoch: 2295, Train loss: 2.677, Val loss: 2.903, Epoch time = 1.678s\n",
      "Epoch: 2296, Train loss: 2.778, Val loss: 2.772, Epoch time = 1.671s\n",
      "Epoch: 2297, Train loss: 3.774, Val loss: 3.043, Epoch time = 1.675s\n",
      "Epoch: 2298, Train loss: 3.158, Val loss: 3.388, Epoch time = 1.637s\n",
      "Epoch: 2299, Train loss: 3.007, Val loss: 2.979, Epoch time = 1.683s\n",
      "Epoch: 2300, Train loss: 2.956, Val loss: 2.755, Epoch time = 1.637s\n",
      "Epoch: 2301, Train loss: 2.825, Val loss: 2.990, Epoch time = 1.524s\n",
      "Epoch: 2302, Train loss: 3.194, Val loss: 2.786, Epoch time = 1.576s\n",
      "Epoch: 2303, Train loss: 2.753, Val loss: 2.601, Epoch time = 1.528s\n",
      "Epoch: 2304, Train loss: 2.624, Val loss: 2.925, Epoch time = 1.570s\n",
      "Epoch: 2305, Train loss: 2.716, Val loss: 3.312, Epoch time = 1.550s\n",
      "Epoch: 2306, Train loss: 2.770, Val loss: 2.716, Epoch time = 1.549s\n",
      "Epoch: 2307, Train loss: 2.972, Val loss: 3.081, Epoch time = 1.545s\n",
      "Epoch: 2308, Train loss: 2.815, Val loss: 2.677, Epoch time = 1.528s\n",
      "Epoch: 2309, Train loss: 3.708, Val loss: 3.214, Epoch time = 1.525s\n",
      "Epoch: 2310, Train loss: 3.091, Val loss: 2.972, Epoch time = 1.595s\n",
      "Epoch: 2311, Train loss: 3.687, Val loss: 2.923, Epoch time = 1.739s\n",
      "Epoch: 2312, Train loss: 2.732, Val loss: 3.205, Epoch time = 1.699s\n",
      "Epoch: 2313, Train loss: 3.206, Val loss: 2.651, Epoch time = 1.691s\n",
      "Epoch: 2314, Train loss: 2.442, Val loss: 2.749, Epoch time = 1.691s\n",
      "Epoch: 2315, Train loss: 2.978, Val loss: 2.789, Epoch time = 1.584s\n",
      "Epoch: 2316, Train loss: 2.893, Val loss: 2.852, Epoch time = 1.611s\n",
      "Epoch: 2317, Train loss: 3.310, Val loss: 2.945, Epoch time = 1.588s\n",
      "Epoch: 2318, Train loss: 2.643, Val loss: 2.927, Epoch time = 1.619s\n",
      "Epoch: 2319, Train loss: 3.108, Val loss: 2.633, Epoch time = 1.539s\n",
      "Epoch: 2320, Train loss: 2.850, Val loss: 2.954, Epoch time = 1.563s\n",
      "Epoch: 2321, Train loss: 3.038, Val loss: 2.490, Epoch time = 1.652s\n",
      "Epoch: 2322, Train loss: 3.057, Val loss: 2.828, Epoch time = 1.631s\n",
      "Epoch: 2323, Train loss: 2.817, Val loss: 3.031, Epoch time = 1.550s\n",
      "Epoch: 2324, Train loss: 2.680, Val loss: 2.966, Epoch time = 1.514s\n",
      "Epoch: 2325, Train loss: 2.687, Val loss: 3.138, Epoch time = 1.540s\n",
      "Epoch: 2326, Train loss: 3.238, Val loss: 2.581, Epoch time = 1.670s\n",
      "Epoch: 2327, Train loss: 2.801, Val loss: 3.660, Epoch time = 1.651s\n",
      "Epoch: 2328, Train loss: 2.955, Val loss: 3.155, Epoch time = 1.618s\n",
      "Epoch: 2329, Train loss: 2.878, Val loss: 3.353, Epoch time = 1.636s\n",
      "Epoch: 2330, Train loss: 3.432, Val loss: 3.505, Epoch time = 1.616s\n",
      "Epoch: 2331, Train loss: 3.323, Val loss: 3.108, Epoch time = 1.643s\n",
      "Epoch: 2332, Train loss: 2.926, Val loss: 3.103, Epoch time = 1.637s\n",
      "Epoch: 2333, Train loss: 2.598, Val loss: 2.968, Epoch time = 1.525s\n",
      "Epoch: 2334, Train loss: 3.665, Val loss: 2.835, Epoch time = 1.516s\n",
      "Epoch: 2335, Train loss: 2.474, Val loss: 3.283, Epoch time = 1.556s\n",
      "Epoch: 2336, Train loss: 3.123, Val loss: 2.625, Epoch time = 1.569s\n",
      "Epoch: 2337, Train loss: 2.996, Val loss: 3.161, Epoch time = 1.565s\n",
      "Epoch: 2338, Train loss: 3.024, Val loss: 2.672, Epoch time = 1.574s\n",
      "Epoch: 2339, Train loss: 2.771, Val loss: 2.666, Epoch time = 1.677s\n",
      "Epoch: 2340, Train loss: 3.404, Val loss: 3.454, Epoch time = 1.614s\n",
      "Epoch: 2341, Train loss: 3.389, Val loss: 2.700, Epoch time = 1.649s\n",
      "Epoch: 2342, Train loss: 2.870, Val loss: 3.022, Epoch time = 1.631s\n",
      "Epoch: 2343, Train loss: 3.458, Val loss: 2.941, Epoch time = 1.678s\n",
      "Epoch: 2344, Train loss: 3.303, Val loss: 2.649, Epoch time = 1.643s\n",
      "Epoch: 2345, Train loss: 3.431, Val loss: 3.432, Epoch time = 1.677s\n",
      "Epoch: 2346, Train loss: 3.033, Val loss: 2.514, Epoch time = 1.708s\n",
      "Epoch: 2347, Train loss: 2.725, Val loss: 3.065, Epoch time = 1.635s\n",
      "Epoch: 2348, Train loss: 2.519, Val loss: 3.509, Epoch time = 1.628s\n",
      "Epoch: 2349, Train loss: 3.007, Val loss: 3.315, Epoch time = 1.611s\n",
      "Epoch: 2350, Train loss: 2.695, Val loss: 2.593, Epoch time = 1.706s\n",
      "Epoch: 2351, Train loss: 2.889, Val loss: 3.393, Epoch time = 1.611s\n",
      "Epoch: 2352, Train loss: 3.436, Val loss: 2.786, Epoch time = 1.569s\n",
      "Epoch: 2353, Train loss: 2.673, Val loss: 3.405, Epoch time = 1.561s\n",
      "Epoch: 2354, Train loss: 2.998, Val loss: 3.140, Epoch time = 1.638s\n",
      "Epoch: 2355, Train loss: 3.015, Val loss: 2.849, Epoch time = 1.584s\n",
      "Epoch: 2356, Train loss: 3.345, Val loss: 2.975, Epoch time = 1.564s\n",
      "Epoch: 2357, Train loss: 3.138, Val loss: 2.464, Epoch time = 1.567s\n",
      "Epoch: 2358, Train loss: 3.597, Val loss: 3.049, Epoch time = 1.587s\n",
      "Epoch: 2359, Train loss: 2.873, Val loss: 2.728, Epoch time = 1.607s\n",
      "Epoch: 2360, Train loss: 2.857, Val loss: 3.379, Epoch time = 1.616s\n",
      "Epoch: 2361, Train loss: 2.886, Val loss: 3.054, Epoch time = 1.606s\n",
      "Epoch: 2362, Train loss: 3.048, Val loss: 2.668, Epoch time = 1.582s\n",
      "Epoch: 2363, Train loss: 2.625, Val loss: 2.756, Epoch time = 1.620s\n",
      "Epoch: 2364, Train loss: 3.154, Val loss: 3.159, Epoch time = 1.583s\n",
      "Epoch: 2365, Train loss: 3.240, Val loss: 3.418, Epoch time = 1.657s\n",
      "Epoch: 2366, Train loss: 2.769, Val loss: 2.855, Epoch time = 1.558s\n",
      "Epoch: 2367, Train loss: 3.164, Val loss: 2.932, Epoch time = 1.602s\n",
      "Epoch: 2368, Train loss: 2.946, Val loss: 2.810, Epoch time = 1.527s\n",
      "Epoch: 2369, Train loss: 3.093, Val loss: 2.527, Epoch time = 1.534s\n",
      "Epoch: 2370, Train loss: 2.776, Val loss: 3.398, Epoch time = 1.626s\n",
      "Epoch: 2371, Train loss: 2.754, Val loss: 3.088, Epoch time = 1.606s\n",
      "Epoch: 2372, Train loss: 3.228, Val loss: 2.785, Epoch time = 1.624s\n",
      "Epoch: 2373, Train loss: 2.813, Val loss: 3.101, Epoch time = 1.660s\n",
      "Epoch: 2374, Train loss: 2.697, Val loss: 3.452, Epoch time = 1.638s\n",
      "Epoch: 2375, Train loss: 3.071, Val loss: 3.405, Epoch time = 1.515s\n",
      "Epoch: 2376, Train loss: 2.917, Val loss: 2.645, Epoch time = 1.529s\n",
      "Epoch: 2377, Train loss: 2.675, Val loss: 2.939, Epoch time = 1.523s\n",
      "Epoch: 2378, Train loss: 2.624, Val loss: 3.528, Epoch time = 1.529s\n",
      "Epoch: 2379, Train loss: 3.523, Val loss: 2.987, Epoch time = 1.662s\n",
      "Epoch: 2380, Train loss: 3.031, Val loss: 2.570, Epoch time = 1.624s\n",
      "Epoch: 2381, Train loss: 2.653, Val loss: 2.707, Epoch time = 1.639s\n",
      "Epoch: 2382, Train loss: 2.809, Val loss: 2.606, Epoch time = 1.638s\n",
      "Epoch: 2383, Train loss: 2.733, Val loss: 3.031, Epoch time = 1.612s\n",
      "Epoch: 2384, Train loss: 2.798, Val loss: 2.757, Epoch time = 1.642s\n",
      "Epoch: 2385, Train loss: 2.677, Val loss: 2.813, Epoch time = 1.624s\n",
      "Epoch: 2386, Train loss: 3.283, Val loss: 3.502, Epoch time = 1.604s\n",
      "Epoch: 2387, Train loss: 2.488, Val loss: 3.082, Epoch time = 1.592s\n",
      "Epoch: 2388, Train loss: 2.769, Val loss: 2.557, Epoch time = 1.612s\n",
      "Epoch: 2389, Train loss: 3.614, Val loss: 3.355, Epoch time = 1.541s\n",
      "Epoch: 2390, Train loss: 3.030, Val loss: 2.635, Epoch time = 1.575s\n",
      "Epoch: 2391, Train loss: 3.755, Val loss: 3.037, Epoch time = 1.552s\n",
      "Epoch: 2392, Train loss: 3.188, Val loss: 3.391, Epoch time = 1.597s\n",
      "Epoch: 2393, Train loss: 2.863, Val loss: 3.323, Epoch time = 1.543s\n",
      "Epoch: 2394, Train loss: 3.228, Val loss: 2.768, Epoch time = 1.549s\n",
      "Epoch: 2395, Train loss: 2.699, Val loss: 2.797, Epoch time = 1.604s\n",
      "Epoch: 2396, Train loss: 2.895, Val loss: 3.095, Epoch time = 1.626s\n",
      "Epoch: 2397, Train loss: 3.336, Val loss: 2.781, Epoch time = 1.610s\n",
      "Epoch: 2398, Train loss: 3.252, Val loss: 2.917, Epoch time = 1.569s\n",
      "Epoch: 2399, Train loss: 2.969, Val loss: 2.795, Epoch time = 1.586s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2400, Train loss: 3.352, Val loss: 2.896, Epoch time = 1.547s\n",
      "Epoch: 2401, Train loss: 3.276, Val loss: 3.331, Epoch time = 1.589s\n",
      "Epoch: 2402, Train loss: 2.880, Val loss: 2.509, Epoch time = 1.633s\n",
      "Epoch: 2403, Train loss: 2.738, Val loss: 2.877, Epoch time = 1.593s\n",
      "Epoch: 2404, Train loss: 3.390, Val loss: 3.322, Epoch time = 1.577s\n",
      "Epoch: 2405, Train loss: 2.999, Val loss: 2.899, Epoch time = 1.581s\n",
      "Epoch: 2406, Train loss: 2.955, Val loss: 2.848, Epoch time = 1.635s\n",
      "Epoch: 2407, Train loss: 3.218, Val loss: 2.775, Epoch time = 1.563s\n",
      "Epoch: 2408, Train loss: 2.737, Val loss: 2.796, Epoch time = 1.571s\n",
      "Epoch: 2409, Train loss: 3.174, Val loss: 2.595, Epoch time = 1.505s\n",
      "Epoch: 2410, Train loss: 2.976, Val loss: 2.502, Epoch time = 1.530s\n",
      "Epoch: 2411, Train loss: 3.288, Val loss: 2.895, Epoch time = 1.578s\n",
      "Epoch: 2412, Train loss: 2.676, Val loss: 3.430, Epoch time = 1.649s\n",
      "Epoch: 2413, Train loss: 2.882, Val loss: 3.362, Epoch time = 1.595s\n",
      "Epoch: 2414, Train loss: 3.025, Val loss: 3.127, Epoch time = 1.494s\n",
      "Epoch: 2415, Train loss: 2.646, Val loss: 2.750, Epoch time = 1.518s\n",
      "Epoch: 2416, Train loss: 3.090, Val loss: 2.836, Epoch time = 1.478s\n",
      "Epoch: 2417, Train loss: 3.227, Val loss: 3.035, Epoch time = 1.481s\n",
      "Epoch: 2418, Train loss: 3.267, Val loss: 3.076, Epoch time = 1.580s\n",
      "Epoch: 2419, Train loss: 2.776, Val loss: 2.863, Epoch time = 1.502s\n",
      "Epoch: 2420, Train loss: 2.865, Val loss: 2.853, Epoch time = 1.551s\n",
      "Epoch: 2421, Train loss: 3.246, Val loss: 3.152, Epoch time = 1.530s\n",
      "Epoch: 2422, Train loss: 3.052, Val loss: 2.582, Epoch time = 1.528s\n",
      "Epoch: 2423, Train loss: 2.548, Val loss: 3.077, Epoch time = 1.621s\n",
      "Epoch: 2424, Train loss: 3.073, Val loss: 2.801, Epoch time = 1.612s\n",
      "Epoch: 2425, Train loss: 3.166, Val loss: 3.454, Epoch time = 1.649s\n",
      "Epoch: 2426, Train loss: 2.604, Val loss: 2.776, Epoch time = 1.605s\n",
      "Epoch: 2427, Train loss: 3.267, Val loss: 3.011, Epoch time = 1.596s\n",
      "Epoch: 2428, Train loss: 3.120, Val loss: 2.453, Epoch time = 1.550s\n",
      "Epoch: 2429, Train loss: 3.174, Val loss: 2.964, Epoch time = 1.558s\n",
      "Epoch: 2430, Train loss: 2.902, Val loss: 2.913, Epoch time = 1.587s\n",
      "Epoch: 2431, Train loss: 3.244, Val loss: 2.776, Epoch time = 1.516s\n",
      "Epoch: 2432, Train loss: 3.179, Val loss: 2.850, Epoch time = 1.489s\n",
      "Epoch: 2433, Train loss: 3.357, Val loss: 3.153, Epoch time = 1.517s\n",
      "Epoch: 2434, Train loss: 3.057, Val loss: 2.907, Epoch time = 1.545s\n",
      "Epoch: 2435, Train loss: 2.640, Val loss: 2.832, Epoch time = 1.518s\n",
      "Epoch: 2436, Train loss: 3.439, Val loss: 3.571, Epoch time = 1.570s\n",
      "Epoch: 2437, Train loss: 2.631, Val loss: 3.195, Epoch time = 1.610s\n",
      "Epoch: 2438, Train loss: 2.770, Val loss: 2.825, Epoch time = 1.598s\n",
      "Epoch: 2439, Train loss: 2.683, Val loss: 3.572, Epoch time = 1.535s\n",
      "Epoch: 2440, Train loss: 2.541, Val loss: 3.090, Epoch time = 1.602s\n",
      "Epoch: 2441, Train loss: 2.929, Val loss: 3.410, Epoch time = 1.635s\n",
      "Epoch: 2442, Train loss: 2.756, Val loss: 2.750, Epoch time = 1.587s\n",
      "Epoch: 2443, Train loss: 3.034, Val loss: 2.819, Epoch time = 1.523s\n",
      "Epoch: 2444, Train loss: 2.828, Val loss: 2.857, Epoch time = 1.518s\n",
      "Epoch: 2445, Train loss: 2.510, Val loss: 3.108, Epoch time = 1.525s\n",
      "Epoch: 2446, Train loss: 2.675, Val loss: 3.115, Epoch time = 1.572s\n",
      "Epoch: 2447, Train loss: 3.120, Val loss: 3.583, Epoch time = 1.597s\n",
      "Epoch: 2448, Train loss: 2.523, Val loss: 2.917, Epoch time = 1.556s\n",
      "Epoch: 2449, Train loss: 3.192, Val loss: 2.971, Epoch time = 1.548s\n",
      "Epoch: 2450, Train loss: 2.527, Val loss: 2.660, Epoch time = 1.565s\n",
      "Epoch: 2451, Train loss: 2.816, Val loss: 3.272, Epoch time = 1.525s\n",
      "Epoch: 2452, Train loss: 2.988, Val loss: 2.829, Epoch time = 1.648s\n",
      "Epoch: 2453, Train loss: 2.873, Val loss: 2.816, Epoch time = 1.610s\n",
      "Epoch: 2454, Train loss: 2.834, Val loss: 2.921, Epoch time = 1.611s\n",
      "Epoch: 2455, Train loss: 3.135, Val loss: 3.579, Epoch time = 1.600s\n",
      "Epoch: 2456, Train loss: 2.854, Val loss: 2.610, Epoch time = 1.585s\n",
      "Epoch: 2457, Train loss: 2.881, Val loss: 3.241, Epoch time = 1.558s\n",
      "Epoch: 2458, Train loss: 2.864, Val loss: 2.970, Epoch time = 1.564s\n",
      "Epoch: 2459, Train loss: 2.565, Val loss: 2.804, Epoch time = 1.558s\n",
      "Epoch: 2460, Train loss: 3.040, Val loss: 2.850, Epoch time = 1.575s\n",
      "Epoch: 2461, Train loss: 2.793, Val loss: 3.084, Epoch time = 1.555s\n",
      "Epoch: 2462, Train loss: 2.637, Val loss: 2.920, Epoch time = 1.557s\n",
      "Epoch: 2463, Train loss: 2.961, Val loss: 3.103, Epoch time = 1.597s\n",
      "Epoch: 2464, Train loss: 3.254, Val loss: 2.659, Epoch time = 1.679s\n",
      "Epoch: 2465, Train loss: 2.773, Val loss: 3.382, Epoch time = 1.619s\n",
      "Epoch: 2466, Train loss: 2.956, Val loss: 2.638, Epoch time = 1.647s\n",
      "Epoch: 2467, Train loss: 2.790, Val loss: 2.774, Epoch time = 1.592s\n",
      "Epoch: 2468, Train loss: 3.016, Val loss: 3.056, Epoch time = 1.598s\n",
      "Epoch: 2469, Train loss: 3.544, Val loss: 2.999, Epoch time = 1.616s\n",
      "Epoch: 2470, Train loss: 2.777, Val loss: 3.175, Epoch time = 1.601s\n",
      "Epoch: 2471, Train loss: 2.894, Val loss: 3.020, Epoch time = 1.666s\n",
      "Epoch: 2472, Train loss: 2.916, Val loss: 2.788, Epoch time = 1.621s\n",
      "Epoch: 2473, Train loss: 3.370, Val loss: 3.743, Epoch time = 1.638s\n",
      "Epoch: 2474, Train loss: 2.862, Val loss: 2.596, Epoch time = 1.623s\n",
      "Epoch: 2475, Train loss: 2.897, Val loss: 2.560, Epoch time = 1.621s\n",
      "Epoch: 2476, Train loss: 3.230, Val loss: 2.741, Epoch time = 1.590s\n",
      "Epoch: 2477, Train loss: 3.243, Val loss: 3.279, Epoch time = 1.542s\n",
      "Epoch: 2478, Train loss: 2.778, Val loss: 2.668, Epoch time = 1.623s\n",
      "Epoch: 2479, Train loss: 2.703, Val loss: 2.948, Epoch time = 1.580s\n",
      "Epoch: 2480, Train loss: 2.605, Val loss: 2.873, Epoch time = 1.521s\n",
      "Epoch: 2481, Train loss: 3.031, Val loss: 3.363, Epoch time = 1.573s\n",
      "Epoch: 2482, Train loss: 3.167, Val loss: 3.084, Epoch time = 1.587s\n",
      "Epoch: 2483, Train loss: 3.003, Val loss: 3.582, Epoch time = 1.524s\n",
      "Epoch: 2484, Train loss: 3.003, Val loss: 2.675, Epoch time = 1.587s\n",
      "Epoch: 2485, Train loss: 2.621, Val loss: 2.677, Epoch time = 1.556s\n",
      "Epoch: 2486, Train loss: 2.909, Val loss: 2.932, Epoch time = 1.613s\n",
      "Epoch: 2487, Train loss: 2.989, Val loss: 2.607, Epoch time = 1.641s\n",
      "Epoch: 2488, Train loss: 2.702, Val loss: 2.800, Epoch time = 1.530s\n",
      "Epoch: 2489, Train loss: 3.109, Val loss: 2.712, Epoch time = 1.531s\n",
      "Epoch: 2490, Train loss: 2.774, Val loss: 2.944, Epoch time = 1.597s\n",
      "Epoch: 2491, Train loss: 2.778, Val loss: 3.006, Epoch time = 1.503s\n",
      "Epoch: 2492, Train loss: 2.892, Val loss: 2.664, Epoch time = 1.483s\n",
      "Epoch: 2493, Train loss: 2.872, Val loss: 2.853, Epoch time = 1.531s\n",
      "Epoch: 2494, Train loss: 3.331, Val loss: 2.880, Epoch time = 1.477s\n",
      "Epoch: 2495, Train loss: 2.784, Val loss: 3.026, Epoch time = 1.530s\n",
      "Epoch: 2496, Train loss: 3.172, Val loss: 3.336, Epoch time = 1.580s\n",
      "Epoch: 2497, Train loss: 2.936, Val loss: 2.859, Epoch time = 1.521s\n",
      "Epoch: 2498, Train loss: 2.780, Val loss: 2.710, Epoch time = 1.534s\n",
      "Epoch: 2499, Train loss: 3.188, Val loss: 2.932, Epoch time = 1.614s\n",
      "Epoch: 2500, Train loss: 2.744, Val loss: 2.841, Epoch time = 1.671s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 2500\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "torch.save(transformer.state_dict(), 'AttTrack2_new.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning mean of loss over epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstop\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_over_time= np.loadtxt('./train_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=100\n",
    "plt.plot(np.convolve(loss_over_time, np.ones(N)/N, mode='valid'),c='red',label='train loss')\n",
    "plt.plot(np.convolve(test_error, np.ones(N)/N, mode='valid'),label='test loss')\n",
    "plt.title('Running mean of loss over epochs')\n",
    "plt.legend()\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "k--- 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     Ad_real\u001b[38;5;241m=\u001b[39m convert_tensor(Ad_real[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     36\u001b[0m     l \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 37\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAd_real\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     g\u001b[38;5;241m.\u001b[39mappend(s)\n\u001b[1;32m     39\u001b[0m lo\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(g))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "a=np.linspace(0.01,1,num=1)\n",
    "#a=[0.1]\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack2_new.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "convert_tensor = transforms.ToTensor()\n",
    "lo=[]\n",
    "for k in range(len(a)):\n",
    "    print(lo)\n",
    "    print('k---',k)\n",
    "    g=[]\n",
    "    for v in range(10):\n",
    "        #print('v-',v)\n",
    "\n",
    "\n",
    "        src1, src2, y,d = collate_fn(1,-100,train=False)\n",
    "\n",
    "        src1= src1.to(DEVICE)\n",
    "        src2= src2.to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        src_padding_mask1=create_mask(src1,-100)\n",
    "        src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "        Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "        #print(Ad[0])\n",
    "\n",
    "        Ad_real = complete_postprocess(Ad,d,a[k])\n",
    "        #print(Ad_real[0])\n",
    "        #print(y[0])\n",
    "        \n",
    "        Ad_real= convert_tensor(Ad_real[0])\n",
    "\n",
    "\n",
    "        l = nn.CrossEntropyLoss()\n",
    "        s = l(Ad_real[0], y[0])\n",
    "        g.append(s)\n",
    "    lo.append(np.mean(g))\n",
    "\n",
    "plt.plot(a,lo)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#postprocess Training\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer(num_encoder_layers, emb_size, nhead)\n",
    "\n",
    "\n",
    "NUM_EPOCHS=1000\n",
    "\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0,tra_to_tens=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch_post_process(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_pp.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "#torch.save(transformer.state_dict(), 'AttTrack24.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Ad tensor([[1.0000e+00, 1.3208e-11, 3.2145e-14, 1.0000e+00, 2.5342e-11],\n",
      "        [3.6520e-11, 6.9168e-12, 1.8871e-09, 8.2978e-11, 1.0000e+00],\n",
      "        [6.1533e-13, 5.4397e-12, 1.0000e+00, 1.4898e-08, 1.4453e-07],\n",
      "        [1.8162e-11, 1.0000e+00, 2.5861e-15, 2.1749e-10, 2.7834e-13]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.3077e-13, 1.2035e-07, 1.0327e-12],\n",
      "        [9.7723e-12, 1.0000e+00, 4.4373e-12, 5.3588e-11],\n",
      "        [2.9410e-10, 3.8465e-07, 1.0000e+00, 1.6540e-13],\n",
      "        [1.5394e-13, 5.1452e-12, 2.7423e-05, 1.0000e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 4.5780e-15, 2.4484e-12, 5.4531e-08],\n",
      "        [1.2219e-11, 1.0000e+00, 4.8334e-16, 1.1085e-07],\n",
      "        [2.5520e-09, 5.2584e-13, 1.0264e-13, 1.0000e+00],\n",
      "        [1.7476e-13, 2.9836e-09, 1.0000e+00, 4.0618e-17]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.7204e-07, 1.4283e-11, 5.9754e-15],\n",
      "        [1.6408e-12, 2.9690e-11, 1.1103e-09, 1.0000e+00],\n",
      "        [5.1200e-13, 3.1887e-12, 1.0000e+00, 1.5771e-10],\n",
      "        [5.1187e-11, 1.0000e+00, 2.0329e-17, 2.4731e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.3778e-11, 9.4094e-13, 5.6794e-08],\n",
      "        [5.6078e-10, 1.7999e-16, 3.9728e-09, 1.0000e+00],\n",
      "        [5.0169e-10, 1.0000e+00, 5.6735e-12, 1.7679e-17],\n",
      "        [2.8110e-13, 1.8593e-09, 1.0000e+00, 2.0718e-14]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 5.3010e-11, 3.0746e-11, 8.8385e-10],\n",
      "        [1.8258e-11, 7.7370e-12, 2.5471e-12, 1.0000e+00],\n",
      "        [2.4364e-11, 1.0000e+00, 4.8361e-10, 3.2161e-11],\n",
      "        [1.4853e-10, 3.4358e-16, 1.0000e+00, 1.3083e-11]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.1496e-08, 7.6294e-10, 2.7973e-10],\n",
      "        [5.8125e-16, 1.3776e-13, 1.0000e+00, 8.1550e-09],\n",
      "        [1.8795e-11, 1.0000e+00, 6.7891e-11, 3.3171e-11],\n",
      "        [5.2860e-12, 1.5653e-09, 2.4449e-13, 1.0000e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.3208e-11, 3.2145e-14, 1.0000e+00, 2.5342e-11],\n",
      "        [3.6520e-11, 6.9168e-12, 1.8871e-09, 8.2978e-11, 1.0000e+00],\n",
      "        [6.1533e-13, 5.4397e-12, 1.0000e+00, 1.4898e-08, 1.4453e-07],\n",
      "        [1.8162e-11, 1.0000e+00, 2.5861e-15, 2.1749e-10, 2.7834e-13]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 6.6007e-13, 1.1318e-08, 1.7676e-15],\n",
      "        [1.8950e-11, 1.0000e+00, 9.0532e-14, 4.3479e-11],\n",
      "        [9.2543e-13, 1.2062e-10, 1.0772e-09, 1.0000e+00],\n",
      "        [1.0000e+00, 6.4033e-14, 4.5544e-11, 6.4841e-11],\n",
      "        [1.1846e-13, 7.2001e-12, 1.0000e+00, 1.2289e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 6.9110e-10, 1.7525e-11, 1.1388e-10, 1.6778e-15],\n",
      "        [1.6887e-14, 9.7213e-09, 3.8230e-13, 5.2771e-13, 1.0000e+00],\n",
      "        [1.7190e-11, 1.0000e+00, 5.4769e-08, 3.7179e-12, 2.2496e-11],\n",
      "        [1.4517e-10, 1.9021e-09, 1.0000e+00, 1.0000e+00, 1.4217e-12]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 7.6914e-09, 7.5075e-11, 1.3244e-16, 1.4587e-13, 7.5266e-11],\n",
      "        [1.1752e-09, 1.0000e+00, 1.0000e+00, 1.1731e-09, 6.4650e-10, 6.6071e-15],\n",
      "        [1.7809e-13, 1.2890e-07, 2.4982e-12, 1.0000e+00, 4.1306e-07, 8.4141e-08],\n",
      "        [2.7812e-09, 6.6308e-15, 6.4542e-14, 1.1681e-08, 5.2690e-09, 1.0000e+00],\n",
      "        [4.6279e-16, 1.0908e-13, 2.7620e-09, 5.4781e-09, 1.0000e+00, 2.8646e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.2692e-09, 5.6027e-09, 9.0579e-09, 2.2377e-09, 1.1550e-11,\n",
      "         1.0000e+00, 1.9998e-10, 1.1938e-16],\n",
      "        [1.4177e-09, 1.2521e-05, 2.3625e-15, 1.0000e+00, 4.7181e-10, 2.7052e-09,\n",
      "         6.2770e-07, 8.5135e-11, 6.0502e-05],\n",
      "        [4.9470e-14, 5.8190e-16, 1.0445e-09, 4.0229e-04, 4.9489e-07, 2.3122e-08,\n",
      "         8.8604e-11, 9.4378e-09, 1.0000e+00],\n",
      "        [3.5256e-12, 1.1991e-05, 1.0000e+00, 1.0543e-09, 1.0000e+00, 4.0779e-13,\n",
      "         3.1017e-13, 1.0296e-16, 5.5739e-08],\n",
      "        [2.8684e-10, 2.0543e-15, 1.0000e+00, 3.3122e-17, 1.0000e+00, 5.2431e-15,\n",
      "         1.0929e-08, 1.4565e-08, 3.0845e-08],\n",
      "        [1.3916e-08, 1.0354e-07, 3.5062e-08, 1.8788e-15, 2.6100e-08, 4.2738e-07,\n",
      "         8.3583e-11, 1.0000e+00, 1.0232e-09]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 6.3410e-09, 6.7496e-10, 1.4914e-16, 1.6263e-13, 8.2645e-08,\n",
      "         2.0250e-08],\n",
      "        [5.7263e-11, 2.5515e-15, 1.0000e+00, 1.3124e-16, 2.3937e-07, 1.0000e+00,\n",
      "         2.0650e-07],\n",
      "        [6.6118e-11, 1.0000e+00, 2.1456e-05, 1.1231e-07, 2.2549e-15, 1.5858e-17,\n",
      "         1.2448e-05],\n",
      "        [2.2109e-06, 1.0036e-15, 8.0798e-11, 3.2866e-06, 1.0000e+00, 6.6684e-06,\n",
      "         1.3732e-12],\n",
      "        [7.3725e-10, 3.7374e-04, 1.0000e+00, 5.6534e-12, 1.2680e-08, 4.3220e-07,\n",
      "         7.8586e-14],\n",
      "        [5.4143e-10, 6.0528e-11, 6.7916e-10, 2.2919e-06, 7.0228e-11, 6.4873e-07,\n",
      "         1.1787e-08],\n",
      "        [1.0000e+00, 3.3976e-08, 1.2526e-09, 7.8690e-14, 3.5548e-07, 8.6249e-11,\n",
      "         3.4587e-13],\n",
      "        [1.2129e-08, 2.2727e-10, 6.2956e-13, 2.3280e-07, 9.1339e-17, 1.0540e-10,\n",
      "         1.0000e+00],\n",
      "        [2.4053e-15, 4.3245e-06, 2.8047e-16, 1.0000e+00, 8.1683e-07, 5.6715e-15,\n",
      "         5.7094e-09]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 3.5656e-08, 9.1908e-11, 7.7482e-08, 1.8867e-15, 7.9615e-12,\n",
      "         4.6705e-10],\n",
      "        [1.8433e-09, 2.9954e-13, 1.0000e+00, 3.4922e-06, 6.8350e-08, 3.6897e-12,\n",
      "         2.2965e-14],\n",
      "        [5.1902e-12, 4.8349e-12, 1.3854e-09, 6.7160e-09, 8.5231e-13, 1.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [7.0134e-17, 6.6990e-07, 9.9290e-08, 1.5435e-07, 1.0000e+00, 1.6194e-16,\n",
      "         1.8836e-16],\n",
      "        [7.7864e-12, 1.0000e+00, 3.2367e-11, 2.9928e-16, 1.8713e-06, 6.5708e-09,\n",
      "         5.5405e-11],\n",
      "        [1.0723e-11, 1.8396e-06, 7.0562e-16, 2.8300e-14, 2.7062e-08, 7.9613e-06,\n",
      "         1.0000e+00],\n",
      "        [7.0484e-10, 5.6643e-18, 8.5046e-07, 1.0000e+00, 2.0760e-08, 5.9672e-09,\n",
      "         2.3208e-13]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 5.5432e-13, 3.0674e-09, 4.0901e-08, 1.0000e+00, 3.5852e-08,\n",
      "         2.9446e-14, 1.7877e-13],\n",
      "        [6.0550e-09, 7.4562e-14, 5.7103e-14, 1.0000e+00, 1.8883e-07, 1.3197e-17,\n",
      "         1.0412e-07, 1.0557e-05],\n",
      "        [3.3792e-13, 1.9537e-11, 1.0000e+00, 4.5278e-09, 2.3022e-15, 1.3690e-06,\n",
      "         1.7784e-07, 1.6222e-09],\n",
      "        [1.6191e-10, 1.5142e-09, 1.7201e-07, 1.9785e-18, 2.7977e-08, 1.0000e+00,\n",
      "         4.6592e-07, 3.8926e-12],\n",
      "        [2.8853e-15, 2.5067e-07, 3.8531e-08, 8.3570e-09, 5.1154e-13, 1.2998e-08,\n",
      "         1.5560e-09, 1.0000e+00],\n",
      "        [2.1181e-13, 1.0025e-06, 2.5474e-07, 1.1177e-05, 1.3012e-14, 6.5283e-08,\n",
      "         1.0000e+00, 8.1365e-18],\n",
      "        [1.1872e-08, 1.0000e+00, 5.5262e-11, 3.7853e-12, 2.3652e-10, 2.1362e-15,\n",
      "         1.2226e-05, 3.3534e-07]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 3.6854e-10, 1.9218e-15, 2.0136e-09, 1.9539e-11, 4.6071e-10,\n",
      "         2.0605e-12],\n",
      "        [5.3939e-13, 9.8380e-09, 7.4503e-06, 1.6441e-06, 8.4064e-14, 2.3178e-11,\n",
      "         1.0000e+00],\n",
      "        [1.0146e-07, 5.6244e-07, 1.0227e-06, 1.0000e+00, 2.2841e-05, 7.6479e-08,\n",
      "         3.6028e-10],\n",
      "        [5.7371e-07, 8.5911e-17, 2.5045e-05, 2.2607e-15, 2.1980e-07, 1.0000e+00,\n",
      "         2.6149e-12],\n",
      "        [1.0000e+00, 1.4291e-07, 3.2929e-16, 1.8148e-10, 9.2495e-12, 7.9239e-10,\n",
      "         1.4617e-08],\n",
      "        [4.9653e-11, 1.0000e+00, 2.4547e-05, 4.6609e-07, 4.6438e-12, 1.3857e-16,\n",
      "         6.1146e-07],\n",
      "        [6.2517e-16, 1.2711e-06, 1.0000e+00, 4.9379e-11, 1.1346e-08, 3.9297e-07,\n",
      "         4.8999e-07],\n",
      "        [3.7189e-12, 8.1172e-08, 1.6303e-06, 1.1948e-08, 1.0000e+00, 1.1546e-08,\n",
      "         3.4708e-11]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.0000e+00, 2.3711e-08, 7.9648e-09, 1.2480e-15, 7.6360e-14,\n",
      "         1.1093e-10, 1.3148e-08],\n",
      "        [5.5825e-08, 9.6570e-10, 2.8720e-15, 9.0374e-07, 6.9365e-08, 3.9584e-06,\n",
      "         1.0000e+00, 1.0750e-11],\n",
      "        [4.8394e-18, 1.2591e-11, 1.4048e-05, 2.9213e-07, 1.2238e-08, 1.0000e+00,\n",
      "         9.1453e-11, 5.9274e-12],\n",
      "        [2.3630e-10, 3.1857e-08, 2.4353e-14, 2.1987e-11, 1.0314e-06, 1.3259e-08,\n",
      "         1.0517e-06, 1.0000e+00],\n",
      "        [6.5984e-11, 4.6698e-09, 7.9529e-06, 1.0000e+00, 7.2727e-10, 8.7391e-07,\n",
      "         8.9445e-06, 8.6595e-12],\n",
      "        [4.9295e-08, 2.6451e-07, 1.0000e+00, 1.0858e-06, 2.5987e-11, 3.2664e-08,\n",
      "         2.0740e-11, 1.1952e-11],\n",
      "        [6.2377e-17, 6.7770e-18, 2.2974e-09, 5.8231e-14, 1.0000e+00, 7.2559e-06,\n",
      "         2.9228e-07, 4.1477e-08]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.8971e-09, 3.9674e-07, 1.1553e-09, 9.7435e-14, 1.9744e-12,\n",
      "         1.0992e-10],\n",
      "        [1.0000e+00, 3.3420e-08, 1.9193e-09, 1.6380e-13, 2.3144e-07, 6.4698e-08,\n",
      "         2.0636e-10],\n",
      "        [4.5306e-08, 1.0000e+00, 1.4955e-11, 1.4411e-16, 2.0908e-07, 3.8958e-16,\n",
      "         2.4212e-05],\n",
      "        [1.5388e-09, 7.8884e-05, 6.5946e-06, 4.5088e-10, 1.3139e-05, 3.3802e-11,\n",
      "         1.0000e+00],\n",
      "        [2.2661e-11, 8.8190e-14, 4.8094e-06, 1.0000e+00, 1.1967e-12, 3.5354e-08,\n",
      "         4.5511e-07],\n",
      "        [1.2766e-09, 2.2997e-07, 7.2842e-07, 1.0726e-11, 1.0000e+00, 2.6252e-07,\n",
      "         7.6513e-07],\n",
      "        [1.0186e-05, 5.2760e-16, 1.0000e+00, 2.7881e-08, 4.4528e-06, 1.3971e-07,\n",
      "         1.2231e-06],\n",
      "        [7.5037e-08, 1.5905e-12, 3.3751e-13, 1.3235e-08, 1.6721e-09, 1.0000e+00,\n",
      "         9.2157e-17]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 7.4238e-08, 7.2422e-10, 1.8636e-06, 4.0429e-09, 3.1636e-11,\n",
      "         2.8215e-17],\n",
      "        [1.0975e-10, 1.0000e+00, 7.2926e-17, 4.1752e-11, 5.9865e-13, 3.9543e-06,\n",
      "         8.1519e-08],\n",
      "        [1.5412e-07, 3.0470e-15, 3.4477e-08, 1.0000e+00, 3.4954e-08, 1.0174e-08,\n",
      "         3.7521e-07],\n",
      "        [9.1785e-08, 5.3045e-08, 8.6910e-10, 1.2129e-13, 1.0000e+00, 4.9372e-11,\n",
      "         1.5511e-09],\n",
      "        [2.1806e-12, 5.8158e-08, 6.6439e-07, 6.4895e-06, 6.4746e-12, 9.6427e-09,\n",
      "         1.0000e+00],\n",
      "        [5.2759e-07, 6.1081e-17, 1.0000e+00, 7.5938e-10, 4.4229e-10, 4.9632e-13,\n",
      "         1.8779e-08],\n",
      "        [2.9687e-15, 8.8869e-10, 4.5086e-13, 7.3062e-06, 1.6923e-07, 1.0000e+00,\n",
      "         1.1164e-08]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.3615e-13, 2.4877e-12, 8.9018e-07, 4.1706e-09, 1.1135e-07,\n",
      "         3.8643e-11, 2.9727e-12, 2.2044e-08],\n",
      "        [9.7861e-11, 3.2110e-12, 3.7641e-12, 3.3891e-08, 1.0000e+00, 3.9010e-13,\n",
      "         1.2919e-09, 3.6837e-06, 5.8218e-09],\n",
      "        [3.9731e-09, 1.8369e-08, 1.8003e-07, 1.2278e-05, 5.5389e-13, 1.0000e+00,\n",
      "         8.3061e-11, 4.1998e-07, 1.2457e-07],\n",
      "        [2.6179e-10, 1.6176e-08, 3.3551e-06, 1.0000e+00, 1.0023e-06, 1.0647e-05,\n",
      "         6.2682e-09, 2.1086e-09, 1.0763e-11],\n",
      "        [3.2555e-10, 1.0000e+00, 8.7867e-09, 2.2932e-09, 2.6841e-11, 9.7398e-09,\n",
      "         8.3386e-08, 9.7438e-13, 1.0000e+00],\n",
      "        [9.5836e-14, 2.0308e-07, 9.0216e-09, 2.1130e-06, 6.6366e-09, 3.3056e-16,\n",
      "         1.0000e+00, 1.0000e+00, 1.8643e-09],\n",
      "        [1.0966e-13, 5.4640e-07, 1.0000e+00, 5.7928e-09, 5.3348e-07, 1.5857e-06,\n",
      "         2.5859e-07, 5.7839e-06, 2.9788e-08]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 5.6821e-07, 3.2107e-08, 8.4264e-10, 4.0343e-10, 1.4878e-08,\n",
      "         9.9680e-09, 1.4063e-12, 1.0725e-07],\n",
      "        [1.3045e-14, 3.6072e-07, 1.5509e-07, 5.5726e-10, 1.4054e-13, 5.9749e-08,\n",
      "         3.3999e-08, 1.0000e+00, 6.1199e-09],\n",
      "        [3.8545e-10, 1.4019e-16, 8.2366e-08, 1.9403e-09, 1.6013e-08, 6.1970e-09,\n",
      "         1.0000e+00, 1.3251e-07, 1.6179e-10],\n",
      "        [1.1713e-11, 4.4027e-15, 5.6466e-06, 4.9333e-07, 3.2987e-09, 5.4639e-10,\n",
      "         5.5035e-09, 3.5945e-09, 1.0000e+00],\n",
      "        [1.6217e-12, 1.3477e-09, 6.0247e-09, 1.0000e+00, 5.4065e-06, 3.3629e-11,\n",
      "         2.0722e-16, 6.6812e-10, 1.2961e-07],\n",
      "        [5.0037e-14, 9.3670e-09, 1.0000e+00, 2.2694e-07, 2.5415e-06, 2.4863e-09,\n",
      "         1.3119e-06, 1.7012e-08, 7.0821e-06],\n",
      "        [7.0686e-10, 4.4368e-08, 2.9286e-14, 9.7661e-11, 8.4579e-06, 1.0000e+00,\n",
      "         1.6326e-08, 1.3278e-06, 4.6197e-09],\n",
      "        [9.2508e-09, 2.9404e-15, 6.9274e-12, 1.1724e-08, 1.0000e+00, 1.2297e-06,\n",
      "         1.2059e-09, 3.9305e-10, 8.9514e-07],\n",
      "        [3.4603e-08, 1.0000e+00, 1.8484e-08, 4.8986e-09, 5.8036e-11, 5.0259e-08,\n",
      "         9.6425e-10, 4.6816e-06, 2.4033e-14]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[1.0000e+00, 6.8383e-10, 9.0930e-09, 1.3986e-08, 1.8819e-08, 4.1949e-08,\n",
      "         1.0000e+00, 4.1769e-09, 4.3367e-13, 2.8554e-07, 1.6143e-15],\n",
      "        [2.0469e-08, 7.1848e-07, 2.9045e-15, 3.7606e-07, 4.3900e-09, 1.0000e+00,\n",
      "         1.4761e-06, 1.2315e-14, 1.1064e-08, 1.5698e-11, 5.1876e-07],\n",
      "        [7.8796e-09, 3.3622e-07, 1.0503e-05, 6.7457e-07, 1.0000e+00, 3.3308e-08,\n",
      "         6.3305e-09, 1.6413e-08, 6.0453e-07, 8.8765e-09, 6.9906e-10],\n",
      "        [1.2833e-09, 2.5090e-08, 2.8234e-13, 1.0000e+00, 2.5776e-12, 2.1368e-06,\n",
      "         2.7721e-12, 3.1815e-07, 2.6644e-06, 4.3113e-08, 1.2655e-06],\n",
      "        [1.5742e-14, 4.4747e-10, 8.8404e-09, 1.5555e-07, 2.1074e-08, 8.9317e-12,\n",
      "         4.7281e-13, 3.4364e-08, 1.0000e+00, 2.2123e-07, 5.8744e-10],\n",
      "        [4.2299e-09, 1.0000e+00, 2.3729e-07, 9.3055e-15, 8.8722e-08, 5.1156e-06,\n",
      "         1.7341e-08, 1.1368e-13, 5.8183e-07, 9.5011e-06, 4.7908e-07],\n",
      "        [1.4219e-11, 5.7207e-07, 1.0000e+00, 1.2491e-16, 3.4321e-06, 2.0263e-12,\n",
      "         3.1585e-08, 3.6489e-09, 4.8510e-07, 9.1949e-08, 1.4609e-07],\n",
      "        [1.2575e-11, 8.5278e-09, 1.6619e-12, 3.5521e-12, 2.5130e-08, 1.0138e-05,\n",
      "         2.3660e-09, 1.2747e-06, 2.5206e-09, 5.3463e-15, 1.0000e+00],\n",
      "        [7.7913e-09, 5.8077e-10, 8.6965e-10, 4.5013e-09, 7.1253e-11, 3.8842e-16,\n",
      "         3.3012e-09, 1.0000e+00, 2.3515e-10, 1.0000e+00, 2.5525e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.2739e-07, 1.1768e-11, 1.0000e+00, 7.5496e-08, 9.5298e-08,\n",
      "         1.2124e-09, 2.1969e-10, 1.7308e-15, 1.7403e-09, 1.7326e-09],\n",
      "        [8.8561e-07, 7.1867e-07, 1.0093e-11, 9.3176e-08, 1.3183e-11, 1.3165e-06,\n",
      "         2.8669e-11, 1.2307e-05, 4.7837e-06, 4.3314e-12, 1.0000e+00],\n",
      "        [7.0637e-06, 1.0000e+00, 9.8148e-09, 5.2777e-10, 3.8038e-16, 4.6468e-09,\n",
      "         3.6439e-11, 1.4961e-13, 1.9338e-08, 9.6025e-06, 1.3304e-07],\n",
      "        [3.9790e-10, 8.4034e-15, 3.1534e-09, 1.1773e-12, 1.0000e+00, 5.6461e-10,\n",
      "         6.0863e-09, 9.8309e-11, 2.8247e-10, 8.9310e-07, 2.2061e-06],\n",
      "        [7.2702e-07, 4.7339e-13, 2.3671e-07, 1.8841e-09, 8.1110e-10, 1.0000e+00,\n",
      "         1.8338e-10, 1.5000e-06, 1.2424e-06, 1.2597e-13, 1.6899e-09],\n",
      "        [2.5604e-07, 2.0192e-16, 7.1095e-09, 1.5365e-11, 5.5623e-07, 1.9295e-07,\n",
      "         5.9101e-16, 1.0000e+00, 4.2330e-07, 7.3506e-07, 3.8144e-07],\n",
      "        [1.0000e+00, 3.4125e-07, 3.1105e-07, 1.0483e-03, 1.0435e-11, 1.5286e-08,\n",
      "         1.8491e-16, 3.7531e-07, 1.1467e-15, 3.1945e-10, 4.5414e-11],\n",
      "        [2.1479e-12, 2.4325e-09, 4.7193e-12, 1.4379e-06, 1.3175e-06, 1.2779e-09,\n",
      "         1.0000e+00, 2.1861e-08, 7.3370e-06, 2.2271e-06, 2.0411e-15],\n",
      "        [1.4347e-12, 1.8427e-06, 1.0000e+00, 1.4256e-11, 3.9910e-08, 1.1523e-06,\n",
      "         3.9485e-07, 3.2629e-14, 7.4466e-06, 5.8601e-09, 6.4205e-10],\n",
      "        [2.4666e-08, 5.3785e-07, 1.6753e-11, 3.0678e-10, 2.9853e-08, 6.4652e-15,\n",
      "         1.3389e-06, 5.8039e-08, 7.9177e-16, 1.0000e+00, 5.5392e-07],\n",
      "        [3.2890e-12, 9.7600e-07, 1.4628e-08, 1.2829e-11, 5.4554e-11, 1.3626e-07,\n",
      "         3.9768e-05, 4.5221e-08, 1.0000e+00, 8.0835e-10, 3.9569e-09]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.4382e-15, 1.2771e-07, 7.7552e-09, 7.5005e-07, 4.5247e-13,\n",
      "         1.2835e-06, 1.1873e-08, 3.9862e-11, 1.1763e-08, 2.1675e-10],\n",
      "        [3.0515e-11, 5.1706e-08, 6.0428e-09, 4.7533e-11, 9.0269e-13, 1.7648e-08,\n",
      "         1.3004e-06, 6.6845e-06, 7.9299e-19, 1.0000e+00, 7.4415e-12],\n",
      "        [4.7698e-09, 2.2709e-11, 4.3503e-07, 8.3855e-10, 6.2569e-10, 1.5540e-06,\n",
      "         1.0000e+00, 7.4796e-13, 1.5343e-15, 1.6232e-05, 2.0983e-07],\n",
      "        [1.0000e+00, 4.3449e-05, 2.3572e-13, 1.5481e-08, 1.6287e-08, 3.3289e-08,\n",
      "         2.7065e-06, 1.1197e-07, 7.7845e-11, 9.0736e-07, 1.7986e-07],\n",
      "        [6.4492e-10, 7.0687e-07, 8.6825e-06, 2.7004e-10, 1.0100e-09, 2.4136e-07,\n",
      "         2.4243e-14, 1.8326e-07, 1.0000e+00, 6.2004e-18, 1.3562e-13],\n",
      "        [1.2458e-07, 1.3582e-07, 2.3565e-09, 1.0000e+00, 2.0190e-05, 4.0500e-08,\n",
      "         1.2911e-08, 1.3471e-11, 1.2270e-07, 4.5341e-07, 1.0000e+00],\n",
      "        [1.7283e-12, 1.0000e+00, 3.0513e-15, 8.4179e-15, 4.5416e-12, 4.2605e-06,\n",
      "         1.1261e-08, 6.8343e-06, 1.4465e-06, 3.6806e-07, 8.8669e-11],\n",
      "        [5.9956e-08, 3.4728e-06, 1.0792e-12, 1.6610e-04, 1.0000e+00, 2.7118e-08,\n",
      "         1.6699e-16, 7.4209e-07, 1.3074e-09, 1.3598e-08, 1.5589e-07],\n",
      "        [2.1765e-09, 7.1083e-08, 2.4062e-06, 9.1063e-08, 1.6832e-11, 1.0000e+00,\n",
      "         1.2998e-07, 2.6466e-13, 1.1411e-09, 1.4956e-08, 6.1586e-11],\n",
      "        [4.8765e-12, 7.2007e-06, 2.2208e-08, 2.3433e-12, 4.3466e-07, 8.0966e-15,\n",
      "         6.7619e-13, 1.0000e+00, 1.5840e-05, 2.3364e-06, 3.2870e-18],\n",
      "        [7.6799e-09, 5.9383e-11, 1.0000e+00, 2.4487e-11, 1.1412e-11, 2.7175e-08,\n",
      "         4.4789e-10, 1.9189e-10, 9.5517e-06, 1.7975e-11, 4.2035e-05]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.9441e-06, 7.9413e-12, 3.9573e-07, 7.1001e-12, 2.1569e-12,\n",
      "         1.4374e-10, 1.6188e-10, 5.6203e-10, 1.0000e+00, 1.0327e-11, 1.5388e-06],\n",
      "        [2.8109e-16, 4.6958e-06, 1.2516e-07, 6.8221e-11, 1.1464e-06, 1.0000e+00,\n",
      "         3.8748e-11, 1.2940e-06, 1.5683e-06, 2.3001e-14, 5.6524e-09, 1.4920e-10],\n",
      "        [9.8685e-06, 2.9716e-06, 2.5529e-09, 1.0000e+00, 1.3846e-04, 2.6090e-10,\n",
      "         3.9866e-12, 1.7096e-08, 1.3460e-11, 4.7506e-06, 4.1391e-13, 1.6009e-10],\n",
      "        [1.0613e-06, 3.6669e-08, 2.5416e-07, 1.1717e-16, 1.6990e-06, 6.9167e-12,\n",
      "         1.0000e+00, 8.3763e-09, 1.3544e-10, 2.8327e-10, 1.0000e+00, 6.9732e-12],\n",
      "        [7.9970e-08, 2.3500e-09, 8.9489e-09, 8.5638e-15, 4.3695e-12, 7.2306e-07,\n",
      "         8.1397e-05, 2.5262e-05, 4.6890e-13, 2.2807e-08, 1.0000e+00, 4.3091e-11],\n",
      "        [4.6492e-12, 3.3205e-15, 2.1052e-08, 2.9190e-11, 5.5105e-04, 2.3381e-08,\n",
      "         3.5147e-10, 1.7868e-11, 1.0000e+00, 5.0030e-11, 1.4179e-11, 1.1256e-06],\n",
      "        [2.6146e-08, 1.7307e-13, 2.6737e-08, 2.3751e-07, 9.8522e-19, 1.1103e-08,\n",
      "         7.0289e-11, 9.6655e-08, 1.9167e-05, 2.5896e-06, 1.2935e-08, 1.0000e+00],\n",
      "        [3.2272e-07, 1.0000e+00, 1.4477e-16, 4.5548e-07, 5.3210e-10, 1.0837e-06,\n",
      "         1.1708e-07, 8.2317e-07, 7.3531e-11, 3.0075e-07, 1.3022e-08, 6.7245e-13],\n",
      "        [1.1259e-06, 1.2494e-05, 5.3774e-10, 1.8576e-05, 1.0000e+00, 4.9362e-09,\n",
      "         7.0753e-07, 2.4967e-12, 6.0827e-08, 1.2180e-07, 6.6762e-08, 2.4374e-17],\n",
      "        [1.9611e-14, 5.4629e-12, 1.2845e-09, 2.0598e-09, 3.7812e-13, 4.5916e-06,\n",
      "         4.7252e-07, 1.0000e+00, 2.6903e-08, 1.2985e-13, 1.7903e-07, 1.6100e-09],\n",
      "        [4.3251e-11, 3.2735e-16, 1.0000e+00, 7.8599e-10, 4.8310e-09, 5.4096e-10,\n",
      "         8.4771e-08, 2.8654e-08, 3.5168e-09, 1.7447e-11, 2.1693e-05, 9.2690e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.6859e-08, 1.8960e-07, 8.4621e-14, 1.7480e-13, 1.1937e-11,\n",
      "         1.0000e+00, 1.7487e-08, 5.5260e-08, 1.0561e-06, 4.5447e-12, 4.3121e-08],\n",
      "        [5.4920e-08, 1.0000e+00, 5.2889e-11, 1.2427e-07, 2.5483e-15, 3.0591e-09,\n",
      "         1.2835e-07, 2.2446e-05, 2.8380e-13, 1.3598e-12, 1.2762e-06, 5.0024e-09],\n",
      "        [1.5882e-09, 2.7749e-16, 1.0000e+00, 1.5918e-07, 7.5627e-11, 1.3314e-10,\n",
      "         2.3987e-08, 4.4052e-09, 6.1927e-07, 7.9119e-07, 2.0861e-06, 4.5111e-07],\n",
      "        [2.4932e-05, 5.2640e-07, 1.3534e-09, 1.4569e-07, 2.8704e-07, 1.0000e+00,\n",
      "         5.1654e-07, 1.5186e-05, 4.1610e-19, 1.9844e-13, 5.7534e-06, 3.5011e-20],\n",
      "        [1.2970e-11, 2.0546e-07, 6.9128e-05, 3.3735e-08, 1.2320e-06, 6.8572e-07,\n",
      "         2.7792e-09, 1.0000e+00, 6.7059e-07, 7.5081e-16, 8.9475e-10, 2.4912e-08],\n",
      "        [2.4692e-11, 2.5087e-05, 4.2903e-06, 1.0000e+00, 1.5950e-07, 2.8430e-09,\n",
      "         3.6516e-10, 2.7736e-12, 1.0792e-06, 1.1831e-05, 1.6018e-08, 2.6213e-13],\n",
      "        [4.3163e-09, 8.4887e-09, 3.4244e-11, 1.0603e-11, 3.8259e-08, 9.7714e-10,\n",
      "         7.8376e-09, 1.2543e-07, 1.0000e+00, 1.5063e-11, 1.7235e-09, 1.0000e+00],\n",
      "        [4.1370e-12, 1.3637e-10, 2.9085e-05, 3.3714e-07, 2.3616e-07, 5.3512e-07,\n",
      "         5.2048e-13, 3.3329e-10, 9.0810e-15, 6.1343e-14, 1.0000e+00, 1.5591e-11],\n",
      "        [2.2985e-13, 1.6936e-12, 8.6271e-09, 1.5332e-10, 1.0000e+00, 4.1739e-07,\n",
      "         1.6136e-13, 1.4445e-04, 7.0937e-10, 2.1345e-06, 8.1006e-11, 6.2832e-10],\n",
      "        [1.0000e+00, 2.9945e-08, 6.5681e-07, 6.6088e-11, 2.6208e-11, 2.6254e-07,\n",
      "         1.0000e+00, 1.3132e-09, 4.1266e-10, 4.0861e-06, 1.1833e-08, 3.7302e-13],\n",
      "        [2.0164e-10, 3.7710e-09, 4.2576e-10, 6.6090e-11, 1.5399e-07, 1.4264e-14,\n",
      "         2.8991e-09, 7.9057e-09, 1.0000e+00, 8.7076e-07, 1.0201e-08, 1.0000e+00],\n",
      "        [9.4712e-07, 1.8611e-11, 2.3090e-06, 5.4427e-06, 2.2108e-05, 7.1176e-09,\n",
      "         1.6452e-07, 9.1639e-18, 1.6055e-09, 1.0000e+00, 2.0060e-10, 3.2530e-10]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.9884e-10, 1.4758e-09, 1.8399e-08, 1.1709e-14, 3.3448e-06,\n",
      "         1.6078e-14, 4.9312e-08, 2.2451e-09, 4.2712e-09, 2.7180e-10, 1.2699e-12,\n",
      "         4.2362e-07],\n",
      "        [4.3743e-11, 1.2367e-11, 2.2035e-06, 2.0397e-08, 1.0055e-07, 3.1538e-13,\n",
      "         4.1348e-11, 6.6618e-12, 1.0000e+00, 2.4279e-10, 3.0026e-08, 6.6608e-09,\n",
      "         1.0413e-09],\n",
      "        [1.6964e-06, 1.4417e-14, 1.5972e-13, 1.1858e-08, 9.4212e-06, 4.2528e-06,\n",
      "         7.7340e-08, 2.1642e-09, 2.8772e-08, 4.1504e-09, 8.9088e-12, 1.5149e-09,\n",
      "         1.0000e+00],\n",
      "        [2.6802e-11, 9.0743e-07, 3.4794e-12, 3.1802e-08, 1.0000e+00, 8.4182e-12,\n",
      "         1.0105e-05, 3.9356e-06, 8.2041e-05, 1.5049e-10, 8.2154e-08, 1.8388e-11,\n",
      "         4.3525e-05],\n",
      "        [3.1915e-07, 1.0000e+00, 3.4551e-13, 4.3907e-08, 1.6044e-07, 2.0117e-11,\n",
      "         1.0000e+00, 3.1465e-07, 1.6036e-18, 1.7402e-06, 4.7980e-07, 1.2823e-09,\n",
      "         3.0871e-17],\n",
      "        [1.1819e-07, 9.5164e-06, 1.0812e-09, 5.3812e-13, 5.0852e-11, 1.0000e+00,\n",
      "         8.2055e-10, 1.1765e-08, 3.0690e-07, 1.2247e-05, 5.1355e-06, 7.8113e-09,\n",
      "         6.0589e-08],\n",
      "        [1.0000e+00, 2.2934e-10, 1.7427e-11, 9.4521e-07, 1.7414e-12, 1.0158e-05,\n",
      "         8.1954e-12, 9.1710e-07, 8.1636e-08, 3.3568e-08, 2.0908e-10, 4.0081e-11,\n",
      "         3.1735e-05],\n",
      "        [4.1521e-05, 1.9480e-11, 3.8279e-17, 2.3215e-15, 8.0812e-11, 1.3210e-07,\n",
      "         1.9369e-06, 1.0471e-06, 2.8879e-08, 1.0000e+00, 1.1764e-12, 1.0000e+00,\n",
      "         1.9672e-09],\n",
      "        [1.1086e-06, 6.3385e-07, 4.8091e-06, 1.0871e-08, 4.0466e-07, 1.9557e-09,\n",
      "         8.4851e-13, 1.0000e+00, 9.5402e-12, 3.7960e-06, 1.1879e-12, 2.9453e-09,\n",
      "         2.8241e-09],\n",
      "        [3.8991e-07, 8.8405e-07, 2.8732e-07, 1.0000e+00, 3.3005e-10, 1.5125e-12,\n",
      "         4.6662e-07, 1.1644e-07, 5.4394e-09, 2.2277e-15, 1.3978e-10, 6.4324e-15,\n",
      "         1.2415e-08],\n",
      "        [2.5028e-13, 3.2431e-08, 1.2078e-14, 1.8208e-17, 1.8928e-07, 6.1009e-05,\n",
      "         8.5069e-06, 4.0336e-12, 1.3726e-05, 2.5831e-14, 1.0000e+00, 1.0785e-08,\n",
      "         7.1157e-08],\n",
      "        [8.4834e-10, 1.3674e-10, 1.0000e+00, 2.8996e-06, 2.8541e-12, 1.5874e-10,\n",
      "         2.4245e-13, 2.1042e-05, 2.3474e-06, 1.2365e-07, 2.3875e-16, 4.9057e-09,\n",
      "         4.3443e-09]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 3.8208e-10, 6.6449e-09, 1.1281e-10, 9.1678e-10, 1.7455e-08,\n",
      "         1.9738e-06, 6.2367e-14, 5.1697e-06, 1.3863e-13, 6.5310e-08, 2.3014e-15,\n",
      "         2.3950e-09],\n",
      "        [1.3835e-06, 1.2126e-11, 1.0000e+00, 1.7588e-07, 3.9146e-12, 7.0994e-11,\n",
      "         2.6134e-05, 3.3070e-06, 2.7705e-15, 1.0344e-16, 1.1195e-10, 2.2885e-05,\n",
      "         1.0391e-04],\n",
      "        [5.7090e-07, 5.0768e-10, 3.2089e-10, 1.4795e-08, 8.5481e-05, 3.5837e-10,\n",
      "         1.9225e-09, 2.5074e-14, 1.2492e-13, 1.0973e-11, 1.0000e+00, 2.9156e-13,\n",
      "         1.7409e-05],\n",
      "        [2.2789e-08, 2.9807e-07, 3.8526e-06, 5.0945e-07, 3.5872e-12, 3.8761e-08,\n",
      "         2.4599e-10, 4.3405e-12, 2.3762e-09, 2.8020e-10, 4.5834e-05, 5.0797e-08,\n",
      "         1.0000e+00],\n",
      "        [2.7279e-12, 6.5129e-13, 1.7469e-05, 3.8007e-04, 3.2763e-12, 1.0000e+00,\n",
      "         6.4856e-14, 4.7180e-05, 9.0880e-04, 2.4264e-13, 5.3904e-12, 2.1987e-08,\n",
      "         6.7587e-11],\n",
      "        [3.8285e-05, 5.1487e-10, 2.1116e-05, 1.6666e-13, 4.2784e-09, 1.7697e-11,\n",
      "         1.0000e+00, 4.9469e-05, 8.1142e-09, 9.6389e-07, 2.3138e-09, 1.6371e-10,\n",
      "         7.8961e-11],\n",
      "        [1.2288e-13, 3.0092e-06, 2.1292e-06, 2.8224e-13, 9.3625e-14, 2.6611e-08,\n",
      "         3.2286e-14, 5.5406e-05, 1.4072e-06, 3.0875e-05, 2.9579e-15, 1.0000e+00,\n",
      "         9.7521e-07],\n",
      "        [5.6288e-08, 1.2547e-06, 1.4303e-06, 1.0000e+00, 4.3609e-08, 4.9310e-06,\n",
      "         1.7478e-10, 4.9763e-11, 1.5460e-09, 4.3977e-12, 1.8731e-07, 6.8806e-13,\n",
      "         5.6517e-07],\n",
      "        [3.5171e-14, 1.5934e-10, 4.8788e-12, 1.7082e-07, 1.0000e+00, 4.1870e-11,\n",
      "         3.1406e-07, 9.9074e-05, 6.4191e-12, 4.9726e-06, 4.9660e-08, 8.0926e-15,\n",
      "         1.2071e-14],\n",
      "        [4.0870e-07, 1.0000e+00, 1.6839e-06, 2.7702e-09, 3.3739e-10, 7.9196e-12,\n",
      "         2.7149e-06, 8.5507e-15, 1.6049e-12, 4.0774e-03, 3.0631e-13, 2.4743e-05,\n",
      "         5.8889e-05],\n",
      "        [4.0767e-10, 7.0620e-18, 2.1581e-09, 6.4885e-11, 1.8089e-06, 8.2691e-10,\n",
      "         3.2107e-05, 1.0000e+00, 2.1043e-06, 5.1418e-06, 2.6993e-12, 1.7469e-08,\n",
      "         5.8453e-16],\n",
      "        [1.1411e-12, 1.0029e-01, 8.1052e-17, 1.0664e-10, 3.2661e-05, 1.2600e-11,\n",
      "         4.2176e-06, 1.2054e-07, 2.7177e-07, 1.0000e+00, 8.8042e-13, 7.8559e-07,\n",
      "         1.9356e-13],\n",
      "        [8.2273e-08, 3.1968e-17, 7.4069e-15, 8.2226e-12, 1.5212e-06, 1.0000e+00,\n",
      "         7.1386e-12, 1.5816e-09, 1.0000e+00, 7.6749e-08, 1.6051e-08, 1.0797e-11,\n",
      "         7.3610e-09]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[1.0000e+00, 4.5812e-08, 8.0000e-08, 5.7235e-06, 9.0963e-11, 1.0000e+00,\n",
      "         2.4902e-11, 1.3578e-05, 2.4258e-08, 7.3952e-13, 1.6545e-08, 5.6285e-09,\n",
      "         1.3241e-06, 6.3093e-14, 1.6255e-10, 1.2161e-08],\n",
      "        [4.9032e-14, 2.1791e-05, 1.8081e-18, 3.2173e-06, 2.3294e-03, 5.5915e-13,\n",
      "         4.3110e-14, 6.3750e-04, 1.4278e-08, 5.7221e-15, 3.7959e-17, 1.3531e-06,\n",
      "         2.0402e-12, 8.7915e-05, 1.0000e+00, 7.8634e-08],\n",
      "        [7.0489e-06, 3.8145e-12, 2.0428e-14, 7.6153e-07, 1.7457e-13, 6.8771e-05,\n",
      "         4.3881e-11, 8.1119e-04, 2.1806e-07, 8.7115e-07, 5.5801e-07, 1.0000e+00,\n",
      "         4.0270e-19, 2.0829e-03, 1.8114e-04, 1.3471e-07],\n",
      "        [6.2384e-10, 1.2492e-08, 4.2202e-10, 3.0724e-10, 2.9506e-15, 3.7966e-08,\n",
      "         1.7727e-15, 8.1248e-09, 1.0000e+00, 3.3908e-06, 2.6556e-06, 1.0488e-08,\n",
      "         1.1701e-05, 3.9001e-14, 2.3419e-08, 1.0000e+00],\n",
      "        [8.3778e-06, 1.0000e+00, 7.1890e-10, 4.3633e-14, 5.5901e-04, 1.2625e-06,\n",
      "         3.2644e-05, 2.8506e-09, 5.1245e-06, 7.1927e-05, 2.5069e-09, 8.0503e-14,\n",
      "         1.1392e-06, 2.6152e-10, 2.6448e-09, 2.2795e-07],\n",
      "        [3.1642e-06, 2.9763e-15, 1.0000e+00, 2.9096e-06, 6.6935e-14, 1.7260e-07,\n",
      "         7.2144e-08, 1.1170e-16, 1.0883e-09, 2.3873e-07, 1.0000e+00, 8.2296e-10,\n",
      "         1.7118e-09, 5.7055e-06, 1.8262e-13, 1.4653e-10],\n",
      "        [2.7456e-05, 7.0620e-07, 4.4319e-20, 4.3725e-10, 3.2240e-05, 1.3119e-04,\n",
      "         2.6351e-05, 1.0000e+00, 5.7471e-10, 8.7328e-10, 2.1684e-18, 1.1613e-05,\n",
      "         1.7535e-12, 2.3399e-10, 1.3414e-06, 9.6021e-09],\n",
      "        [4.5846e-09, 6.2335e-08, 1.2782e-07, 1.6624e-16, 1.3803e-05, 1.3509e-06,\n",
      "         1.0000e+00, 8.5622e-06, 3.3353e-09, 1.0000e+00, 6.7213e-06, 1.0728e-09,\n",
      "         4.1202e-15, 3.1918e-07, 1.1973e-12, 4.0051e-10],\n",
      "        [7.2831e-07, 1.0404e-12, 1.0000e+00, 5.5214e-08, 3.9771e-09, 1.9593e-08,\n",
      "         3.1422e-05, 9.4634e-16, 2.1856e-12, 7.8549e-08, 9.9989e-01, 6.5428e-15,\n",
      "         1.9085e-08, 9.8898e-07, 1.6922e-14, 9.3119e-13],\n",
      "        [1.5138e-13, 6.1774e-06, 3.1009e-10, 3.6780e-12, 1.0000e+00, 1.5871e-13,\n",
      "         1.0845e-04, 2.3600e-08, 1.0054e-11, 2.1780e-10, 1.6486e-15, 2.3464e-14,\n",
      "         2.0338e-07, 4.0266e-05, 7.3016e-06, 8.1278e-11],\n",
      "        [1.1520e-05, 7.7786e-06, 1.1364e-07, 1.6729e-06, 2.2031e-10, 1.7179e-07,\n",
      "         2.3757e-17, 5.0414e-09, 6.6044e-05, 1.3887e-18, 4.8497e-14, 1.1436e-10,\n",
      "         1.0000e+00, 1.1036e-11, 3.0424e-11, 3.6365e-05],\n",
      "        [4.7010e-13, 7.6415e-10, 1.3631e-05, 1.5349e-12, 3.6057e-06, 8.2104e-15,\n",
      "         3.5410e-07, 9.3970e-13, 1.0618e-11, 5.3810e-09, 1.8963e-10, 9.5651e-06,\n",
      "         9.7239e-13, 1.0000e+00, 3.6706e-06, 1.5165e-11],\n",
      "        [4.6541e-12, 9.5259e-17, 3.5756e-11, 1.0000e+00, 5.9641e-13, 6.4332e-09,\n",
      "         5.6132e-15, 3.3801e-07, 4.6023e-11, 4.4663e-16, 2.3595e-08, 2.3055e-06,\n",
      "         1.0234e-06, 2.0952e-12, 2.1830e-06, 2.7585e-07]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.1733e-10, 1.9923e-04, 2.1517e-09, 1.0252e-06, 2.2148e-12,\n",
      "         3.3126e-11, 9.7547e-15, 2.8066e-04, 1.0051e-12, 7.4544e-11, 1.6542e-04,\n",
      "         3.9980e-11, 6.8622e-15, 2.2014e-07, 6.8949e-09],\n",
      "        [1.4136e-07, 4.4921e-16, 9.2920e-13, 3.3134e-08, 8.0909e-07, 1.6846e-12,\n",
      "         1.6654e-04, 4.3131e-04, 7.4518e-13, 1.7631e-15, 1.1823e-11, 1.0000e+00,\n",
      "         3.7002e-15, 1.2606e-05, 8.1833e-04, 8.7078e-10],\n",
      "        [1.3274e-06, 1.0644e-05, 1.0000e+00, 7.3392e-08, 1.9060e-03, 3.6867e-12,\n",
      "         2.1070e-07, 1.2615e-09, 7.9252e-15, 3.9019e-07, 3.6741e-05, 6.8453e-16,\n",
      "         1.6240e-04, 1.0754e-09, 1.5766e-09, 1.1227e-05],\n",
      "        [1.3207e-06, 1.0000e+00, 1.2435e-13, 2.3921e-09, 3.8022e-05, 1.0000e+00,\n",
      "         1.4547e-14, 6.9160e-12, 2.4219e-07, 6.0943e-05, 3.5141e-09, 2.9899e-15,\n",
      "         9.5527e-05, 3.6792e-06, 7.6051e-14, 5.4681e-13],\n",
      "        [4.7100e-08, 2.0672e-08, 1.9822e-12, 2.9548e-08, 2.6853e-10, 1.1457e-07,\n",
      "         6.7711e-09, 2.8549e-04, 6.8789e-06, 9.4278e-17, 7.4080e-16, 1.2359e-05,\n",
      "         2.0573e-04, 2.9364e-06, 1.0000e+00, 4.4988e-07],\n",
      "        [1.0000e+00, 5.1856e-09, 1.2023e-07, 4.4252e-09, 1.6212e-11, 4.6732e-09,\n",
      "         3.2731e-11, 1.0359e-12, 3.8168e-04, 2.8115e-08, 2.9962e-05, 1.7888e-05,\n",
      "         1.0794e-08, 9.0286e-08, 1.3620e-10, 2.3880e-11],\n",
      "        [5.4340e-09, 3.1284e-04, 4.7863e-06, 1.7368e-15, 2.3723e-10, 1.1043e-07,\n",
      "         1.8682e-17, 1.6454e-14, 3.0974e-04, 6.7448e-08, 1.8518e-04, 2.2766e-12,\n",
      "         1.0000e+00, 4.8743e-11, 7.2871e-05, 1.0755e-05],\n",
      "        [6.5314e-06, 4.6015e-06, 4.0344e-16, 8.8741e-10, 8.7784e-13, 2.6434e-04,\n",
      "         1.6652e-16, 3.6986e-07, 1.0000e+00, 2.8206e-08, 1.3545e-10, 7.1641e-06,\n",
      "         9.5505e-07, 9.8990e-12, 2.8678e-08, 1.1867e-06],\n",
      "        [4.3191e-06, 1.5159e-15, 2.4161e-05, 1.0000e+00, 2.9313e-08, 5.4345e-12,\n",
      "         1.0000e+00, 1.4697e-05, 1.0427e-10, 7.5383e-06, 5.0805e-07, 6.6460e-05,\n",
      "         2.2431e-18, 3.7954e-11, 5.2063e-11, 1.0216e-11],\n",
      "        [1.3862e-07, 1.7760e-13, 1.4488e-05, 1.0645e-08, 2.9160e-15, 5.8040e-16,\n",
      "         1.2002e-09, 1.1776e-09, 5.8985e-11, 8.6110e-02, 9.9999e-01, 6.9196e-10,\n",
      "         4.9584e-07, 1.7603e-06, 4.6572e-16, 2.2937e-08],\n",
      "        [4.2483e-05, 4.8008e-09, 2.3415e-03, 2.5491e-06, 7.3941e-12, 5.4957e-09,\n",
      "         4.2603e-05, 1.4365e-10, 5.2545e-13, 1.0000e+00, 1.0000e+00, 2.8854e-12,\n",
      "         2.0894e-08, 3.3101e-07, 1.9098e-18, 5.3132e-06],\n",
      "        [1.3257e-05, 1.2795e-10, 4.4208e-15, 2.1195e-08, 1.7229e-15, 5.5372e-06,\n",
      "         2.7566e-08, 2.0839e-06, 2.7651e-09, 2.1365e-11, 1.7965e-07, 3.5931e-10,\n",
      "         4.9488e-10, 1.0000e+00, 3.0058e-08, 3.0444e-03],\n",
      "        [1.4275e-12, 8.0698e-05, 9.6595e-09, 4.7106e-10, 1.0000e+00, 1.1681e-05,\n",
      "         1.2933e-06, 6.7743e-15, 1.1023e-12, 2.0602e-13, 6.8714e-16, 4.1690e-12,\n",
      "         8.3990e-11, 1.5821e-11, 8.7192e-11, 4.8263e-14],\n",
      "        [6.7182e-09, 1.7180e-13, 2.2398e-05, 5.6497e-12, 2.3494e-12, 1.6383e-11,\n",
      "         1.1248e-10, 4.7041e-10, 2.4104e-05, 1.0547e-12, 1.5193e-07, 8.9334e-10,\n",
      "         2.4043e-06, 2.9046e-05, 3.5834e-04, 1.0000e+00],\n",
      "        [1.1179e-09, 3.2369e-12, 2.1316e-13, 8.2935e-06, 3.3849e-15, 6.7519e-11,\n",
      "         1.3513e-05, 1.0000e+00, 7.4022e-10, 7.1979e-08, 6.9294e-08, 1.3022e-03,\n",
      "         5.7154e-12, 2.0097e-04, 2.1452e-08, 1.7163e-09],\n",
      "        [2.9681e-08, 6.7151e-08, 1.9600e-08, 1.0000e+00, 6.8864e-08, 1.3257e-06,\n",
      "         5.5455e-02, 1.8797e-09, 3.0606e-07, 5.8392e-07, 1.0960e-10, 2.1663e-09,\n",
      "         7.9542e-17, 2.1899e-13, 6.9904e-11, 3.4975e-11]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.9116e-09, 1.9892e-09, 9.8746e-09, 2.2921e-12, 8.1690e-08,\n",
      "         3.3591e-08, 2.6895e-12, 5.3150e-10, 1.4756e-06, 6.4185e-12, 2.8094e-05,\n",
      "         6.4301e-09, 4.7107e-06, 2.3957e-16, 3.3431e-09],\n",
      "        [2.3800e-11, 5.1094e-11, 1.0000e+00, 4.9116e-15, 1.2626e-10, 6.6539e-10,\n",
      "         1.8975e-14, 2.9649e-02, 1.3571e-06, 4.2771e-09, 3.7996e-10, 3.2121e-15,\n",
      "         2.4494e-11, 3.5585e-06, 1.0493e-13, 1.8512e-05],\n",
      "        [6.5660e-06, 1.9843e-13, 5.6519e-11, 8.5350e-09, 5.1781e-08, 1.0000e+00,\n",
      "         4.6272e-06, 5.2270e-13, 2.1499e-07, 1.0331e-07, 1.6700e-05, 1.1520e-18,\n",
      "         3.0662e-07, 7.4206e-09, 6.6814e-15, 6.1638e-06],\n",
      "        [1.1573e-08, 6.9746e-10, 2.8929e-05, 1.2699e-05, 1.5137e-07, 1.4282e-05,\n",
      "         2.8190e-06, 6.2907e-05, 2.0823e-13, 6.4518e-10, 1.0672e-06, 1.9858e-08,\n",
      "         7.8715e-08, 1.0000e+00, 2.6499e-10, 7.3711e-12],\n",
      "        [7.9084e-13, 2.4806e-12, 5.1846e-06, 7.1944e-05, 2.2187e-14, 7.9694e-06,\n",
      "         2.0136e-11, 3.3352e-11, 1.0000e+00, 4.2094e-13, 6.2667e-12, 3.0974e-11,\n",
      "         1.5862e-14, 2.5800e-14, 3.5909e-13, 6.1729e-13],\n",
      "        [3.7591e-12, 1.2743e-07, 1.0000e+00, 2.1862e-09, 6.2492e-13, 8.9976e-17,\n",
      "         3.6666e-11, 1.0000e+00, 9.2332e-09, 3.6659e-09, 2.8547e-12, 8.1993e-07,\n",
      "         4.2608e-14, 2.5020e-05, 6.3382e-05, 4.3832e-06],\n",
      "        [3.3366e-12, 3.1604e-13, 3.2021e-15, 1.0000e+00, 2.8303e-11, 1.2434e-07,\n",
      "         8.8775e-09, 2.9184e-11, 9.1735e-05, 2.7386e-10, 3.7134e-07, 3.1927e-05,\n",
      "         4.6747e-14, 1.1118e-04, 9.7196e-08, 7.0978e-16],\n",
      "        [1.0277e-11, 1.4609e-11, 1.6858e-16, 1.0247e-10, 1.0000e+00, 2.5733e-10,\n",
      "         7.6713e-07, 1.4366e-15, 9.8355e-15, 5.8991e-10, 3.9589e-04, 2.3958e-07,\n",
      "         4.5761e-04, 3.2556e-08, 6.1649e-05, 2.5001e-09],\n",
      "        [1.4423e-05, 1.0000e+00, 1.1276e-06, 9.7192e-19, 2.6815e-14, 7.4512e-16,\n",
      "         2.7923e-03, 1.3630e-06, 3.9452e-13, 6.8666e-11, 1.6804e-14, 7.4002e-11,\n",
      "         8.5184e-05, 8.6681e-11, 1.2882e-09, 2.0597e-03],\n",
      "        [1.9182e-11, 3.5556e-14, 2.2406e-09, 1.1585e-08, 1.4034e-06, 3.6060e-06,\n",
      "         7.3111e-17, 5.9252e-06, 5.1587e-17, 1.0000e+00, 1.0000e+00, 5.1078e-12,\n",
      "         5.5341e-18, 3.2585e-08, 7.0980e-06, 1.3164e-04],\n",
      "        [1.0693e-09, 4.7682e-15, 5.3466e-14, 3.7714e-07, 4.8554e-10, 8.4796e-07,\n",
      "         3.0656e-13, 1.7870e-09, 1.4831e-14, 1.0000e+00, 9.5963e-01, 3.7940e-12,\n",
      "         1.0405e-17, 1.7926e-12, 1.3825e-04, 3.0876e-06],\n",
      "        [9.0883e-05, 8.5647e-08, 2.1236e-16, 4.0982e-05, 3.6169e-05, 2.2870e-11,\n",
      "         8.0396e-12, 1.5582e-14, 4.4134e-07, 8.7662e-11, 9.5586e-10, 1.0000e+00,\n",
      "         3.1993e-07, 1.6516e-10, 5.4675e-09, 8.9817e-14],\n",
      "        [1.5328e-13, 6.7770e-06, 1.4026e-04, 5.9388e-13, 1.5484e-08, 2.3803e-10,\n",
      "         3.8834e-16, 9.2468e-09, 2.7627e-07, 1.0513e-06, 1.6337e-08, 7.6910e-18,\n",
      "         2.5409e-04, 5.9943e-16, 1.8384e-12, 1.0000e+00],\n",
      "        [7.0002e-15, 8.4393e-12, 3.5280e-07, 7.2299e-07, 1.9243e-04, 2.0027e-14,\n",
      "         3.7554e-05, 1.2234e-04, 1.3526e-10, 3.8151e-06, 1.7454e-08, 5.1997e-05,\n",
      "         5.6178e-09, 6.4528e-10, 1.0000e+00, 3.3902e-10],\n",
      "        [2.4369e-08, 8.8805e-05, 3.5644e-09, 8.6687e-09, 4.1437e-04, 2.9300e-12,\n",
      "         3.9824e-06, 8.8209e-13, 1.4026e-04, 1.6514e-18, 2.0104e-15, 8.7170e-05,\n",
      "         1.0000e+00, 3.5940e-06, 3.7804e-11, 1.1076e-06],\n",
      "        [4.7127e-11, 4.8669e-02, 3.2009e-08, 4.7145e-15, 9.1769e-12, 1.3250e-13,\n",
      "         1.0000e+00, 1.4714e-07, 7.0506e-10, 5.2219e-14, 3.6298e-16, 1.9053e-11,\n",
      "         6.6103e-06, 2.0109e-11, 2.4734e-03, 1.8257e-10]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "M0 tensor([[5.3599e-12],\n",
      "        [6.4932e-11],\n",
      "        [3.2767e-06],\n",
      "        [1.2677e-07],\n",
      "        [1.0807e-11],\n",
      "        [1.3027e-08],\n",
      "        [1.1755e-09],\n",
      "        [2.3905e-05],\n",
      "        [3.1125e-07],\n",
      "        [3.0882e-11],\n",
      "        [4.3953e-12],\n",
      "        [1.6141e-08],\n",
      "        [2.8326e-12],\n",
      "        [4.4588e-10],\n",
      "        [7.0934e-06],\n",
      "        [2.0879e-12]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0.]])\n",
      "Ad tensor([[1.0000e+00, 3.2020e-12, 1.6379e-12, 4.8769e-11, 4.9058e-11],\n",
      "        [1.2388e-10, 1.0000e+00, 6.3422e-13, 1.0000e+00, 5.8920e-15],\n",
      "        [4.8584e-12, 3.9758e-07, 7.4334e-14, 1.0835e-10, 1.0000e+00],\n",
      "        [2.5032e-15, 4.0213e-11, 1.0000e+00, 4.7466e-10, 1.1454e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 5.2496e-13, 4.3161e-11, 1.3953e-12],\n",
      "        [1.1566e-10, 1.2822e-12, 3.2388e-14, 1.0000e+00],\n",
      "        [6.1699e-12, 9.7385e-12, 1.0000e+00, 3.4181e-14],\n",
      "        [1.0847e-10, 1.0000e+00, 7.3919e-12, 2.8089e-14]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.7940e-13, 1.0000e+00, 4.5528e-13, 1.4021e-11],\n",
      "        [1.2389e-12, 6.7078e-10, 4.8816e-10, 2.7167e-13, 1.0000e+00],\n",
      "        [2.9213e-10, 1.0000e+00, 3.5516e-09, 8.3490e-16, 8.5991e-08],\n",
      "        [1.8215e-08, 6.6176e-14, 1.4573e-10, 1.0000e+00, 1.3072e-16]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.1322e-14, 5.0997e-13, 9.9088e-13],\n",
      "        [1.1426e-09, 1.2278e-09, 1.0640e-14, 1.0000e+00],\n",
      "        [1.0000e+00, 1.3771e-11, 3.3490e-12, 1.7682e-11],\n",
      "        [4.0567e-08, 3.0464e-17, 1.0000e+00, 7.6442e-17],\n",
      "        [3.0892e-10, 1.0000e+00, 3.0584e-14, 1.5370e-06]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.0000e+00, 3.6539e-13, 5.9486e-13, 3.8756e-14],\n",
      "        [1.4894e-10, 1.7805e-14, 9.3905e-10, 1.0000e+00, 8.0973e-13],\n",
      "        [4.4063e-14, 1.9816e-10, 6.5226e-13, 2.2334e-13, 1.0000e+00],\n",
      "        [1.8306e-10, 8.0537e-11, 1.0000e+00, 1.5553e-09, 9.9089e-15]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.0756e-16, 4.2303e-13, 2.2402e-10],\n",
      "        [1.0000e+00, 2.8966e-09, 6.1833e-10, 2.4008e-19],\n",
      "        [6.1668e-11, 1.8691e-11, 1.0000e+00, 2.9022e-08],\n",
      "        [2.9988e-09, 3.8650e-13, 5.8167e-08, 1.0000e+00],\n",
      "        [3.8756e-10, 1.0000e+00, 2.0123e-12, 2.0116e-14]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.8108e-11, 1.0107e-11, 2.3079e-13],\n",
      "        [6.4372e-14, 5.6044e-11, 5.2524e-16, 1.0000e+00],\n",
      "        [2.0392e-11, 1.0000e+00, 1.1614e-09, 1.8021e-11],\n",
      "        [2.5167e-11, 5.6782e-10, 1.0000e+00, 4.6909e-14]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 3.2020e-12, 1.6379e-12, 4.8769e-11, 4.9058e-11],\n",
      "        [1.2388e-10, 1.0000e+00, 6.3422e-13, 1.0000e+00, 5.8920e-15],\n",
      "        [4.8584e-12, 3.9758e-07, 7.4334e-14, 1.0835e-10, 1.0000e+00],\n",
      "        [2.5032e-15, 4.0213e-11, 1.0000e+00, 4.7466e-10, 1.1454e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.1971e-10, 6.9988e-09, 7.8959e-13, 1.2362e-11, 1.7464e-12],\n",
      "        [1.1330e-10, 1.0000e+00, 4.7873e-13, 1.2350e-08, 5.7384e-15, 1.7431e-09],\n",
      "        [2.3936e-09, 1.9664e-11, 1.5301e-12, 1.0606e-07, 1.0000e+00, 5.1699e-10],\n",
      "        [8.0854e-11, 1.2016e-10, 1.0000e+00, 3.8004e-09, 3.1654e-15, 2.8298e-09],\n",
      "        [2.5950e-14, 1.6966e-07, 1.4579e-11, 3.8785e-13, 7.0546e-10, 1.0000e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.5485e-16, 7.7959e-12, 4.3333e-08, 9.2711e-10],\n",
      "        [1.2936e-08, 5.7172e-09, 3.3918e-20, 2.0080e-17, 1.0000e+00],\n",
      "        [5.1247e-08, 4.3051e-07, 1.0000e+00, 4.0008e-09, 2.2822e-16],\n",
      "        [1.5717e-09, 9.8977e-12, 1.1033e-08, 1.1158e-10, 1.1280e-07],\n",
      "        [1.2423e-08, 3.3285e-11, 6.6451e-09, 1.0000e+00, 6.8218e-17],\n",
      "        [6.0368e-14, 1.0000e+00, 1.1415e-09, 2.4746e-10, 3.3300e-07]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 4.0763e-13, 1.2631e-13, 1.0000e+00, 1.7153e-11, 4.0084e-11,\n",
      "         4.8526e-14],\n",
      "        [2.2834e-10, 8.8045e-09, 3.2396e-12, 1.2682e-10, 1.0000e+00, 1.0000e+00,\n",
      "         8.4853e-11],\n",
      "        [8.3279e-10, 1.0000e+00, 1.3037e-11, 6.3365e-08, 1.2925e-11, 1.0713e-11,\n",
      "         2.0984e-10],\n",
      "        [1.0191e-08, 1.4862e-10, 1.0000e+00, 4.8341e-13, 9.9065e-15, 1.0269e-15,\n",
      "         1.0000e+00],\n",
      "        [5.0585e-10, 1.1479e-13, 8.9299e-16, 5.2621e-08, 1.0000e+00, 1.0000e+00,\n",
      "         3.8811e-14]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.6414e-10, 1.1715e-15, 2.3720e-12, 1.0000e+00, 7.9430e-10,\n",
      "         4.4011e-12],\n",
      "        [4.5041e-06, 3.4817e-09, 1.0000e+00, 1.9300e-15, 3.0700e-14, 1.1704e-14,\n",
      "         8.5713e-07],\n",
      "        [6.5354e-11, 1.2679e-17, 7.5148e-14, 1.0000e+00, 2.1148e-08, 2.5485e-06,\n",
      "         1.7225e-06],\n",
      "        [1.0000e+00, 6.4451e-11, 4.9379e-07, 2.3014e-16, 3.0348e-04, 1.8297e-11,\n",
      "         6.8949e-18],\n",
      "        [1.0930e-08, 1.0000e+00, 5.1657e-10, 3.5541e-15, 3.1229e-06, 3.1258e-05,\n",
      "         7.2333e-10],\n",
      "        [2.7525e-11, 1.0000e+00, 1.1578e-07, 5.0788e-10, 8.4551e-08, 1.0000e+00,\n",
      "         2.4748e-13],\n",
      "        [7.7862e-12, 2.5413e-16, 8.5407e-08, 1.0000e+00, 5.2046e-09, 2.1506e-11,\n",
      "         1.0000e+00]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.7887e-09, 9.1401e-13, 1.7029e-09, 1.6887e-12, 2.1281e-08],\n",
      "        [7.9989e-11, 7.7797e-11, 2.9816e-07, 3.9853e-09, 2.1065e-15, 1.0000e+00],\n",
      "        [1.1987e-08, 1.0997e-07, 2.9742e-14, 1.0000e+00, 3.4293e-10, 2.9395e-06],\n",
      "        [4.7917e-10, 2.1251e-08, 6.1099e-06, 1.5975e-14, 1.0000e+00, 1.3902e-16],\n",
      "        [1.0000e+00, 4.2183e-06, 2.3865e-09, 1.8819e-14, 1.8158e-10, 5.8082e-09],\n",
      "        [6.3061e-12, 2.7976e-15, 1.0000e+00, 3.5642e-15, 8.1914e-06, 1.7848e-06],\n",
      "        [4.1366e-09, 1.0000e+00, 7.5426e-16, 2.1381e-06, 9.0482e-03, 3.6878e-15]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 8.5386e-10, 5.0334e-12, 7.5443e-15, 1.1476e-09, 6.5706e-10,\n",
      "         9.9032e-13],\n",
      "        [9.6140e-11, 1.0000e+00, 7.4998e-14, 1.2774e-10, 1.3419e-14, 1.2858e-09,\n",
      "         1.0448e-06],\n",
      "        [2.2173e-08, 1.3204e-09, 2.0850e-13, 1.0061e-07, 1.6333e-08, 7.0971e-08,\n",
      "         1.0000e+00],\n",
      "        [2.4170e-10, 7.7451e-12, 1.0000e+00, 1.2365e-05, 1.3736e-10, 5.2354e-09,\n",
      "         3.2707e-12],\n",
      "        [2.1536e-07, 1.1181e-08, 8.2626e-10, 4.5592e-16, 1.9848e-17, 1.0000e+00,\n",
      "         1.4280e-08],\n",
      "        [1.6844e-09, 4.2609e-10, 3.1546e-08, 7.8508e-09, 1.0000e+00, 2.1548e-14,\n",
      "         1.1640e-08]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 7.5067e-14, 5.7366e-10, 7.6745e-10, 9.6125e-11, 1.9606e-07],\n",
      "        [3.5229e-13, 1.9162e-08, 2.7784e-16, 2.4544e-10, 1.0000e+00, 1.7771e-08],\n",
      "        [4.0899e-07, 8.0344e-11, 1.0000e+00, 7.0119e-10, 2.6820e-12, 1.7454e-09],\n",
      "        [7.0894e-14, 1.0223e-06, 5.3419e-09, 1.6055e-08, 6.3733e-07, 1.4201e-16],\n",
      "        [3.7905e-09, 5.4549e-08, 1.5391e-08, 1.0000e+00, 1.3956e-10, 2.4153e-17],\n",
      "        [9.5651e-07, 2.8957e-10, 1.4859e-11, 6.0250e-17, 3.0058e-05, 1.0000e+00],\n",
      "        [4.9637e-13, 1.0000e+00, 1.1162e-09, 1.6461e-10, 5.9198e-06, 1.9309e-08]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.2575e-16, 4.4947e-08, 1.2176e-09, 1.1655e-12, 1.5307e-09,\n",
      "         1.1786e-11],\n",
      "        [4.6268e-15, 1.0000e+00, 2.6262e-06, 6.5440e-08, 1.0000e+00, 2.9503e-12,\n",
      "         3.5234e-09],\n",
      "        [3.6083e-07, 4.0572e-10, 1.5333e-11, 1.0000e+00, 2.2958e-10, 1.9728e-14,\n",
      "         2.2825e-08],\n",
      "        [1.4463e-12, 1.0797e-07, 7.8534e-16, 2.5495e-10, 4.1725e-10, 1.6497e-07,\n",
      "         1.0000e+00],\n",
      "        [1.1565e-08, 1.2987e-13, 3.1552e-06, 2.5163e-16, 5.8027e-09, 1.0000e+00,\n",
      "         5.5014e-09],\n",
      "        [1.4324e-08, 2.7249e-07, 1.0000e+00, 7.2929e-13, 2.5933e-06, 7.5874e-08,\n",
      "         4.8089e-18]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.0000e+00, 7.2814e-11, 1.7240e-11, 2.2831e-09, 1.0929e-15,\n",
      "         4.1377e-07, 9.9976e-13],\n",
      "        [3.1267e-14, 5.6098e-12, 2.3834e-07, 2.6852e-06, 7.6522e-07, 1.0000e+00,\n",
      "         7.1636e-16, 3.5284e-07],\n",
      "        [1.5754e-09, 8.6390e-13, 4.6193e-13, 1.1940e-11, 8.3021e-11, 1.7728e-07,\n",
      "         1.6764e-05, 1.0000e+00],\n",
      "        [1.3400e-06, 5.0947e-08, 4.0700e-11, 1.8319e-07, 1.0000e+00, 4.2188e-06,\n",
      "         4.2418e-13, 4.4400e-08],\n",
      "        [2.0281e-14, 2.4764e-10, 8.9011e-08, 1.0000e+00, 1.2431e-07, 2.2433e-08,\n",
      "         2.4894e-08, 2.6579e-09],\n",
      "        [1.1762e-08, 9.3536e-08, 3.3980e-05, 5.7081e-07, 1.4563e-15, 2.3108e-18,\n",
      "         1.0000e+00, 5.7810e-09],\n",
      "        [4.0662e-11, 3.3015e-10, 1.0000e+00, 6.4880e-08, 1.0795e-16, 5.5190e-06,\n",
      "         1.0527e-07, 2.2956e-17]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.5700e-12, 5.5230e-12, 7.3722e-12, 4.9414e-08, 2.7837e-09,\n",
      "         1.3014e-15, 5.9064e-13],\n",
      "        [1.0000e+00, 6.9064e-12, 7.2259e-08, 3.0823e-11, 2.0817e-10, 2.7364e-07,\n",
      "         1.3644e-09, 6.6933e-11],\n",
      "        [5.2463e-10, 4.9457e-14, 6.9450e-07, 1.1492e-10, 1.0000e+00, 2.4513e-18,\n",
      "         2.2227e-09, 9.9108e-09],\n",
      "        [4.8718e-08, 9.1459e-08, 3.2632e-07, 2.7580e-10, 4.8971e-11, 1.0789e-07,\n",
      "         1.0000e+00, 2.7349e-07],\n",
      "        [9.4715e-08, 4.0768e-10, 3.0215e-09, 3.1563e-13, 4.6734e-14, 1.0000e+00,\n",
      "         2.1322e-12, 2.1929e-06],\n",
      "        [1.7215e-09, 3.8090e-10, 5.8034e-12, 8.1355e-10, 2.6996e-08, 8.2031e-11,\n",
      "         6.5917e-09, 1.0000e+00],\n",
      "        [9.6070e-07, 1.4878e-07, 1.0000e+00, 3.3032e-08, 2.5741e-05, 4.1615e-09,\n",
      "         1.9493e-06, 3.9583e-17],\n",
      "        [2.5470e-14, 1.0000e+00, 8.4396e-08, 1.0000e+00, 3.6037e-11, 7.7290e-11,\n",
      "         5.9556e-09, 5.5109e-08]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.0000e+00, 1.3966e-08, 3.3318e-12, 9.2873e-07, 2.2512e-15,\n",
      "         3.1069e-08, 1.2030e-08, 6.5565e-13, 1.2529e-09, 4.4947e-10],\n",
      "        [3.7511e-16, 1.3896e-14, 4.0092e-07, 1.0000e+00, 1.7578e-15, 1.1094e-07,\n",
      "         2.1055e-18, 1.7397e-11, 1.0000e+00, 2.6835e-10, 8.8540e-12],\n",
      "        [3.9288e-14, 7.2784e-12, 2.9994e-09, 3.2613e-10, 8.9242e-06, 1.0000e+00,\n",
      "         1.5630e-07, 1.6966e-11, 1.8036e-08, 1.9538e-09, 8.7718e-09],\n",
      "        [2.3761e-13, 4.8177e-13, 1.4850e-08, 1.0000e+00, 9.9307e-11, 3.2928e-06,\n",
      "         1.6921e-13, 4.1416e-13, 1.0000e+00, 1.8315e-12, 2.8319e-12],\n",
      "        [1.1806e-07, 3.5633e-08, 9.0792e-13, 2.9052e-08, 1.0000e+00, 8.9522e-08,\n",
      "         1.0000e+00, 4.1361e-08, 1.3174e-11, 2.9461e-13, 1.3957e-09],\n",
      "        [7.8027e-08, 1.3668e-07, 1.7606e-08, 1.0155e-07, 4.4380e-10, 1.2498e-10,\n",
      "         5.1165e-09, 1.6801e-05, 1.0607e-08, 1.6757e-05, 1.0000e+00],\n",
      "        [8.3880e-09, 5.5540e-09, 1.0000e+00, 2.2435e-09, 6.2449e-14, 1.6710e-08,\n",
      "         4.8743e-08, 1.0715e-09, 4.1782e-07, 1.7539e-07, 5.8172e-12],\n",
      "        [8.7460e-09, 1.4710e-07, 2.6686e-06, 3.7533e-12, 6.2148e-17, 1.7558e-12,\n",
      "         5.1017e-12, 1.0000e+00, 5.5543e-10, 1.0000e+00, 3.0079e-07]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 8.8565e-10, 5.9252e-10, 1.3888e-10, 1.2235e-10, 1.0000e+00,\n",
      "         9.3214e-09, 2.9913e-11, 1.0606e-11, 4.4054e-09, 8.2170e-10],\n",
      "        [1.0000e+00, 5.4145e-09, 1.1357e-05, 2.8730e-14, 1.9831e-10, 1.0000e+00,\n",
      "         3.2069e-07, 6.3236e-06, 3.1446e-12, 1.7829e-08, 5.8628e-15],\n",
      "        [1.3415e-07, 1.2038e-06, 3.3546e-09, 7.6816e-11, 2.0598e-10, 7.6451e-08,\n",
      "         1.1082e-05, 1.4387e-10, 6.9268e-08, 1.0000e+00, 1.2221e-09],\n",
      "        [5.6883e-10, 4.5248e-09, 1.1971e-11, 9.2205e-10, 4.2134e-12, 5.5705e-06,\n",
      "         1.0000e+00, 3.8398e-13, 2.4648e-05, 4.4406e-08, 9.7243e-08],\n",
      "        [2.7926e-07, 1.2654e-07, 7.8914e-20, 1.0000e+00, 5.7178e-10, 1.8084e-08,\n",
      "         1.6899e-10, 1.3501e-13, 1.4841e-07, 3.1931e-13, 1.0000e+00],\n",
      "        [1.9691e-09, 1.0000e+00, 2.7631e-05, 2.4487e-06, 2.3305e-11, 4.6016e-10,\n",
      "         6.0532e-10, 2.3548e-07, 2.9060e-06, 1.3336e-10, 2.4462e-06],\n",
      "        [1.7668e-08, 3.4073e-06, 1.6379e-17, 1.0000e+00, 5.7665e-07, 7.1211e-09,\n",
      "         3.3322e-15, 2.5050e-09, 9.5561e-13, 1.7237e-11, 5.1710e-05],\n",
      "        [5.4895e-11, 1.7727e-14, 7.7620e-04, 1.7954e-08, 4.3649e-07, 3.3856e-10,\n",
      "         8.7412e-10, 1.0000e+00, 1.0498e-16, 4.9016e-10, 4.1082e-07],\n",
      "        [6.7289e-17, 1.0555e-06, 7.4631e-07, 2.3152e-10, 1.9247e-13, 8.7714e-14,\n",
      "         2.6311e-05, 7.5480e-10, 1.0000e+00, 3.3498e-08, 5.7146e-07],\n",
      "        [1.3654e-09, 9.2968e-06, 1.0000e+00, 1.0452e-15, 3.5749e-06, 3.0715e-08,\n",
      "         9.4569e-09, 1.0000e+00, 3.2156e-10, 1.6488e-08, 2.2515e-16],\n",
      "        [7.4781e-09, 5.7135e-17, 4.9970e-10, 6.6973e-07, 1.0000e+00, 2.8902e-08,\n",
      "         1.2995e-05, 2.1217e-06, 7.2126e-13, 5.4275e-05, 7.3727e-12]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 4.6035e-10, 9.7497e-13, 1.0616e-13, 1.7598e-08, 1.9039e-12,\n",
      "         1.1435e-15, 1.2907e-09, 1.8490e-07, 1.0873e-10],\n",
      "        [7.8171e-08, 1.2256e-12, 5.0283e-10, 1.2865e-07, 1.0000e+00, 1.5868e-15,\n",
      "         1.1690e-05, 4.1276e-09, 2.0078e-07, 7.1446e-10],\n",
      "        [4.4108e-08, 6.3025e-11, 5.3434e-20, 1.0000e+00, 2.3632e-05, 4.2237e-13,\n",
      "         6.9507e-10, 2.4447e-09, 8.5997e-08, 1.0000e+00],\n",
      "        [7.1967e-14, 1.7200e-07, 1.0000e+00, 5.0869e-14, 2.6144e-09, 7.2253e-09,\n",
      "         1.0364e-05, 1.3085e-11, 4.3893e-06, 1.6389e-18],\n",
      "        [2.5739e-09, 1.0000e+00, 1.5459e-05, 4.0860e-07, 4.3852e-15, 8.7972e-09,\n",
      "         1.6579e-09, 4.7631e-05, 1.4138e-14, 9.7972e-08],\n",
      "        [1.0000e+00, 4.1179e-10, 4.2883e-11, 1.3920e-10, 4.6627e-10, 1.2771e-08,\n",
      "         7.0391e-13, 7.9451e-09, 1.7092e-08, 3.6094e-10],\n",
      "        [1.0678e-06, 2.8746e-12, 4.4134e-10, 3.0916e-12, 3.3593e-14, 1.0000e+00,\n",
      "         6.7291e-07, 5.3683e-07, 1.6068e-09, 5.3706e-14],\n",
      "        [2.1734e-09, 2.7135e-06, 1.7167e-11, 1.0000e+00, 1.7748e-08, 5.9052e-14,\n",
      "         1.1511e-12, 2.4304e-14, 1.8437e-09, 1.5654e-05],\n",
      "        [1.5716e-14, 2.2674e-12, 7.9854e-08, 2.0530e-12, 2.9227e-07, 1.3916e-07,\n",
      "         1.0000e+00, 1.3200e-07, 2.6281e-11, 9.8996e-07],\n",
      "        [1.7931e-08, 3.3642e-09, 5.3254e-13, 2.6269e-12, 6.8199e-10, 7.6174e-07,\n",
      "         6.5094e-10, 1.0000e+00, 2.3078e-06, 4.2817e-07],\n",
      "        [3.9649e-11, 8.5185e-13, 1.5523e-05, 2.3886e-08, 9.8103e-08, 1.6298e-10,\n",
      "         9.3550e-11, 5.0245e-09, 1.0000e+00, 7.6424e-09]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[1.0000e+00, 3.1592e-16, 1.9636e-09, 3.4317e-13, 4.1179e-15, 4.6582e-07,\n",
      "         2.9175e-11, 2.4692e-08, 4.1774e-06, 8.0590e-07],\n",
      "        [6.2459e-11, 3.9834e-09, 1.0037e-11, 7.8639e-06, 3.3353e-05, 8.0210e-13,\n",
      "         1.7775e-08, 4.6322e-14, 1.8463e-14, 1.0000e+00],\n",
      "        [8.1751e-11, 2.8165e-05, 6.0061e-19, 1.0000e+00, 1.6190e-11, 4.1116e-15,\n",
      "         5.4136e-08, 4.9514e-08, 1.7905e-07, 4.7167e-07],\n",
      "        [1.9391e-09, 6.3253e-07, 1.5844e-06, 4.2218e-11, 1.0000e+00, 2.0911e-12,\n",
      "         1.7971e-07, 1.1632e-06, 2.9971e-06, 8.1487e-07],\n",
      "        [3.2061e-06, 2.2442e-06, 1.2016e-07, 4.1415e-10, 4.7307e-07, 6.8830e-14,\n",
      "         1.0000e+00, 1.5375e-07, 5.8858e-18, 2.6894e-07],\n",
      "        [2.0822e-08, 6.5055e-06, 2.0525e-13, 5.6345e-06, 3.2037e-15, 5.8709e-04,\n",
      "         2.9712e-17, 1.4295e-08, 1.0000e+00, 2.5107e-14],\n",
      "        [3.2446e-13, 1.0000e+00, 2.6242e-12, 7.1597e-06, 3.7594e-12, 2.4687e-08,\n",
      "         6.1408e-09, 3.4795e-10, 1.9889e-05, 5.9245e-11],\n",
      "        [6.7529e-06, 3.5255e-13, 5.6707e-05, 8.9722e-10, 2.8467e-14, 1.0000e+00,\n",
      "         3.3599e-10, 6.9723e-09, 1.2327e-06, 7.1657e-08],\n",
      "        [1.7814e-06, 4.3751e-11, 2.6830e-13, 1.9076e-07, 4.9416e-07, 5.3072e-10,\n",
      "         1.5577e-07, 1.0000e+00, 1.3129e-10, 5.8698e-10],\n",
      "        [9.4329e-09, 4.6812e-13, 1.0000e+00, 1.0404e-13, 3.1608e-06, 1.1866e-06,\n",
      "         3.3969e-07, 8.5597e-12, 1.1121e-10, 9.4869e-09]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.9020e-14, 3.9993e-16, 1.3522e-07, 8.6736e-09, 1.4930e-07,\n",
      "         2.6768e-11, 5.0135e-09, 6.5671e-08, 1.4470e-06],\n",
      "        [1.1895e-17, 1.0000e+00, 3.1268e-06, 2.4822e-17, 6.9663e-05, 4.0077e-07,\n",
      "         2.6850e-07, 1.1423e-12, 6.0473e-09, 1.2226e-07],\n",
      "        [1.5279e-06, 1.7142e-13, 1.1571e-06, 1.0000e+00, 3.0853e-13, 2.8109e-11,\n",
      "         1.7249e-11, 1.1522e-06, 1.4111e-04, 1.0568e-07],\n",
      "        [1.2111e-13, 2.0442e-10, 4.4735e-08, 3.6991e-14, 1.6048e-05, 2.2598e-06,\n",
      "         1.0000e+00, 7.8415e-08, 6.3495e-11, 3.1173e-08],\n",
      "        [1.2806e-12, 1.4193e-05, 1.0000e+00, 1.0416e-08, 1.1566e-12, 2.8189e-06,\n",
      "         1.0940e-07, 6.6613e-08, 8.6030e-14, 5.1441e-08],\n",
      "        [5.1912e-11, 1.3424e-10, 1.5312e-12, 5.5200e-06, 1.0644e-08, 1.8684e-05,\n",
      "         1.1434e-06, 1.7896e-13, 1.0000e+00, 6.7071e-10],\n",
      "        [1.3284e-08, 2.6711e-06, 3.4594e-08, 2.1101e-06, 4.6724e-13, 3.4185e-09,\n",
      "         4.8426e-07, 1.3416e-10, 2.0064e-12, 1.0000e+00],\n",
      "        [4.0107e-09, 4.2337e-06, 2.9562e-13, 1.4893e-16, 5.5932e-14, 1.0000e+00,\n",
      "         3.3425e-09, 1.2691e-11, 8.5254e-06, 3.2540e-08],\n",
      "        [1.0442e-10, 1.1506e-06, 4.3480e-06, 7.4145e-07, 1.0000e+00, 3.1955e-16,\n",
      "         8.1473e-08, 2.2052e-10, 6.4702e-07, 5.2385e-16],\n",
      "        [1.0779e-07, 1.1518e-11, 1.5215e-05, 4.9860e-15, 1.9947e-11, 8.9003e-08,\n",
      "         4.0973e-06, 1.0000e+00, 3.9327e-13, 1.1025e-09]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.7066e-07, 3.0847e-16, 3.8924e-12, 1.1155e-08, 4.1906e-12,\n",
      "         5.5479e-14, 2.5255e-08, 1.7475e-06, 1.0333e-08],\n",
      "        [2.2879e-13, 1.4904e-11, 1.0000e+00, 1.8503e-05, 1.6866e-08, 1.9701e-06,\n",
      "         5.7167e-11, 4.8762e-11, 3.6963e-15, 3.3691e-15],\n",
      "        [2.2740e-15, 6.5660e-08, 1.7539e-09, 1.0356e-09, 2.6188e-11, 4.9276e-05,\n",
      "         1.0000e+00, 8.6280e-13, 2.7954e-07, 2.8338e-10],\n",
      "        [1.6565e-07, 9.6357e-11, 4.3040e-14, 3.9281e-11, 5.9327e-15, 5.5985e-06,\n",
      "         1.6217e-06, 1.7860e-09, 1.0000e+00, 9.1093e-07],\n",
      "        [2.5928e-07, 6.6991e-13, 2.9826e-06, 1.0000e+00, 2.3507e-16, 5.8426e-14,\n",
      "         6.4668e-10, 2.1478e-05, 2.2733e-10, 2.3156e-09],\n",
      "        [5.6183e-10, 2.1405e-08, 8.5686e-08, 3.7907e-14, 1.0000e+00, 1.7332e-06,\n",
      "         2.9972e-08, 1.3162e-08, 8.5894e-17, 1.0976e-08],\n",
      "        [3.0070e-12, 2.8202e-13, 7.1230e-10, 1.0215e-06, 2.5374e-05, 1.1274e-07,\n",
      "         4.9720e-07, 7.1027e-15, 5.4461e-07, 1.0000e+00],\n",
      "        [7.0947e-07, 1.0000e+00, 3.6299e-09, 2.4495e-07, 3.5447e-06, 1.3605e-17,\n",
      "         8.8175e-11, 1.8923e-09, 8.6401e-11, 6.5404e-08],\n",
      "        [6.1655e-08, 1.9792e-14, 1.0882e-07, 2.9858e-06, 1.2716e-08, 8.7956e-07,\n",
      "         3.8716e-13, 1.0000e+00, 1.2706e-06, 2.6183e-12],\n",
      "        [6.4216e-11, 1.1714e-08, 2.3113e-06, 1.1150e-15, 1.1397e-06, 1.0000e+00,\n",
      "         9.5684e-07, 1.1941e-11, 7.3342e-09, 3.0715e-10]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 4.7504e-07, 3.0613e-09, 1.1543e-08, 2.9975e-14, 1.0130e-08,\n",
      "         5.0364e-11, 1.0000e+00, 6.9715e-13, 3.6491e-10, 1.3064e-07],\n",
      "        [5.5259e-08, 3.8508e-13, 2.0624e-13, 1.0000e+00, 2.6624e-16, 7.0170e-08,\n",
      "         6.9658e-15, 2.5538e-07, 2.3221e-06, 1.0059e-06, 1.4481e-11],\n",
      "        [3.8301e-13, 5.5822e-07, 1.1475e-06, 1.7940e-07, 2.1464e-11, 2.8508e-06,\n",
      "         1.1419e-14, 1.5161e-16, 6.8870e-14, 1.0000e+00, 2.4205e-07],\n",
      "        [4.7090e-11, 4.4188e-04, 2.1129e-14, 4.1716e-09, 1.4027e-09, 2.0860e-11,\n",
      "         1.8089e-10, 3.1614e-11, 4.0198e-12, 1.0647e-07, 1.0000e+00],\n",
      "        [7.4111e-11, 8.4101e-11, 3.9203e-05, 8.4151e-07, 6.6894e-06, 1.0000e+00,\n",
      "         2.7564e-13, 1.1039e-13, 1.1177e-10, 4.3287e-06, 4.2117e-10],\n",
      "        [1.0112e-13, 2.4018e-07, 1.0000e+00, 3.2105e-14, 1.2604e-09, 2.3248e-06,\n",
      "         2.2960e-07, 9.4844e-16, 1.4219e-05, 3.0955e-06, 2.1504e-14],\n",
      "        [1.1894e-11, 1.5668e-05, 1.0517e-05, 3.0782e-07, 6.3390e-09, 1.4724e-12,\n",
      "         1.1664e-09, 3.5564e-12, 1.0000e+00, 2.7365e-12, 3.3874e-10],\n",
      "        [8.1275e-10, 1.0000e+00, 6.4760e-12, 4.0563e-11, 2.4219e-14, 2.9973e-09,\n",
      "         1.4376e-08, 1.3299e-07, 3.5531e-07, 2.7467e-11, 3.3593e-06],\n",
      "        [2.8605e-08, 6.6671e-07, 1.4842e-06, 4.8450e-14, 2.5324e-07, 4.0349e-13,\n",
      "         1.0000e+00, 4.3958e-06, 4.4513e-08, 4.4685e-15, 4.1052e-16],\n",
      "        [8.7207e-10, 4.7430e-16, 3.3279e-11, 9.2380e-09, 1.0000e+00, 5.1973e-05,\n",
      "         2.3702e-06, 5.4495e-09, 1.9272e-07, 5.5997e-13, 1.1989e-05]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 1.6397e-12, 2.2278e-16, 1.0472e-11, 4.1238e-08, 1.4498e-09,\n",
      "         4.7697e-08, 9.9449e-17, 1.4640e-10, 1.9531e-08, 1.2513e-07],\n",
      "        [5.6096e-10, 1.0000e+00, 4.5098e-11, 2.9386e-10, 9.6080e-09, 4.0819e-06,\n",
      "         5.5152e-17, 4.6629e-06, 2.7960e-07, 3.1412e-13, 1.0000e+00],\n",
      "        [1.0093e-09, 2.1085e-07, 1.3816e-06, 7.8474e-15, 1.0118e-10, 3.9721e-05,\n",
      "         1.4072e-07, 3.0602e-10, 1.0000e+00, 1.5253e-06, 2.4690e-15],\n",
      "        [2.2813e-07, 3.1219e-11, 2.9655e-06, 1.8762e-08, 4.0829e-08, 5.5902e-04,\n",
      "         7.0601e-12, 1.4087e-09, 4.0426e-11, 1.0000e+00, 5.2219e-12],\n",
      "        [1.7818e-07, 1.2884e-12, 1.0508e-09, 4.5728e-05, 1.0647e-06, 4.9447e-07,\n",
      "         1.0000e+00, 7.1121e-08, 1.8039e-05, 4.0425e-10, 6.2578e-12],\n",
      "        [1.8519e-06, 1.2337e-04, 2.6949e-05, 1.1790e-06, 8.5354e-15, 1.0000e+00,\n",
      "         4.9549e-10, 1.5073e-07, 6.7989e-09, 6.4894e-06, 5.8189e-11],\n",
      "        [3.2255e-10, 6.5473e-09, 3.8964e-09, 1.3529e-12, 1.2658e-06, 6.0077e-08,\n",
      "         3.2682e-06, 1.0000e+00, 4.1287e-07, 1.0513e-14, 1.5963e-05],\n",
      "        [1.0000e+00, 1.7355e-10, 2.6315e-16, 1.1873e-12, 3.7307e-06, 3.1337e-09,\n",
      "         4.8894e-10, 1.2091e-11, 1.0583e-11, 1.5146e-07, 4.8953e-05],\n",
      "        [9.9070e-10, 4.4523e-12, 8.4884e-09, 5.7852e-13, 1.0000e+00, 4.9393e-15,\n",
      "         5.7978e-08, 1.2586e-07, 1.5375e-10, 1.3463e-06, 3.4525e-06],\n",
      "        [1.2985e-11, 8.1169e-06, 1.0000e+00, 7.5983e-08, 1.8030e-15, 4.3102e-06,\n",
      "         6.3703e-12, 3.2764e-11, 4.0141e-07, 5.7296e-08, 2.1446e-13],\n",
      "        [4.6543e-07, 2.5584e-10, 4.7959e-12, 1.0000e+00, 9.9972e-11, 1.7553e-14,\n",
      "         3.9943e-08, 1.9446e-11, 5.0949e-09, 4.6670e-14, 1.7198e-05]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 6.5181e-13, 5.2045e-07, 7.6935e-08, 3.4738e-06, 5.0386e-08,\n",
      "         1.3235e-08, 1.0000e+00, 1.6322e-14, 7.5185e-14, 1.9126e-14, 1.5430e-11,\n",
      "         3.8926e-13],\n",
      "        [6.7739e-10, 2.8338e-12, 2.3892e-06, 1.0901e-13, 6.1191e-06, 2.0891e-11,\n",
      "         3.5851e-20, 3.1276e-10, 4.7722e-05, 3.7449e-06, 1.0000e+00, 7.5126e-15,\n",
      "         7.9140e-06],\n",
      "        [4.6948e-12, 3.5872e-09, 2.4406e-07, 3.4809e-10, 1.8377e-14, 2.3434e-12,\n",
      "         4.7261e-08, 7.8065e-15, 1.0000e+00, 1.0000e+00, 2.2089e-07, 1.9098e-07,\n",
      "         5.0580e-13],\n",
      "        [9.6124e-09, 1.6905e-10, 4.1059e-15, 1.0000e+00, 1.2436e-07, 9.7066e-13,\n",
      "         3.3951e-07, 3.2339e-06, 1.4414e-11, 3.5873e-08, 6.2988e-13, 3.5738e-11,\n",
      "         3.0601e-08],\n",
      "        [4.2977e-07, 2.5381e-05, 2.9895e-15, 1.1318e-14, 1.4404e-06, 1.0000e+00,\n",
      "         4.1954e-09, 4.4823e-06, 2.1308e-15, 1.7011e-11, 9.2161e-13, 6.6551e-15,\n",
      "         3.4867e-10],\n",
      "        [5.2901e-05, 1.3773e-07, 1.0000e+00, 2.1033e-14, 5.5859e-11, 4.0818e-11,\n",
      "         1.6999e-13, 3.3178e-05, 2.6724e-11, 2.9706e-13, 6.0920e-05, 3.0332e-05,\n",
      "         3.9745e-07],\n",
      "        [6.2260e-07, 1.7583e-09, 6.2333e-11, 8.1600e-08, 3.5225e-10, 3.7804e-08,\n",
      "         1.0000e+00, 1.2992e-07, 2.6864e-10, 2.6723e-08, 1.1195e-17, 2.8849e-08,\n",
      "         1.2565e-07],\n",
      "        [2.4140e-11, 1.0000e+00, 2.3560e-09, 2.7939e-10, 4.9419e-12, 8.2790e-07,\n",
      "         1.4383e-09, 8.7575e-10, 9.5847e-08, 5.9395e-08, 1.2970e-05, 2.0812e-08,\n",
      "         2.8689e-07],\n",
      "        [2.0388e-10, 1.5027e-08, 4.1562e-08, 4.9064e-09, 2.9916e-16, 2.4402e-06,\n",
      "         1.3913e-06, 1.3511e-11, 6.3933e-09, 7.3812e-11, 1.2025e-05, 3.6070e-13,\n",
      "         1.0000e+00],\n",
      "        [1.6296e-09, 9.3811e-06, 1.9891e-05, 1.4265e-13, 4.8467e-14, 2.1300e-13,\n",
      "         1.4017e-06, 4.7522e-11, 2.3112e-07, 1.7831e-12, 7.6178e-13, 1.0000e+00,\n",
      "         2.4583e-06],\n",
      "        [5.6577e-07, 7.8101e-08, 2.4059e-13, 2.7965e-11, 1.0000e+00, 2.3696e-09,\n",
      "         4.6932e-14, 7.8666e-06, 7.4556e-10, 9.4492e-09, 2.0547e-04, 1.9797e-09,\n",
      "         1.1911e-13]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 5.7768e-08, 1.0000e+00, 8.0893e-11, 2.0123e-12, 1.8964e-09,\n",
      "         3.8854e-07, 2.6043e-10, 1.5722e-12, 1.3300e-10, 7.5759e-09, 1.0000e+00,\n",
      "         3.0553e-09, 4.0954e-11],\n",
      "        [1.2493e-09, 7.6384e-11, 4.2408e-12, 1.6526e-06, 2.6966e-08, 1.2413e-17,\n",
      "         8.5206e-10, 2.8666e-03, 1.0000e+00, 6.6297e-07, 9.1234e-11, 5.9594e-12,\n",
      "         2.5203e-12, 6.8629e-08],\n",
      "        [5.8979e-04, 3.5897e-13, 3.3611e-05, 2.4532e-04, 1.3334e-19, 4.2368e-13,\n",
      "         1.0000e+00, 2.5334e-15, 2.3678e-11, 2.4858e-11, 3.3791e-04, 1.0000e+00,\n",
      "         7.0552e-15, 1.0948e-06],\n",
      "        [3.6083e-05, 8.8605e-06, 3.5820e-09, 1.3579e-14, 1.1149e-07, 9.6768e-10,\n",
      "         1.6470e-13, 3.6097e-12, 2.7267e-05, 9.1310e-08, 2.2811e-07, 7.0091e-13,\n",
      "         1.0000e+00, 3.9950e-11],\n",
      "        [1.4471e-07, 1.3801e-08, 2.7648e-13, 3.3446e-12, 3.0233e-04, 5.2742e-14,\n",
      "         2.0079e-13, 2.0028e-04, 1.0397e-06, 1.0000e+00, 1.2231e-06, 3.1343e-11,\n",
      "         2.4396e-11, 1.0886e-05],\n",
      "        [3.6792e-09, 5.2442e-11, 2.6645e-07, 2.9824e-16, 1.6926e-04, 3.6343e-12,\n",
      "         5.1292e-09, 1.0000e+00, 2.1732e-03, 8.7408e-05, 9.2240e-10, 1.4038e-07,\n",
      "         6.3480e-14, 1.6504e-06],\n",
      "        [5.1363e-10, 1.0000e+00, 2.0739e-06, 1.3516e-06, 5.7734e-11, 4.5453e-08,\n",
      "         3.6796e-07, 3.7049e-08, 2.5857e-13, 4.5791e-13, 3.0840e-06, 3.3112e-09,\n",
      "         1.2431e-06, 8.5428e-16],\n",
      "        [1.0000e+00, 8.4799e-09, 1.1109e-03, 1.5720e-13, 3.8109e-09, 2.1268e-14,\n",
      "         1.8849e-09, 3.0157e-07, 1.1758e-06, 4.0994e-09, 2.7076e-08, 1.4978e-03,\n",
      "         4.2125e-04, 1.0850e-07],\n",
      "        [4.1897e-12, 1.0335e-12, 2.3391e-09, 3.6619e-04, 1.1788e-02, 1.0000e+00,\n",
      "         1.7788e-10, 1.0773e-13, 5.8388e-11, 1.8464e-14, 1.7389e-13, 1.8162e-12,\n",
      "         4.2243e-09, 3.1089e-05],\n",
      "        [5.8558e-12, 3.2430e-12, 8.5385e-10, 4.0387e-08, 1.0000e+00, 1.0000e+00,\n",
      "         1.6448e-11, 7.4707e-07, 4.8546e-08, 1.8902e-12, 4.3958e-18, 3.6768e-14,\n",
      "         5.7291e-05, 3.8952e-06],\n",
      "        [1.9304e-12, 2.2799e-16, 9.2393e-13, 3.3769e-12, 3.8451e-05, 8.2450e-05,\n",
      "         3.6849e-09, 8.1419e-07, 9.5983e-07, 1.9034e-05, 3.6095e-04, 1.9063e-10,\n",
      "         1.0322e-13, 1.0000e+00],\n",
      "        [3.4901e-12, 3.6117e-06, 1.7093e-14, 1.0000e+00, 2.0895e-17, 1.0744e-13,\n",
      "         9.7982e-05, 3.1634e-17, 2.1352e-05, 3.6472e-09, 1.3674e-08, 1.6990e-12,\n",
      "         6.5523e-11, 2.0960e-13],\n",
      "        [1.8577e-12, 8.9592e-05, 1.4747e-12, 1.9291e-09, 1.5482e-16, 2.7562e-15,\n",
      "         2.9905e-06, 1.2416e-10, 1.4324e-07, 4.1672e-07, 1.0000e+00, 3.6687e-08,\n",
      "         5.9667e-13, 1.8654e-05]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 9.0670e-10, 7.2154e-13, 8.1515e-14, 6.1553e-10, 7.1007e-13,\n",
      "         4.8673e-06, 1.2863e-11, 1.0837e-07, 1.2501e-14, 2.2194e-06, 2.8209e-11],\n",
      "        [1.8502e-07, 1.0000e+00, 7.3106e-04, 4.9366e-20, 2.3922e-06, 1.4755e-06,\n",
      "         3.7569e-10, 1.4151e-06, 1.0247e-08, 1.0752e-10, 4.8980e-15, 3.1810e-10],\n",
      "        [1.0000e+00, 1.4428e-08, 8.2675e-13, 1.0222e-12, 5.5579e-08, 1.5414e-12,\n",
      "         1.6211e-09, 1.2346e-10, 1.0603e-07, 3.0815e-15, 4.9326e-06, 9.1754e-06],\n",
      "        [1.3202e-09, 1.9657e-09, 1.1643e-10, 1.7113e-12, 6.2760e-11, 1.4240e-09,\n",
      "         1.1739e-12, 2.6263e-10, 1.0000e+00, 5.6728e-07, 5.1859e-06, 1.2780e-09],\n",
      "        [2.2120e-11, 7.8734e-10, 9.0695e-16, 6.8831e-05, 4.3216e-04, 1.0000e+00,\n",
      "         5.9846e-08, 1.5810e-11, 1.6358e-09, 1.5399e-07, 7.6697e-13, 8.2844e-04],\n",
      "        [1.3005e-10, 7.9631e-08, 4.2852e-12, 1.3144e-06, 1.0000e+00, 6.2954e-04,\n",
      "         8.1538e-06, 1.8611e-16, 1.8017e-08, 7.1529e-17, 1.2134e-08, 6.2278e-11],\n",
      "        [2.7457e-05, 1.6882e-13, 5.0283e-06, 1.0171e-08, 1.8324e-10, 8.7753e-14,\n",
      "         7.9613e-08, 3.4000e-09, 5.1842e-04, 6.2504e-11, 1.0000e+00, 1.2900e-08],\n",
      "        [7.7321e-06, 3.5028e-07, 1.1185e-14, 5.9554e-08, 4.2434e-11, 3.6934e-04,\n",
      "         1.0512e-09, 6.1388e-06, 2.7970e-11, 1.9021e-06, 4.5902e-11, 1.0000e+00],\n",
      "        [1.5263e-11, 1.7462e-10, 2.4748e-10, 3.4870e-09, 4.1307e-20, 1.3883e-07,\n",
      "         2.1297e-09, 1.9150e-05, 1.4738e-05, 1.0000e+00, 2.6138e-12, 3.4405e-06],\n",
      "        [4.9347e-09, 3.5183e-10, 8.8095e-06, 4.3114e-05, 6.4440e-19, 3.0935e-07,\n",
      "         1.1240e-13, 1.0000e+00, 1.7485e-12, 1.0000e+00, 6.2288e-09, 1.8594e-10],\n",
      "        [8.0734e-07, 2.8314e-05, 1.0000e+00, 1.1070e-05, 7.2018e-12, 2.4048e-13,\n",
      "         1.2477e-11, 7.3383e-05, 2.8270e-14, 1.1488e-08, 2.3671e-05, 9.4234e-19],\n",
      "        [1.0000e+00, 7.8035e-15, 3.4418e-07, 2.5402e-08, 1.2938e-12, 4.5334e-15,\n",
      "         2.3285e-10, 1.0357e-05, 4.5143e-06, 2.1795e-11, 1.0000e+00, 7.6819e-08],\n",
      "        [1.0667e-07, 3.4286e-09, 7.7303e-13, 2.0180e-12, 5.0077e-05, 2.2155e-09,\n",
      "         1.0000e+00, 4.1182e-12, 1.3881e-13, 4.4457e-09, 7.3316e-13, 5.6390e-10],\n",
      "        [5.4942e-11, 1.3914e-15, 7.7747e-06, 1.0000e+00, 6.2436e-11, 1.3019e-07,\n",
      "         7.1592e-11, 4.3319e-07, 1.7520e-10, 3.6113e-05, 5.1114e-06, 1.1755e-09]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M0 tensor([[1.0000e+00, 1.1996e-12, 1.0255e-11, 2.0220e-07, 5.7017e-07, 2.1454e-06,\n",
      "         4.8091e-08, 1.0000e+00, 9.5154e-10, 1.0232e-14, 1.4152e-12, 2.2612e-09,\n",
      "         5.5220e-13, 7.5752e-09],\n",
      "        [9.3027e-08, 1.2644e-05, 9.4885e-05, 1.0000e+00, 7.0743e-16, 1.3051e-11,\n",
      "         1.1569e-17, 1.7563e-06, 6.0237e-06, 2.7243e-08, 1.3370e-17, 8.3696e-07,\n",
      "         6.0274e-08, 3.1208e-15],\n",
      "        [7.2693e-11, 2.6343e-11, 1.0000e+00, 1.3955e-05, 2.1027e-11, 2.7942e-12,\n",
      "         1.5236e-06, 6.8884e-11, 1.0000e+00, 1.1325e-05, 2.4358e-08, 6.4577e-11,\n",
      "         3.6428e-07, 7.6886e-21],\n",
      "        [1.7958e-11, 1.5413e-09, 3.3569e-05, 1.0839e-14, 4.0876e-08, 4.2644e-11,\n",
      "         1.0731e-04, 1.3368e-13, 4.7266e-09, 1.1295e-05, 1.0000e+00, 3.3064e-08,\n",
      "         5.8995e-05, 2.8600e-10],\n",
      "        [1.3002e-08, 2.0627e-10, 5.3731e-13, 6.3530e-05, 1.0270e-05, 1.2735e-14,\n",
      "         1.3824e-10, 1.2224e-06, 6.0207e-06, 7.0138e-15, 3.0967e-08, 1.0000e+00,\n",
      "         1.5269e-07, 2.0257e-12],\n",
      "        [8.3994e-09, 4.8494e-10, 1.6476e-06, 1.0492e-05, 1.2843e-09, 2.1316e-06,\n",
      "         4.5345e-14, 7.9395e-07, 8.1267e-09, 2.6688e-12, 3.6853e-05, 3.3139e-08,\n",
      "         1.0000e+00, 9.2839e-10],\n",
      "        [1.2541e-05, 2.8009e-12, 3.3528e-17, 6.2517e-14, 1.0000e+00, 3.3736e-08,\n",
      "         3.4665e-12, 8.2329e-08, 1.0487e-07, 4.0996e-07, 1.3030e-13, 3.2087e-05,\n",
      "         5.5090e-11, 1.9589e-06],\n",
      "        [2.0927e-06, 1.0000e+00, 1.7403e-07, 2.7642e-08, 4.6763e-15, 1.3260e-14,\n",
      "         9.1141e-07, 2.4306e-11, 1.5470e-11, 1.4013e-05, 3.4766e-11, 1.3795e-09,\n",
      "         1.9929e-09, 1.7830e-04],\n",
      "        [2.4774e-08, 1.8772e-14, 4.3673e-07, 2.4512e-12, 1.1890e-12, 1.0000e+00,\n",
      "         1.3693e-06, 7.1623e-06, 5.1096e-13, 3.2768e-09, 4.6196e-10, 1.1198e-18,\n",
      "         3.1956e-10, 3.4379e-07],\n",
      "        [3.2673e-10, 8.8432e-05, 4.9725e-07, 3.4525e-16, 2.0108e-10, 8.6551e-05,\n",
      "         8.6160e-06, 4.1962e-16, 7.0781e-11, 1.0000e+00, 2.0830e-08, 1.7364e-18,\n",
      "         9.7990e-08, 6.6322e-07],\n",
      "        [1.4416e-04, 1.0785e-13, 3.1364e-07, 1.9147e-18, 2.7434e-08, 1.2675e-06,\n",
      "         1.0000e+00, 2.5983e-06, 1.4167e-05, 1.8215e-08, 2.1974e-07, 9.8793e-14,\n",
      "         1.8829e-13, 1.2197e-11],\n",
      "        [2.3717e-07, 1.1562e-04, 1.3728e-16, 2.2552e-09, 2.1808e-06, 9.7106e-07,\n",
      "         3.1830e-07, 3.9783e-07, 1.2473e-18, 2.5052e-06, 4.3102e-07, 1.0629e-06,\n",
      "         3.5198e-12, 1.0000e+00]], grad_fn=<SliceBackward0>)\n",
      "M0 tensor([[1.0000e+00, 2.6220e-10, 7.2448e-16, 3.0632e-12, 1.0762e-10, 1.3928e-10,\n",
      "         1.5011e-11, 8.0434e-07, 6.8972e-07, 2.8521e-12, 4.7000e-07, 3.3111e-13,\n",
      "         1.3480e-06, 1.1028e-10],\n",
      "        [3.0774e-10, 2.0500e-05, 9.0944e-14, 2.5400e-12, 1.5704e-06, 3.6200e-05,\n",
      "         5.4835e-07, 7.3445e-06, 9.8391e-13, 1.0000e+00, 9.1972e-13, 2.5635e-09,\n",
      "         3.7992e-12, 2.5928e-16],\n",
      "        [4.5763e-09, 5.8399e-17, 2.4621e-06, 5.0259e-12, 1.0000e+00, 1.6705e-12,\n",
      "         6.5842e-08, 4.4827e-05, 4.9892e-16, 9.1317e-05, 1.1394e-03, 1.0745e-06,\n",
      "         1.2407e-04, 1.8141e-09],\n",
      "        [7.4854e-05, 1.8516e-08, 2.1062e-08, 1.3231e-15, 8.1865e-04, 3.8004e-06,\n",
      "         7.1285e-09, 1.0000e+00, 4.2106e-09, 2.5805e-07, 1.2009e-09, 1.4966e-05,\n",
      "         2.4812e-15, 2.8772e-13],\n",
      "        [1.7244e-07, 6.3394e-05, 4.3083e-11, 4.8984e-07, 2.8382e-19, 3.8661e-09,\n",
      "         2.6731e-05, 1.0922e-15, 1.0000e+00, 1.4692e-14, 1.3834e-05, 6.9139e-09,\n",
      "         4.7910e-09, 9.6737e-09],\n",
      "        [1.8694e-05, 1.1995e-07, 1.0000e+00, 3.5478e-07, 9.9449e-13, 8.6165e-18,\n",
      "         2.1589e-06, 8.5338e-17, 6.2754e-12, 2.6811e-08, 4.5826e-10, 3.7536e-11,\n",
      "         8.8274e-07, 1.0000e+00],\n",
      "        [1.4497e-05, 4.5783e-15, 1.3343e-08, 2.0236e-05, 8.4638e-05, 5.6563e-12,\n",
      "         1.6274e-09, 1.7556e-17, 5.7794e-17, 6.5982e-10, 1.6217e-06, 9.1153e-19,\n",
      "         1.0000e+00, 3.6834e-05],\n",
      "        [1.0000e+00, 7.6698e-09, 1.0644e-07, 8.1884e-14, 6.6517e-09, 1.3577e-10,\n",
      "         1.1488e-11, 2.2453e-04, 9.8650e-11, 3.2950e-09, 2.1971e-09, 1.4753e-09,\n",
      "         8.7365e-10, 5.9456e-06],\n",
      "        [4.0916e-08, 6.8566e-18, 1.9023e-10, 3.3728e-12, 2.0556e-04, 4.7967e-10,\n",
      "         2.9344e-09, 1.9786e-10, 9.6555e-05, 3.7226e-09, 1.0000e+00, 7.6551e-06,\n",
      "         8.4523e-06, 6.5432e-12],\n",
      "        [7.0732e-13, 5.8210e-06, 1.7707e-07, 9.2701e-11, 9.7721e-08, 1.1504e-15,\n",
      "         1.0000e+00, 3.1727e-07, 1.8770e-06, 4.8390e-12, 9.5794e-10, 1.3292e-10,\n",
      "         3.2135e-06, 3.6247e-07],\n",
      "        [1.6727e-11, 3.7395e-09, 9.8906e-08, 1.0000e+00, 2.3128e-11, 1.9146e-03,\n",
      "         4.1997e-15, 2.6034e-13, 4.7034e-11, 3.8448e-13, 4.9710e-09, 2.0005e-06,\n",
      "         4.1044e-07, 3.4437e-09],\n",
      "        [6.2527e-10, 1.1134e-08, 4.4153e-17, 2.2394e-05, 4.0387e-11, 1.0000e+00,\n",
      "         3.2587e-12, 7.1726e-06, 3.6110e-07, 2.5523e-06, 3.7646e-09, 7.0095e-06,\n",
      "         5.1972e-13, 6.7287e-17],\n",
      "        [1.6503e-10, 2.2471e-07, 8.8933e-09, 7.2872e-05, 9.1794e-08, 2.6163e-05,\n",
      "         2.4850e-09, 2.3737e-06, 4.6032e-05, 4.7120e-05, 9.0701e-05, 1.0000e+00,\n",
      "         5.7230e-13, 7.0241e-13],\n",
      "        [4.8073e-10, 1.0000e+00, 3.7951e-09, 6.7181e-08, 4.3652e-13, 4.7175e-09,\n",
      "         3.8361e-07, 3.0554e-10, 3.7909e-10, 1.5639e-06, 5.4370e-18, 9.1272e-11,\n",
      "         3.7501e-13, 9.5893e-09]], grad_fn=<SelectBackward0>)\n",
      "M0 tensor([[1.4528e-08],\n",
      "        [5.9690e-10],\n",
      "        [1.8674e-05],\n",
      "        [4.2992e-09],\n",
      "        [9.1805e-08],\n",
      "        [1.2313e-13],\n",
      "        [1.1654e-05],\n",
      "        [6.8953e-14],\n",
      "        [3.2662e-09],\n",
      "        [1.3124e-07],\n",
      "        [4.1746e-09],\n",
      "        [1.2663e-10],\n",
      "        [3.2555e-06],\n",
      "        [8.0760e-04]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#recon+testerror\n",
    "for r in range(1,3):\n",
    "    r=8\n",
    "    src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=r)\n",
    "\n",
    "    #print(src1.size())\n",
    "    src1= src1.to(DEVICE)\n",
    "    src2= src2.to(DEVICE)\n",
    "    \n",
    "    src_padding_mask1=create_mask(src1,-100)\n",
    "    src_padding_mask2=create_mask(src2,-100)\n",
    "    \n",
    "    \n",
    "    #transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "    transformer.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "    Ad = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "    \n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    #print('L',val_loss)\n",
    "    a=0.1\n",
    "    #print(Ad)\n",
    "    #pp_A = complete_postprocess(Ad,d,a)\n",
    "    \n",
    "    #err_p=err_perc(pp_A,y)\n",
    "    #print('err',r,err_p)\n",
    "\n",
    "#print(src1.size())\n",
    "\n",
    "    print('y',y[6])\n",
    "    print('Ad',Ad[6])\n",
    "    #print('pp',pp_A[6])\n",
    "    #print('d',d[6])\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(pp_A[i])\n",
    "    \n",
    "    \n",
    "    make_reconstructed_edgelist(Ad,run=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umap AdjacencyTrans2\n",
    "\n",
    "\n",
    "emb_size= 150 ###!!!!24 for n2v emb\n",
    "nhead= 6    ###!!!! 6 for n2v emb\n",
    "num_encoder_layers = 3\n",
    "\n",
    "\n",
    "transformer = AdjacencyTransformer_2(num_encoder_layers, emb_size, nhead,out=True)\n",
    "\n",
    "\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = Loss(pen=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "loss_over_time=[]\n",
    "test_error=[]\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer,loss_fn)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer,loss_fn)\n",
    "    \n",
    "    \n",
    "    loss_over_time.append(train_loss)\n",
    "    np.savetxt('./'+'train_loss_Ad2.txt', np.c_[loss_over_time],delimiter='\\t',header='trainloss')\n",
    "    \n",
    "    test_error.append(val_loss)\n",
    "                \n",
    "    np.savetxt('./'+'test_loss_Ad2.txt', np.c_[test_error],delimiter='\\t',header='testloss')\n",
    "\n",
    "    \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "loss_over_time= np.loadtxt('./train_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "test_error= np.loadtxt('./test_loss_Ad2.txt',skiprows=1, delimiter='\\t', usecols=(0), unpack=True)\n",
    "\n",
    "\n",
    "N=1\n",
    "\n",
    "plt.plot(np.convolve(np.log10(loss_over_time), np.ones(N)/N, mode='valid'),c='red')\n",
    "plt.plot(np.convolve(np.log10(test_error), np.ones(N)/N, mode='valid'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "transformer.load_state_dict(torch.load('AttTrack_2.pt',map_location=torch.device('cpu')))\n",
    "transformer.eval()\n",
    "\n",
    "run=95\n",
    "t= 8\n",
    "src1, src2, y,d = collate_fn(31,-100,recon=True,train=False,run=run)\n",
    "src_padding_mask1=create_mask(src1,-100)\n",
    "src_padding_mask2=create_mask(src2,-100)\n",
    "\n",
    "\n",
    "Ad,out1,out2,out_dec1,src_t1,src_t2 = transformer(src1,src2,src_padding_mask1,src_padding_mask2)\n",
    "\n",
    "\n",
    "\n",
    "out_dec1=torch.transpose(out_dec1,2,1)\n",
    "out_dec1=torch.transpose(out_dec1,1,0)\n",
    "print(out_dec1.shape)\n",
    "\n",
    "\n",
    "src_t1=src_t1[:,t,:]#[1:]\n",
    "src_t2=src_t2[:,t,:]#[1:]\n",
    "\n",
    "ind1=np.where(src_t1 == -100)\n",
    "ind2=np.where(src_t2 == -100)\n",
    "\n",
    "a=out1.detach().numpy()\n",
    "b=out_dec1.detach().numpy()\n",
    "\n",
    "a=a[:,t,:]#[1:]\n",
    "b=b[:,t,:]#[1:]\n",
    "\n",
    "a=a[0:ind1[0][0]]\n",
    "\n",
    "b=b[0:ind2[0][0]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "blue_list=['#2a186c','#2e1f98','#1a3b9f','#0c5294','#16638d','#25738a','#328388','#3c9387','#45a383','#53b47c','#69c46f']\n",
    "red_list=['#2f0303','#6e0302','#9a0303','#c40303','#f30203','#ff1f03','#ff4a04','#fe7104','#ffa001','#fec701','#fef903']\n",
    "c_list=[]\n",
    "\n",
    "for p in range(len(a)):\n",
    "    c_list.append(blue_list[p])\n",
    "    \n",
    "for t in range(len(b)):\n",
    "    c_list.append(red_list[t])\n",
    "\n",
    "#print(c_list)\n",
    "c_list=['blue']*len(a)+['black']*len(b)\n",
    "\n",
    "#print(src_t1.shape)\n",
    "\n",
    "src=np.vstack((a,b))\n",
    "\n",
    "'''\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, stratify=mnist.target, random_state=42\n",
    ")\n",
    "'''\n",
    "print(src.shape)\n",
    "reducer = umap.UMAP(metric='cosine',n_neighbors=4)\n",
    "embedding = reducer.fit_transform(src)\n",
    "#print(embedding_train,embedding_train.shape)\n",
    "#embedding_test = reducer.transform(X_test)\n",
    "print(embedding)\n",
    "plt.scatter(embedding[:, 0],embedding[:, 1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "'''[[11.102701   9.834718 ]\n",
    " [10.975245  11.376655 ]\n",
    " [11.55883   10.9941   ]\n",
    " [10.942158  10.440168 ]\n",
    " [10.304249  10.682447 ]\n",
    " [10.096922  10.017049 ]\n",
    " [10.49952   12.192604 ]\n",
    " [ 8.663966  11.4105625]\n",
    " [ 9.177266  12.255981 ]\n",
    " [ 8.936496  10.613881 ]\n",
    " [10.011719  11.911004 ]\n",
    " [ 9.29462   11.477478 ]\n",
    " [ 9.607173  10.698044 ]]'''\n",
    "\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)\n",
    "#plt.savefig('./umap_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "print(src.shape)\n",
    "tsne_results = tsne.fit_transform(src)\n",
    "\n",
    "\n",
    "\n",
    "print(tsne_results)\n",
    "\n",
    "plt.scatter(tsne_results[:,0],tsne_results[:,1],c=c_list)\n",
    "plt.gca().set_aspect('equal')\n",
    "#plt.savefig('./tsne_1_12_16.png',transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
